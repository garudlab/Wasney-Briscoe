{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6af3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting working directory\n",
    "import sys\n",
    "sys.path.insert(0, \"/u/project/ngarud/michaelw/Diversity-Along-Gut/ConventionalMouse/scripts/postprocessing/postprocessing_scripts/\")\n",
    "sys.path.insert(0, \"/u/project/ngarud/michaelw/Diversity-Along-Gut/ConventionalMouse/scripts/strain_phasing/\")\n",
    "\n",
    "# Normal Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import random as rand\n",
    "from random import randint,sample,choices\n",
    "from math import log\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import figure_utils as fu\n",
    "from matplotlib import colormaps\n",
    "plasma_cmap = colormaps.get_cmap('plasma')\n",
    "\n",
    "# config\n",
    "import config\n",
    "\n",
    "# predefined functions\n",
    "from strain_phasing_functions import *\n",
    "from parse_midas_data import *\n",
    "import diversity_utils\n",
    "import species_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a693a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_path = \"%s%s\" % (config.metadata_directory, \"species_snps.txt\")\n",
    "with open(species_path, 'r') as file:\n",
    "    species_list = [line.strip() for line in file]\n",
    "\n",
    "species_code_map = species_utils.parse_species_code_maps()[0]\n",
    "\n",
    "sample_metadata_map = parse_sample_metadata_map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6a939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"229547\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster directory already exists.\n",
      "Figure directory already exists.\n"
     ]
    }
   ],
   "source": [
    "strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "#Raw cluster\n",
    "# raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "species_raw_cluster_dir = \"%s%s/\" % (raw_cluster_path, species)\n",
    "if not os.path.exists(species_raw_cluster_dir):\n",
    "    os.makedirs(species_raw_cluster_dir)\n",
    "    print(\"Cluster directory created successfully!\")\n",
    "else:\n",
    "    print(\"Cluster directory already exists.\")\n",
    "\n",
    "strain_phasing_figures_dir = \"%s%s\" % (config.figure_directory, \"strain_phasing/\")\n",
    "if not os.path.exists(strain_phasing_figures_dir):\n",
    "    os.makedirs(strain_phasing_figures_dir)\n",
    "    print(\"Figure directory created successfully!\")\n",
    "else:\n",
    "    print(\"Figure directory already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25251fc4",
   "metadata": {},
   "source": [
    "## Selecting host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc3a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_array = []\n",
    "for species in species_list:\n",
    "    highcoverage_samples = diversity_utils.calculate_highcoverage_samples(species)\n",
    "    if len(highcoverage_samples) == 0:\n",
    "        continue\n",
    "    haploid_samples = diversity_utils.calculate_haploid_samples(species, quick_and_dirty=True) #  quick_and_dirty=True\n",
    "    non_haploid_samples = [s for s in highcoverage_samples if s not in set(haploid_samples)]\n",
    "    number_of_haploid_samples = len(haploid_samples)\n",
    "    number_of_non_haploid_samples = len(non_haploid_samples)\n",
    "    qp_array.append([species, number_of_haploid_samples, \"QP\"])\n",
    "    qp_array.append([species, number_of_non_haploid_samples, \"Not QP\"])\n",
    "qp_df = pd.DataFrame(data = qp_array, columns = [\"species\", \"sample_count\", \"QP\"])\n",
    "qp_df['sample_count'] = qp_df[\"sample_count\"].astype(float)\n",
    "qp_df['species_name'] = [species_code_map[species] for species in qp_df['species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd77f555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>QP</th>\n",
       "      <th>species_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>207693</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>f__Oscillospiraceae (207693)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100555</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__Lawsonibacter (100555)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>214603</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__CAG-95 (214603)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>203686</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__UBA3282 (203686)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>261672</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__Angelakisella (261672)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>217378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__1XD42-69 (217378)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>229722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__Ruminiclostridium_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>231109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__CAG-81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>231118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__Clostridium_Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>266763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not QP</td>\n",
       "      <td>g__Eubacterium_F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species  sample_count      QP                  species_name\n",
       "33   207693           6.0  Not QP  f__Oscillospiraceae (207693)\n",
       "11   100555           4.0  Not QP     g__Lawsonibacter (100555)\n",
       "51   214603           4.0  Not QP            g__CAG-95 (214603)\n",
       "17   203686           4.0  Not QP           g__UBA3282 (203686)\n",
       "97   261672           2.0  Not QP     g__Angelakisella (261672)\n",
       "..      ...           ...     ...                           ...\n",
       "63   217378           0.0  Not QP          g__1XD42-69 (217378)\n",
       "67   229722           0.0  Not QP        g__Ruminiclostridium_E\n",
       "69   231109           0.0  Not QP                     g__CAG-81\n",
       "71   231118           0.0  Not QP              g__Clostridium_Q\n",
       "127  266763           0.0  Not QP              g__Eubacterium_F\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp_df[qp_df['QP'] == \"Not QP\"].sort_values(\"sample_count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9153ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "205567    37.0\n",
       "263040    27.0\n",
       "234341    18.0\n",
       "261649    18.0\n",
       "213999    18.0\n",
       "          ... \n",
       "231349     1.0\n",
       "217240     1.0\n",
       "261755     1.0\n",
       "217378     1.0\n",
       "100111     1.0\n",
       "Name: sample_count, Length: 64, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp_df.groupby(\"species\")['sample_count'].sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c486cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"229547\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "min_cluster_size = 1000\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "min_cluster_fraction = 1/10\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more   \n",
    "# max_d = 3.5\n",
    "max_d = 4.75 # 4.5 for species 229547\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 10 \n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "poly_cov_frac = 1/5 #\n",
    "\n",
    "## Number of clusters to calculate\n",
    "n_clusters = 100\n",
    "\n",
    "#Minimum number of snvs per sample\n",
    "min_num_snvs_per_sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a read support of 4 for each polymorphism.\n"
     ]
    }
   ],
   "source": [
    "Fs,Ass,Dss = return_FAD(species, min_coverage=min_coverage, \n",
    "                        min_sample_coverage=min_sample_coverage, \n",
    "                        poly_cov_frac = poly_cov_frac, \n",
    "                        calculate_poly_cov_frac=False, \n",
    "                        read_support = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out samples without an adequate number of SNVs when all is said and done\n",
    "sample_with_adequate_snv_count = ~((~np.isnan(Fs)).sum() < min_num_snvs_per_sample)\n",
    "\n",
    "Fs = Fs.loc[:,sample_with_adequate_snv_count]\n",
    "Ass = Ass.loc[:,sample_with_adequate_snv_count]\n",
    "Dss = Dss.loc[:,sample_with_adequate_snv_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 14897 SNVs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 s, sys: 3.25 s, total: 30.2 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fss = Ass.values/(Dss.values + (Dss.values == 0)) #This is so it doesn't produce a na (division by 0)\n",
    "\n",
    "cluster_As = Ass.values\n",
    "cluster_Ds = Dss.values\n",
    "cluster_fs = cluster_As/(cluster_Ds + (cluster_Ds == 0))\n",
    "\n",
    "## for compatibility in case of threshold number of SNVs\n",
    "num = min(max_num_snvs,Fs.shape[0])\n",
    "\n",
    "i_list = Dss.T.mean().sort_values(ascending=False).index[:num]\n",
    "\n",
    "sys.stderr.write(\"Processing %s SNVs\" % num)\n",
    "\n",
    "## simply shuffles indices if no threshold is specified\n",
    "#i_list = sample(range(Fs.shape[0]),num)\n",
    "i_list_idx = Fs.loc[i_list].index\n",
    "\n",
    "Ass_sub = Ass.loc[i_list_idx]\n",
    "Dss_sub = Dss.loc[i_list_idx]\n",
    "Fs_sub = Fs.loc[i_list_idx]\n",
    "\n",
    "fss_sub = Ass_sub.values/(Dss_sub.values + (Dss_sub.values == 0))\n",
    "\n",
    "cluster_As_sub = Ass_sub.values\n",
    "cluster_Ds_sub = Dss_sub.values\n",
    "cluster_fs_sub = cluster_As_sub/(cluster_Ds_sub + (cluster_Ds_sub == 0))\n",
    "\n",
    "D_mat = np.zeros([num,num])\n",
    "D_mat_1 = D_mat_fun1(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "D_mat = np.zeros([num,num]) \n",
    "D_mat_2 = D_mat_fun2(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "\n",
    "D_mat = np.fmin(D_mat_1,D_mat_2) #I believe this is filling in the minimum of the two polarizations\n",
    "D_mat = symmetrize(D_mat)\n",
    "\n",
    "D_mat_1 = pd.DataFrame(D_mat_1,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "D_mat_2 = pd.DataFrame(D_mat_2,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "\n",
    "D_mat_close = pd.DataFrame(D_mat < max_d) \n",
    "\n",
    "D_mat_close.index = Fs_sub.index\n",
    "D_mat_close.columns = Fs_sub.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10420\n"
     ]
    }
   ],
   "source": [
    "## extracts up to 100 clusters\n",
    "## in practice all SNVs should fall into one of a fairly small number of clusters\n",
    "## really should re-write this with a while loop but this works for now\n",
    "## the idea is that we exhaust all clusters—there should only be a small number of them ultimately\n",
    "\n",
    "###Idea with while loop:\n",
    "##### While there are still variants out there, have it try to be clusterings\n",
    "\n",
    "all_clus_pol = []\n",
    "all_clus_idx = []\n",
    "all_clus_A = []\n",
    "all_clus_D = []\n",
    "\n",
    "all_clus_F = []\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        clus,clus_idxs = return_clus(D_mat_close,Fs_sub)\n",
    "#         clus,clus_idxs = return_clus(D_mat_close,Fs_sub, co_cluster_pct=0.5) #Finding points that cluster with 25% other points. That's a cluster.\n",
    "                                                            #We would modify this function to get smaller clusters...\n",
    "        clus_pol = polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2)\n",
    "        clus_pol.index = clus_idxs\n",
    "        D_mat_close = drop_clus_idxs(D_mat_close,clus_idxs)\n",
    "\n",
    "        if clus_pol.shape[0] > min_cluster_size and clus_pol.shape[0] > Fs.shape[0]*min_cluster_fraction:\n",
    "\n",
    "            all_clus_D.append(Dss.loc[clus.index].mean().values)\n",
    "            all_clus_pol.append(clus_pol)\n",
    "            all_clus_A.append(clus_pol.mean()*all_clus_D[-1])\n",
    "            all_clus_F.append(clus_pol.mean())\n",
    "\n",
    "            print(clus_pol.shape[0])\n",
    "\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cluster 1\n",
      "\n",
      "\t0.0% finished\n",
      "\t6.713% finished\n",
      "\t13.426% finished\n",
      "\t20.138% finished\n",
      "\t26.851% finished\n",
      "\t33.564% finished\n",
      "\t40.277% finished\n",
      "\t46.989% finished\n",
      "\t53.702% finished\n",
      "\t60.415% finished\n",
      "\t67.128% finished\n",
      "\t73.84% finished\n",
      "\t80.553% finished\n",
      "\t87.266% finished\n",
      "\t93.979% finished"
     ]
    }
   ],
   "source": [
    "## now, choosing a representative SNV from each cluster, and finding all other sites (not just limited to the 20k)\n",
    "## which are consistent w/ being linked to it\n",
    "\n",
    "final_clusters = []\n",
    "\n",
    "all_aligned_sites = []\n",
    "\n",
    "for i in range(len(all_clus_D)):\n",
    "    \n",
    "    sys.stderr.write(f'\\n\\nCluster {i+1}\\n')\n",
    "    ancD = all_clus_D[i]\n",
    "    ancF = all_clus_F[i]\n",
    "\n",
    "    dss = Dss.values\n",
    "    fss = Fs.values\n",
    "    \n",
    "    disAnc_forward = []\n",
    "    disAnc_backward = []\n",
    "\n",
    "    for j in range(Dss.shape[0]):\n",
    "        disAnc_forward.append(calc_dis(ancD,dss[j],ancF,fss[j]))\n",
    "        disAnc_backward.append(calc_dis(ancD,dss[j],ancF,1-fss[j]))\n",
    "        if j % 1000 == 0:\n",
    "            sys.stderr.write(f\"\\n\\t{np.around(100*j/Dss.shape[0],3)}% finished\")\n",
    "    \n",
    "    disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "    disAnc = np.array(disAnc)\n",
    "    aligned_sites = Fs.loc[disAnc < max_d].index\n",
    "    f_dist =  pd.DataFrame(np.array([disAnc_forward,disAnc_backward]).T,index=Fs.index)\n",
    "    pols = f_dist.T.idxmin() > 0\n",
    "    \n",
    "    aligned_sites = [a for a in aligned_sites if a not in all_aligned_sites]\n",
    "    \n",
    "    pols = pols.loc[aligned_sites]\n",
    "    re_polarize = pols.loc[pols].index\n",
    "    \n",
    "    all_aligned_sites.extend(aligned_sites)\n",
    "    \n",
    "    Fs_cluster = Fs.loc[aligned_sites]\n",
    "    \n",
    "    Fs_cluster.loc[re_polarize] = 1 - Fs_cluster.loc[re_polarize]\n",
    "        \n",
    "    final_clusters.append(Fs_cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING RAW FILE\n",
    "species_raw_cluster_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_RawCluster.pckl\")\n",
    "\n",
    "pickle_object = open(species_raw_cluster_path, \"wb\")\n",
    "pickle.dump(final_clusters, pickle_object)\n",
    "pickle_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #LOADING THE RAW FILE\n",
    "# species_raw_cluster_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_RawCluster.pckl\")\n",
    "\n",
    "\n",
    "# final_clusters = pd.read_pickle(species_raw_cluster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating polarized clusters\n",
    "\n",
    "Once clusters have been identified and internally polarized, they need to be polarized relative to one another. In the best case, the sum of strain frequencies will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(final_clusters) > 1:\n",
    "    sys.stderr.write(\"More than 1 strains detected.\\n\")\n",
    "## If only a single cluster is detected, add a second \"cluster\" which is simply 1 minus the allele frequencies\n",
    "## in the first cluster\n",
    "## aids in visualization for people not familiar with this kind of clustering\n",
    "if len(final_clusters) == 1:\n",
    "    final_clusters.append(1-final_clusters[0])\n",
    "\n",
    "## add cluster centroids\n",
    "final_f = []\n",
    "for cluster in final_clusters:\n",
    "    final_f.append(cluster.mean())\n",
    "df_final_f = pd.DataFrame(final_f)\n",
    "\n",
    "## now, polarize clusters so that the sum of squareds of the centroids to 1 is minimized\n",
    "## the idea here is that accurate strain frequencies should sum to 1\n",
    "polarize = True\n",
    "\n",
    "pol_d2 = {}\n",
    "\n",
    "for i in range(df_final_f.shape[0]):\n",
    "    df_final_f_temp = df_final_f.copy() #Makes a copy of the centroids\n",
    "    df_final_f_temp.iloc[i] = 1 - df_final_f_temp.iloc[i] #gets the polarized version of ONE of the centroids.\n",
    "    pol_d2[i] =  ((1 - df_final_f_temp.sum())**2).sum()   #Get the across centroids for all samples (should be close to 1), \n",
    "                                                                #subtract this from 1, and square. Sum all those values\n",
    "                                                                #Ideally, this value is really close to 0. \n",
    "                                                                #Add this value to the dictionary.\n",
    "\n",
    "    pol_d2 = pd.Series(pol_d2)                                #Make the dictionary a series \n",
    "\n",
    "    if pol_d2.min() < ((1 - df_final_f.sum())**2).sum(): #If any of the above repolarizations actually made the overall sum of centroids closer to 1, repolarize.\n",
    "        clus_to_re_pol = pol_d2.idxmin()\n",
    "        final_f[clus_to_re_pol] = 1 - final_f[clus_to_re_pol]\n",
    "        final_clusters[clus_to_re_pol] = 1 - final_clusters[clus_to_re_pol]\n",
    "        df_final_f = pd.DataFrame(final_f)  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out samples in which each cluster does not have adequate snvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indices = []\n",
    "\n",
    "for i,cluster in enumerate(final_clusters):\n",
    "    if i == 0:\n",
    "        good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "    else:\n",
    "        new_good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "        good_samples = good_samples & new_good_samples\n",
    "\n",
    "for i,cluster in enumerate(final_clusters): \n",
    "    final_clusters[i] = final_clusters[i].T.loc[good_samples].T\n",
    "    final_f[i] = final_f[i].T.loc[good_samples]\n",
    "\n",
    "Fs = Fs.T.loc[good_samples].T\n",
    "Ass = Ass.T.loc[good_samples].T\n",
    "Dss = Dss.T.loc[good_samples].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter all all na columns if there are any - THis is redundant\n",
    "if len(final_clusters) > 0:\n",
    "    for i,cluster in enumerate(final_clusters):\n",
    "        if i == 0:\n",
    "            mask = ~np.isnan(cluster).all(axis = 0)\n",
    "        final_clusters[i] = cluster.loc[:,mask]\n",
    "        final_f[i] = final_f[i][mask]\n",
    "    Fs = Fs.loc[:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #SAVING RAW FILE\n",
    "# species_centroid_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_ClusterCentroid.pckl\")\n",
    "# species_polarized_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_PolarizedCluster.pckl\")\n",
    "# final_Fs_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_final_Fs.pckl\")\n",
    "\n",
    "# pickle_object = open(species_centroid_cluster_path, \"wb\")\n",
    "# pickle.dump(final_f, pickle_object)\n",
    "# pickle_object.close()\n",
    "\n",
    "# pickle_object = open(species_polarized_cluster_path, \"wb\")\n",
    "# pickle.dump(final_clusters, pickle_object)\n",
    "# pickle_object.close()\n",
    "\n",
    "# pickle_object = open(final_Fs_path, \"wb\")\n",
    "# pickle.dump(Fs, pickle_object)\n",
    "# pickle_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot the chosen polarization of strains\n",
    "## sum of strain frequencies should be ~1 at all timepoints\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(pd.DataFrame(final_f).sum().values,zorder=10,lw=3)\n",
    "ax.set_ylim([.5,1.5])\n",
    "ax.axhline(1,color=\"k\",ls=\"--\")\n",
    "ax.set_ylabel(\"Sum of strain frequencies\",size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting strain frequencies\n",
    "\n",
    "Strain frequencies can be plotted using a main key (e.g. Day) and a secondary key (e.g. Capsule), yielding a two-level identification of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More ordering utilities\n",
    "mnum = list(set(Fs.T.index.get_level_values(\"mouse_number\")))\n",
    "msite = list(set(Fs.T.index.get_level_values(\"region\")))\n",
    "mcage = list(set(Fs.T.index.get_level_values(\"cage\")))\n",
    "\n",
    "mnum_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"mouse_number\").index.get_level_values(\"mouse_number\") == m).ravel() for m in list(set(mnum))}\n",
    "\n",
    "msite_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"region\").index.get_level_values(\"region\") == m).ravel() for m in list(set(msite))}\n",
    "\n",
    "mcage_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"cage\").index.get_level_values(\"cage\") == m).ravel() for m in list(set(mcage))}\n",
    "\n",
    "all_sample_dics = {\"region\":msite_sample_dic,\"mouse_number\":mnum_sample_dic,\"cage\":mcage_sample_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_sort = \"cage\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "subset = True\n",
    "subset_value = 5000\n",
    "\n",
    "if subset:\n",
    "    high_coverage_snv_idxs = Dss.median(axis = 1).sort_values(ascending = False).index\n",
    "\n",
    "\n",
    "cmap_cage = []\n",
    "# colors_library = [(1, 1, 0.8, 0.25), #creamy yellow\n",
    "#                   (0.529, 0.808, 0.922, 0.25), #sky blue\n",
    "#                   (0.529, 0.808, 0.922, 0.25)]\n",
    "#                   #(0.529, 0.808, 0.722, 0.25), #green blue\n",
    "#                   #(0.294, 0.427, 0.804, 0.15)] #purple blue\n",
    "colors_library = [(0.9058823529411765, 0.8823529411764706, 0.9372549019607843, 2/3), #light purple\n",
    "                  (0.6666666666666666, 0.596078431372549, 0.6627450980392157, 2/3), #dark purple\n",
    "                  (0.9058823529411765, 0.8823529411764706, 0.9372549019607843, 2/3)]\n",
    "                  #(0.529, 0.808, 0.722, 0.25), #green blue\n",
    "                  #(0.294, 0.427, 0.804, 0.15)] #purple blue\n",
    "if \"Cage 1\" in mcage_sample_dic:\n",
    "#     cmap_cage.append(colors_library(0))\n",
    "    cmap_cage.append(colors_library[0])\n",
    "if \"Cage 2\" in mcage_sample_dic:\n",
    "    # cmap_cage.append(colors_library[1])\n",
    "    cmap_cage.append((1,1,1,2/3))\n",
    "    \n",
    "if \"Cage 3\" in mcage_sample_dic:\n",
    "    cmap_cage.append(colors_library[2])\n",
    "    \n",
    "\n",
    "cmap_clus = get_cmap(5,name=\"Set3\")\n",
    "# manual_cmap_clus = [(0.396078431372549,0.2627450980392157,0.12941176470588237,1),\n",
    "#                     (0.8235294117647058,0.7058823529411765,0.5490196078431373,1)]\n",
    "\n",
    "manual_cmap_clus = [(0.16470588235294117,0.4,0.4549019607843137,1),\n",
    "                    (0.5019607843137255,0.5019607843137255,0.5019607843137255,1)]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    if subset:\n",
    "        high_coverage_snv_idxs_strain = high_coverage_snv_idxs.intersection(final_clusters[i].index)[:subset_value]\n",
    "    ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "    ax.plot(ff.values,zorder=100,lw=6,color=manual_cmap_clus[i],label=f\"Strain {i+1}\");\n",
    "    ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "    if len(final_clusters) != 0:\n",
    "        if subset:\n",
    "            ff_c = reorder_sort(final_clusters[i].loc[high_coverage_snv_idxs_strain].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "        else:\n",
    "            ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "        ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=manual_cmap_clus[i],alpha=.01)\n",
    "    else:\n",
    "        raise ValueError(\"No clusters to plot.\")\n",
    "\n",
    "\n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff.index.get_level_values(secondary_key))\n",
    "\n",
    "\n",
    "#Making the vertical lines and labels\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)   \n",
    "        ax.text(e, -0.1, abbreviate_gut_site(second_xlabels[e], blank_inoc = True), ha='center',va='top', clip_on=False,size=15, rotation=0)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "    if (max(all_sample_dics[key_to_sort]) == key):\n",
    "        ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "        ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=2, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    else:\n",
    "        ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "        ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "#Making the vertical colors\n",
    "key_to_sort = \"cage\"\n",
    "i = 0    \n",
    "for key, item in sorted(all_sample_dics[key_to_sort].items()):\n",
    "        \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "        \n",
    "    if (key == max(all_sample_dics[key_to_sort])):\n",
    "        ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=2, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "        ax.axvspan(xmin - .25,xmax+.25,alpha=.2,color='black') \n",
    "\n",
    "        i+=1\n",
    "    else:\n",
    "        ax.axvspan(xmin - .1,xmax+.1,color=cmap_cage[i]) #alpha=.2,\n",
    "        ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=2, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "        ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=2, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "        i+=1\n",
    "\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels([label if label != \"Inoculum\" else \"\" for label in labels])\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"Strain frequency\",size=30)\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "if \"Inoculum\" in labels:\n",
    "    ax.set_xlim([-1,max(major_x)+0.25])\n",
    "    \n",
    "\n",
    "# ax.set_xlabel(\"Sample\", size = 30)\n",
    "\n",
    "\n",
    "fig.legend(prop={\"size\":20})\n",
    "# fig.legend(prop={\"size\":20}, bbox_to_anchor=(0.85, 0.5));\n",
    "\n",
    "#plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_sort(f,\"cage\").sort_index(key=lambda x: x.map(order_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b9f7e",
   "metadata": {},
   "source": [
    "## Bootstrapping support and saving file for R plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2587bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_f[0]).reset_index().sort_values(by = [\"cage\", \"mouse_number\"]).rename(columns = {0: \"freq\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99695608",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strains = True\n",
    "bootstrap_ci = True\n",
    "boostrap_k = 100\n",
    "bootstrap_N = 1000\n",
    "np.random.seed(6)\n",
    "columns_to_keep = ['sample',\n",
    "                   'species', \n",
    "                   'strain', \n",
    "                   'cage', \n",
    "                   'mouse_number', \n",
    "                   'region', \n",
    "                   'freq']\n",
    "if len(final_clusters) > 0:\n",
    "    for i in np.arange(len(final_f)): \n",
    "        if i == 0:\n",
    "            final_f_all_strains = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "            final_f_all_strains['species'] = [species]*len(final_f_all_strains)\n",
    "            final_f_all_strains['strain'] = [i + 1]*len(final_f_all_strains)\n",
    "            final_f_all_strains = final_f_all_strains[columns_to_keep]\n",
    "            iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().loc[:,['sample', 0.75]]\n",
    "            iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().loc[:,['sample', 0.25]]\n",
    "            final_f_all_strains = final_f_all_strains.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "            if bootstrap_ci:\n",
    "                upper_ci_vec = []\n",
    "                lower_ci_vec = []\n",
    "                \n",
    "                for sample_i in range(final_clusters[i].shape[1]):\n",
    "                    \n",
    "                    sample_name = final_clusters[i].iloc[:,sample_i].name[3]\n",
    "                    snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                    mean_freq_vec = []\n",
    "                    for n in range(bootstrap_N):\n",
    "                        sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                        mean_freq = np.mean(sampled_snv_freqs)\n",
    "                        mean_freq_vec.append(mean_freq)\n",
    "                    upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                    lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                    upper_ci_vec.append(upper_ci)\n",
    "                    lower_ci_vec.append(lower_ci)\n",
    "                    \n",
    "                \n",
    "\n",
    "        else:\n",
    "            final_f_all_strains_temp = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "            final_f_all_strains_temp['species'] = [species]*len(final_f_all_strains_temp)\n",
    "            final_f_all_strains_temp['strain'] = [i + 1]*len(final_f_all_strains_temp)\n",
    "            final_f_all_strains_temp = final_f_all_strains_temp[columns_to_keep]\n",
    "            iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().reset_index().loc[:,['sample', 0.75]]\n",
    "            iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().reset_index().loc[:,['sample', 0.25]]\n",
    "            final_f_all_strains_temp = final_f_all_strains_temp.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "            final_f_all_strains = pd.concat([final_f_all_strains, final_f_all_strains_temp], ignore_index=True)\n",
    "            \n",
    "            if bootstrap_ci:\n",
    "                \n",
    "                for sample_i in range(final_clusters[i].shape[1]): #if we did not infer multiple strains, but there may have been multiple strains in the inoculum, just calculate CI for inoculum\n",
    "             \n",
    "                    snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                    mean_freq_vec = []\n",
    "\n",
    "                    for n in range(bootstrap_N):\n",
    "                        sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                        mean_freq = np.mean(sampled_snv_freqs)\n",
    "                        mean_freq_vec.append(mean_freq)\n",
    "                    upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                    lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                    upper_ci_vec.append(upper_ci)\n",
    "                    lower_ci_vec.append(lower_ci)\n",
    "            \n",
    "            \n",
    "        final_f_all_strains['upper_ci'] = upper_ci_vec\n",
    "        final_f_all_strains['lower_ci'] = lower_ci_vec\n",
    "    # Renaming\n",
    "    final_f_all_strains = final_f_all_strains.rename(columns = {0.25: \"quantile_25\", 0.75: \"quantile_75\"})\n",
    "    # sorting\n",
    "    final_f_all_strains = final_f_all_strains.sort_values(by = [\"strain\", \"cage\", \"mouse_number\"]).reset_index(drop = True)\n",
    "\n",
    "    #Saving \n",
    "    \n",
    "    output_file = \"%sstrain_phasing/strain_clusters/%s/%s_strain_frequency.csv\" % (config.project_directory, species, species)\n",
    "    final_f_all_strains.to_csv(output_file, sep = \"\\t\", index = False)\n",
    "else:\n",
    "    sys.stderr.write(\"Only one strain detected\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6944bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"%s%s\" % (config.metadata_directory, \"species_snps.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c493fd",
   "metadata": {},
   "source": [
    "# For loop through species list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "338966fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 229547 (1 / 117)\n",
      "Using a read support of 4 for each polymorphism.\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "min_cluster_size = 1000\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "min_cluster_fraction = 1/10\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more   \n",
    "max_d = 3.5\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 10 \n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "poly_cov_frac = 1/5 #\n",
    "\n",
    "## Number of clusters to calculate\n",
    "n_clusters = 100\n",
    "\n",
    "#Minimum number of snvs per sample\n",
    "min_num_snvs_per_sample = 100\n",
    "\n",
    "\n",
    "# Load species list\n",
    "species_path = \"%s%s\" % (config.metadata_directory, \"species_snps.txt\")\n",
    "with open(species_path, 'r') as file:\n",
    "    species_list = [line.strip() for line in file]\n",
    "\n",
    "# Load subject list\n",
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "subject_sample_map = parse_subject_sample_map()\n",
    "subjects = subject_sample_map.keys()\n",
    "\n",
    "# for species_i,species in enumerate(species_list):\n",
    "for species_i,species in enumerate([\"229547\"]):\n",
    "    sys.stderr.write(\"Processing %s (%s / %s)\\n\" % (species, species_i + 1, len(species_list)))\n",
    "    # Defining directories\n",
    "\n",
    "    strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "\n",
    "    # Load Fs\n",
    "    snp_alignment_path = \"%s/%s/%s.strainfinder.p\" %  (strainfinder_dir ,species, species)\n",
    "    if os.path.exists(snp_alignment_path):\n",
    "        Fs,Ass,Dss = return_FAD(species, \n",
    "                                min_coverage=min_coverage, \n",
    "                                min_sample_coverage=min_sample_coverage, \n",
    "                                poly_cov_frac = poly_cov_frac, \n",
    "                                calculate_poly_cov_frac=False, \n",
    "                                read_support = True) \n",
    "    else:\n",
    "        sys.stderr.write(\"No SNP alignment file for species. Skipping to next species.\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Filter out samplers without adequate read coverage\n",
    "    sample_with_adequate_snv_count = ~((~np.isnan(Fs)).sum() < min_num_snvs_per_sample)\n",
    "\n",
    "    Fs = Fs.loc[:,sample_with_adequate_snv_count]\n",
    "    Ass = Ass.loc[:,sample_with_adequate_snv_count]\n",
    "    Dss = Dss.loc[:,sample_with_adequate_snv_count]\n",
    "\n",
    "\n",
    "    # Making directories if need be\n",
    "    raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "    species_raw_cluster_dir = \"%s%s/\" % (raw_cluster_path, species)\n",
    "\n",
    "\n",
    "\n",
    "    # Read in raw cluster file\n",
    "    species_raw_cluster_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_RawCluster.pckl\")\n",
    "\n",
    "    if not os.path.exists(species_raw_cluster_path):\n",
    "        sys.stderr.write(f\"\\tFile not found: {species_raw_cluster_path}, making raw cluster.\\n\")\n",
    "        \n",
    "        ## CONSTRUCTION DISTANCE MATRIX\n",
    "        fss = Ass.values/(Dss.values + (Dss.values == 0)) #This is so it doesn't produce a na (division by 0)\n",
    "\n",
    "        cluster_As = Ass.values\n",
    "        cluster_Ds = Dss.values\n",
    "        cluster_fs = cluster_As/(cluster_Ds + (cluster_Ds == 0))\n",
    "\n",
    "        ## for compatibility in case of threshold number of SNVs\n",
    "        num = min(max_num_snvs,Fs.shape[0])\n",
    "\n",
    "        i_list = Dss.T.mean().sort_values(ascending=False).index[:num]\n",
    "\n",
    "        sys.stderr.write(\"Processing %s SNVs\" % num)\n",
    "\n",
    "        ## simply shuffles indices if no threshold is specified\n",
    "        #i_list = sample(range(Fs.shape[0]),num)\n",
    "        i_list_idx = Fs.loc[i_list].index\n",
    "\n",
    "        Ass_sub = Ass.loc[i_list_idx]\n",
    "        Dss_sub = Dss.loc[i_list_idx]\n",
    "        Fs_sub = Fs.loc[i_list_idx]\n",
    "\n",
    "        fss_sub = Ass_sub.values/(Dss_sub.values + (Dss_sub.values == 0))\n",
    "\n",
    "        cluster_As_sub = Ass_sub.values\n",
    "        cluster_Ds_sub = Dss_sub.values\n",
    "        cluster_fs_sub = cluster_As_sub/(cluster_Ds_sub + (cluster_Ds_sub == 0))\n",
    "\n",
    "        D_mat = np.zeros([num,num])\n",
    "        D_mat_1 = D_mat_fun1(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "        D_mat = np.zeros([num,num]) \n",
    "        D_mat_2 = D_mat_fun2(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "\n",
    "        D_mat = np.fmin(D_mat_1,D_mat_2) #I believe this is filling in the minimum of the two polarizations\n",
    "        D_mat = symmetrize(D_mat)\n",
    "\n",
    "        D_mat_1 = pd.DataFrame(D_mat_1,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "        D_mat_2 = pd.DataFrame(D_mat_2,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "\n",
    "        D_mat_close = pd.DataFrame(D_mat < max_d) \n",
    "\n",
    "        D_mat_close.index = Fs_sub.index\n",
    "        D_mat_close.columns = Fs_sub.index\n",
    "        \n",
    "        ## extracts up to 100 clusters\n",
    "        ## in practice all SNVs should fall into one of a fairly small number of clusters\n",
    "        ## really should re-write this with a while loop but this works for now\n",
    "        ## the idea is that we exhaust all clusters—there should only be a small number of them ultimately\n",
    "\n",
    "        ###Idea with while loop:\n",
    "        ##### While there are still variants out there, have it try to be clusterings\n",
    "\n",
    "        all_clus_pol = []\n",
    "        all_clus_idx = []\n",
    "        all_clus_A = []\n",
    "        all_clus_D = []\n",
    "\n",
    "        all_clus_F = []\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "            \n",
    "            try:\n",
    "\n",
    "                clus,clus_idxs = return_clus(D_mat_close,Fs_sub)\n",
    "        #         clus,clus_idxs = return_clus(D_mat_close,Fs_sub, co_cluster_pct=0.5) #Finding points that cluster with 25% other points. That's a cluster.\n",
    "                                                                    #We would modify this function to get smaller clusters...\n",
    "                clus_pol = polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2)\n",
    "                clus_pol.index = clus_idxs\n",
    "                D_mat_close = drop_clus_idxs(D_mat_close,clus_idxs)\n",
    "\n",
    "                if clus_pol.shape[0] > min_cluster_size and clus_pol.shape[0] > Fs.shape[0]*min_cluster_fraction:\n",
    "\n",
    "                    all_clus_D.append(Dss.loc[clus.index].mean().values)\n",
    "                    all_clus_pol.append(clus_pol)\n",
    "                    all_clus_A.append(clus_pol.mean()*all_clus_D[-1])\n",
    "                    all_clus_F.append(clus_pol.mean())\n",
    "\n",
    "                    print(clus_pol.shape[0])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if len(all_clus_D) == 0:\n",
    "            sys.stderr.write(\"No cluster found. Skipping to next species.\\n\")\n",
    "        ## now, choosing a representative SNV from each cluster, and finding all other sites (not just limited to the 20k)\n",
    "        ## which are consistent w/ being linked to it\n",
    "\n",
    "        final_clusters = []\n",
    "\n",
    "        all_aligned_sites = []\n",
    "\n",
    "        for i in range(len(all_clus_D)):\n",
    "            \n",
    "            sys.stderr.write(f'\\n\\nCluster {i+1}\\n')\n",
    "            ancD = all_clus_D[i]\n",
    "            ancF = all_clus_F[i]\n",
    "\n",
    "            dss = Dss.values\n",
    "            fss = Fs.values\n",
    "            \n",
    "            disAnc_forward = []\n",
    "            disAnc_backward = []\n",
    "\n",
    "            for j in range(Dss.shape[0]):\n",
    "                disAnc_forward.append(calc_dis(ancD,dss[j],ancF,fss[j]))\n",
    "                disAnc_backward.append(calc_dis(ancD,dss[j],ancF,1-fss[j]))\n",
    "                if j % 1000 == 0:\n",
    "                    sys.stderr.write(f\"\\n\\t{np.around(100*j/Dss.shape[0],3)}% finished\")\n",
    "            \n",
    "            disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "            disAnc = np.array(disAnc)\n",
    "            aligned_sites = Fs.loc[disAnc < max_d].index\n",
    "            f_dist =  pd.DataFrame(np.array([disAnc_forward,disAnc_backward]).T,index=Fs.index)\n",
    "            pols = f_dist.T.idxmin() > 0\n",
    "            \n",
    "            aligned_sites = [a for a in aligned_sites if a not in all_aligned_sites]\n",
    "            \n",
    "            pols = pols.loc[aligned_sites]\n",
    "            re_polarize = pols.loc[pols].index\n",
    "            \n",
    "            all_aligned_sites.extend(aligned_sites)\n",
    "            \n",
    "            Fs_cluster = Fs.loc[aligned_sites]\n",
    "            \n",
    "            Fs_cluster.loc[re_polarize] = 1 - Fs_cluster.loc[re_polarize]\n",
    "                \n",
    "            final_clusters.append(Fs_cluster)\n",
    "\n",
    "        if len(final_clusters) == 0:\n",
    "            sys.stderr.write(\"No clusters detected. Skipping to next species.\\n\")\n",
    "        else:\n",
    "            if not os.path.exists(species_raw_cluster_dir):\n",
    "                os.makedirs(species_raw_cluster_dir)\n",
    "                print(\"Cluster directory created successfully!\")\n",
    "            else:\n",
    "                print(\"Cluster directory already exists.\")\n",
    "            # #SAVING RAW FILE\n",
    "            pickle_object = open(species_raw_cluster_path, \"wb\")\n",
    "            pickle.dump(final_clusters, pickle_object)\n",
    "            pickle_object.close()\n",
    "\n",
    "    else:\n",
    "        final_clusters = pd.read_pickle(species_raw_cluster_path)\n",
    "\n",
    "    # Polarizing clusters\n",
    "    if len(final_clusters) == 0:\n",
    "        sys.stderr.write(\"\\tNo strains detected, skipping species.\\n\")\n",
    "        continue\n",
    "    \n",
    "    ## If only a single cluster is detected, add a second \"cluster\" which is simply 1 minus the allele frequencies\n",
    "    ## in the first cluster\n",
    "    ## aids in visualization for people not familiar with this kind of clustering\n",
    "    if len(final_clusters) == 1:\n",
    "        final_clusters.append(1-final_clusters[0])\n",
    "\n",
    "    ## add cluster centroids\n",
    "    final_f = []\n",
    "    for cluster in final_clusters:\n",
    "        final_f.append(cluster.mean())\n",
    "    df_final_f = pd.DataFrame(final_f)\n",
    "\n",
    "    ## now, polarize clusters so that the sum of squareds of the centroids to 1 is minimized\n",
    "    ## the idea here is that accurate strain frequencies should sum to 1\n",
    "    polarize = True\n",
    "\n",
    "    pol_d2 = {}\n",
    "\n",
    "    for i in range(df_final_f.shape[0]):\n",
    "        df_final_f_temp = df_final_f.copy() #Makes a copy of the centroids\n",
    "        df_final_f_temp.iloc[i] = 1 - df_final_f_temp.iloc[i] #gets the polarized version of ONE of the centroids.\n",
    "        pol_d2[i] =  ((1 - df_final_f_temp.sum())**2).sum()   #Get the across centroids for all samples (should be close to 1), \n",
    "                                                                    #subtract this from 1, and square. Sum all those values\n",
    "                                                                    #Ideally, this value is really close to 0. \n",
    "                                                                    #Add this value to the dictionary.\n",
    "\n",
    "        pol_d2 = pd.Series(pol_d2)                                #Make the dictionary a series \n",
    "\n",
    "        if pol_d2.min() < ((1 - df_final_f.sum())**2).sum(): #If any of the above repolarizations actually made the overall sum of centroids closer to 1, repolarize.\n",
    "            clus_to_re_pol = pol_d2.idxmin()\n",
    "            final_f[clus_to_re_pol] = 1 - final_f[clus_to_re_pol]\n",
    "            final_clusters[clus_to_re_pol] = 1 - final_clusters[clus_to_re_pol]\n",
    "            df_final_f = pd.DataFrame(final_f)  \n",
    "\n",
    "    # Filtering out samples without adequate snvs after polarization step\n",
    "    good_indices = []\n",
    "\n",
    "    for i,cluster in enumerate(final_clusters):\n",
    "        if i == 0:\n",
    "            good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "        else:\n",
    "            new_good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "            good_samples = good_samples & new_good_samples\n",
    "\n",
    "    if sum(good_samples == False) == len(good_samples): # if no sample is \"good\" across all clusters, then skip to the next species\n",
    "        sys.stderr.write(\"\\tNo good samples after filter, skipping species.\\n\")\n",
    "        continue\n",
    "\n",
    "    for i,cluster in enumerate(final_clusters): \n",
    "        final_clusters[i] = final_clusters[i].T.loc[good_samples].T\n",
    "        final_f[i] = final_f[i].T.loc[good_samples]\n",
    "\n",
    "    Fs = Fs.T.loc[good_samples].T\n",
    "    Ass = Ass.T.loc[good_samples].T\n",
    "    Dss = Dss.T.loc[good_samples].T\n",
    "\n",
    "    # Filtering out columsn that have only na (redundant)\n",
    "    #Filter all all na columns if there are any - THis is redundant\n",
    "    if len(final_clusters) > 0:\n",
    "        for i,cluster in enumerate(final_clusters):\n",
    "            if i == 0:\n",
    "                mask = ~np.isnan(cluster).all(axis = 0)\n",
    "            final_clusters[i] = cluster.loc[:,mask]\n",
    "            final_f[i] = final_f[i][mask]\n",
    "        # Fs = Fs.loc[:,mask]\n",
    "        Fs = Fs.loc[:, Fs.columns[mask]]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Bootstrapping CI and outputting dataframe\n",
    "    all_strains = True\n",
    "    bootstrap_ci = True\n",
    "    boostrap_k = 100\n",
    "    bootstrap_N = 1000\n",
    "    np.random.seed(6)\n",
    "    columns_to_keep = ['sample',\n",
    "                    'species', \n",
    "                    'strain', \n",
    "                    'cage', \n",
    "                    'mouse_number', \n",
    "                    'region', \n",
    "                    'freq']\n",
    "    if len(final_clusters) > 0:\n",
    "        for i in np.arange(len(final_f)): \n",
    "            if i == 0:\n",
    "                final_f_all_strains = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "                final_f_all_strains['species'] = [species]*len(final_f_all_strains)\n",
    "                final_f_all_strains['strain'] = [i + 1]*len(final_f_all_strains)\n",
    "                final_f_all_strains = final_f_all_strains[columns_to_keep]\n",
    "                iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().loc[:,['sample', 0.75]]\n",
    "                iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().loc[:,['sample', 0.25]]\n",
    "                final_f_all_strains = final_f_all_strains.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "                if bootstrap_ci:\n",
    "                    upper_ci_vec = []\n",
    "                    lower_ci_vec = []\n",
    "                    \n",
    "                    for sample_i in range(final_clusters[i].shape[1]):\n",
    "                        \n",
    "                        sample_name = final_clusters[i].iloc[:,sample_i].name[3]\n",
    "                        snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                        mean_freq_vec = []\n",
    "                        for n in range(bootstrap_N):\n",
    "                            sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                            mean_freq = np.mean(sampled_snv_freqs)\n",
    "                            mean_freq_vec.append(mean_freq)\n",
    "                        upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                        lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                        upper_ci_vec.append(upper_ci)\n",
    "                        lower_ci_vec.append(lower_ci)\n",
    "                        \n",
    "                    \n",
    "\n",
    "            else:\n",
    "                final_f_all_strains_temp = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "                final_f_all_strains_temp['species'] = [species]*len(final_f_all_strains_temp)\n",
    "                final_f_all_strains_temp['strain'] = [i + 1]*len(final_f_all_strains_temp)\n",
    "                final_f_all_strains_temp = final_f_all_strains_temp[columns_to_keep]\n",
    "                iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().reset_index().loc[:,['sample', 0.75]]\n",
    "                iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().reset_index().loc[:,['sample', 0.25]]\n",
    "                final_f_all_strains_temp = final_f_all_strains_temp.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "                final_f_all_strains = pd.concat([final_f_all_strains, final_f_all_strains_temp], ignore_index=True)\n",
    "                \n",
    "                if bootstrap_ci:\n",
    "                    \n",
    "                    for sample_i in range(final_clusters[i].shape[1]): #if we did not infer multiple strains, but there may have been multiple strains in the inoculum, just calculate CI for inoculum\n",
    "                \n",
    "                        snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                        mean_freq_vec = []\n",
    "\n",
    "                        for n in range(bootstrap_N):\n",
    "                            sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                            mean_freq = np.mean(sampled_snv_freqs)\n",
    "                            mean_freq_vec.append(mean_freq)\n",
    "                        upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                        lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                        upper_ci_vec.append(upper_ci)\n",
    "                        lower_ci_vec.append(lower_ci)\n",
    "                \n",
    "                \n",
    "            final_f_all_strains['upper_ci'] = upper_ci_vec\n",
    "            final_f_all_strains['lower_ci'] = lower_ci_vec\n",
    "        # Renaming\n",
    "        final_f_all_strains = final_f_all_strains.rename(columns = {0.25: \"quantile_25\", 0.75: \"quantile_75\"})\n",
    "        # sorting\n",
    "        final_f_all_strains = final_f_all_strains.sort_values(by = [\"strain\", \"cage\", \"mouse_number\"]).reset_index(drop = True)\n",
    "\n",
    "        #Saving \n",
    "        \n",
    "        output_file = \"%sstrain_phasing/strain_clusters/%s/%s_strain_frequency.csv\" % (config.project_directory, species, species)\n",
    "        final_f_all_strains.to_csv(output_file, sep = \"\\t\", index = False)\n",
    "    \n",
    "    else:\n",
    "        sys.stderr.write(\"\\tOnly one strain detected\\n\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
