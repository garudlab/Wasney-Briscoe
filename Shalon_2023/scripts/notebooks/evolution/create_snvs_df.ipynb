{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/u/home/m/michaelw/project-ngarud/microbiome_evolution/microbiome_evolution_SHALON/\")\n",
    "\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "\n",
    "import bz2\n",
    "\n",
    "#MIDAS postprocessing scripts\n",
    "from calculate_intersample_changes import *\n",
    "import parse_midas_data\n",
    "import diversity_utils\n",
    "import core_gene_utils\n",
    "import parse_patric\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "within_host_changes_condition = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/metadata/species_snps.txt\"\n",
    "\n",
    "with open(species_list_path) as f:\n",
    "    species_list = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acidaminococcus_intestini_54097',\n",
       " 'Actinomyces_graevenitzii_58300',\n",
       " 'Actinomyces_sp_57735',\n",
       " 'Actinomyces_sp_62581',\n",
       " 'Actinomyces_viscosus_57672',\n",
       " 'Adlercreutzia_equolifaciens_60310',\n",
       " 'Aggregatibacter_aphrophilus_58143',\n",
       " 'Akkermansia_muciniphila_55290',\n",
       " 'Alistipes_finegoldii_56071',\n",
       " 'Alistipes_indistinctus_62207']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_species = []\n",
    "\n",
    "for species in species_list:\n",
    "    haploid_samples = diversity_utils.calculate_haploid_samples(species)\n",
    "    if len(haploid_samples) > 1:\n",
    "        good_species.append(species)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acidaminococcus_intestini_54097',\n",
       " 'Akkermansia_muciniphila_55290',\n",
       " 'Alistipes_finegoldii_56071',\n",
       " 'Alistipes_onderdonkii_55464',\n",
       " 'Alistipes_putredinis_61533']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_species[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Bacteroides_massiliensis_44749\"\n",
    "haploid_samples = diversity_utils.calculate_haploid_samples(species)\n",
    "haploid_samples_hmp = diversity_utils.calculate_haploid_samples(species, use_HMP_freqs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to capture within host SNV changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SRR18585014', 'SRR18585015', 'SRR18585017', 'SRR18585018',\n",
       "       'SRR18585019', 'SRR18585020', 'SRR18585021', 'SRR18585023',\n",
       "       'SRR18585022', 'SRR18585024', 'SRR18585025', 'SRR18585026',\n",
       "       'SRR18585028', 'SRR18585172', 'SRR18585174', 'SRR18585176'],\n",
       "      dtype='|S11')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haploid_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Bacteroides_massiliensis_44749 (1/69 species)\n"
     ]
    }
   ],
   "source": [
    "subject_sample_map = parse_midas_data.parse_subject_sample_map()\n",
    "sample_metadata_map = parse_midas_data.parse_sample_order_map()\n",
    "\n",
    "#Initialize df\n",
    "snp_changes_df = pd.DataFrame(columns = ['species','contig','site_pos','gene','variant_type','sample1', 'sample2', 'alternate_freq_1', 'depth_1', 'alternate_freq_2', 'depth_2'])\n",
    "accession_1 = []\n",
    "accession_2 = []\n",
    "species_vec = []\n",
    "contig = []\n",
    "site_pos = []\n",
    "gene = []\n",
    "variant_type = []\n",
    "sample1 = []\n",
    "sample2 = []\n",
    "alternate_freq_1 = []\n",
    "depth_1 = []\n",
    "alternate_freq_2 = []\n",
    "depth_2 = []\n",
    "opportunities_vec = []\n",
    "gene_description_vec = []\n",
    "\n",
    "\n",
    "counter = 0\n",
    "no_of_species = len(good_species)\n",
    "species = \"Bacteroides_massiliensis_44749\"\n",
    "counter += 1\n",
    "print \"%s%s (%s/%s species)\" % (\"Processing \", species, counter, no_of_species)\n",
    "#gene descriptions\n",
    "genome_ids = parse_midas_data.get_ref_genome_ids(species)\n",
    "non_shared_genes = core_gene_utils.parse_non_shared_reference_genes(species)\n",
    "gene_descriptions = parse_patric.load_patric_gene_descriptions(genome_ids, non_shared_genes)\n",
    "centroid_gene_map = parse_midas_data.load_centroid_gene_map(species)\n",
    "#haploid samples\n",
    "haploid_samples = diversity_utils.calculate_haploid_samples(species, use_HMP_freqs = True)\n",
    "#change map\n",
    "intersample_change_map = load_intersample_change_map(species)\n",
    "intersample_change_map_within_host = {}\n",
    "\n",
    "if within_host_changes_condition:\n",
    "\n",
    "    for subject in subject_sample_map:\n",
    "        host_samples = subject_sample_map[subject].keys()\n",
    "        within_host_sample_combos = list(itertools.combinations(host_samples, 2))\n",
    "\n",
    "        for sample_combo in within_host_sample_combos:\n",
    "            if sample_combo in intersample_change_map:\n",
    "                intersample_change_map_within_host[sample_combo] = intersample_change_map[sample_combo]\n",
    "            elif (sample_combo[1], sample_combo[0]) in within_host_sample_combos:\n",
    "                sample_combo = (sample_combo[1], sample_combo[0])\n",
    "                intersample_change_map_within_host[sample_combo] = intersample_change_map[sample_combo]\n",
    "            else: \n",
    "                continue\n",
    "    intersample_change_map = intersample_change_map_within_host\n",
    "    \n",
    "#Intersample change map subsetting\n",
    "#     ICMs = {key: intersample_change_map[key]['snps'] for key in intersample_change_map.keys() if len(intersample_change_map[key]['snps'][2]) > 0} #do we actually want this?\n",
    "ICMs = {key: intersample_change_map[key]['snps'] for key in intersample_change_map.keys()}\n",
    "ICMs = {key: ICMs[key] for key in ICMs.keys() if ((key[0] in haploid_samples) & (key[1] in haploid_samples))}\n",
    "\n",
    "if len(ICMs) != 0:\n",
    "    for key in ICMs.keys():\n",
    "        for snp in ICMs[key][2]:\n",
    "            accession_1.append(key[0])\n",
    "            accession_2.append(key[1])\n",
    "            species_vec.append(species)\n",
    "            contig.append(snp[1])\n",
    "            site_pos.append(snp[2])\n",
    "            gene.append(snp[0])\n",
    "            variant_type.append(snp[3])\n",
    "            sample1.append(key[0])\n",
    "            sample2.append(key[1])\n",
    "            alternate_freq_1.append(snp[4])\n",
    "            depth_1.append(snp[5])\n",
    "            alternate_freq_2.append(snp[6])\n",
    "            depth_2.append(snp[7])\n",
    "            if snp[0] in gene_descriptions:\n",
    "                gene_description_vec.append(gene_descriptions[snp[0]])\n",
    "            elif snp[0] in centroid_gene_map:\n",
    "                if centroid_gene_map[snp[0]] in gene_descriptions:\n",
    "                    gene_description_vec.append(gene_descriptions[centroid_gene_map[snp[0]]])\n",
    "            else:\n",
    "                gene_description_vec.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICMs = {key: intersample_change_map[key]['snps'] for key in intersample_change_map.keys()}\n",
    "ICMs = {key: ICMs[key] for key in ICMs.keys() if ((key[0] in haploid_samples) & (key[1] in haploid_samples))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('SRR18585014', 'SRR18585015'): (2427270.0, 6.93445e-10, []),\n",
       " ('SRR18585014', 'SRR18585017'): (2431730.0, 4.92197e-11, []),\n",
       " ('SRR18585015', 'SRR18585017'): (2423810.0, 5.27426e-10, []),\n",
       " ('SRR18585018', 'SRR18585019'): (2350560.0, 8.02574e-08, []),\n",
       " ('SRR18585020', 'SRR18585022'): (2405880.0, 1.12478e-21, []),\n",
       " ('SRR18585020', 'SRR18585023'): (2406250.0, 1.62153e-22, []),\n",
       " ('SRR18585021', 'SRR18585022'): (2430200.0, 1.21175e-17, []),\n",
       " ('SRR18585021', 'SRR18585023'): (2430510.0, 7.58251e-18, []),\n",
       " ('SRR18585024', 'SRR18585026'): (2433280.0, 4.76305e-16, []),\n",
       " ('SRR18585025', 'SRR18585026'): (2415330.0, 1.68232e-10, [])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_sample_map = parse_midas_data.parse_subject_sample_map()\n",
    "sample_metadata_map = parse_midas_data.parse_sample_order_map()\n",
    "\n",
    "#Initialize df\n",
    "snp_changes_df = pd.DataFrame(columns = ['species','contig','site_pos','gene','variant_type','sample1', 'sample2', 'alternate_freq_1', 'depth_1', 'alternate_freq_2', 'depth_2'])\n",
    "accession_1 = []\n",
    "accession_2 = []\n",
    "species_vec = []\n",
    "contig = []\n",
    "site_pos = []\n",
    "gene = []\n",
    "variant_type = []\n",
    "sample1 = []\n",
    "sample2 = []\n",
    "alternate_freq_1 = []\n",
    "depth_1 = []\n",
    "alternate_freq_2 = []\n",
    "depth_2 = []\n",
    "opportunities_vec = []\n",
    "gene_description_vec = []\n",
    "\n",
    "counter = 0\n",
    "no_of_species = len(good_species)\n",
    "\n",
    "for species in good_species:\n",
    "    counter += 1\n",
    "    \n",
    "    print \"%s%s (%s/%s species)\" % (\"Processing \", species, counter, no_of_species)\n",
    "    \n",
    "    #gene descriptions\n",
    "    genome_ids = parse_midas_data.get_ref_genome_ids(species)\n",
    "    non_shared_genes = core_gene_utils.parse_non_shared_reference_genes(species)\n",
    "    gene_descriptions = parse_patric.load_patric_gene_descriptions(genome_ids, non_shared_genes)\n",
    "    centroid_gene_map = parse_midas_data.load_centroid_gene_map(species)\n",
    "    \n",
    "    #haploid samples\n",
    "    haploid_samples = diversity_utils.calculate_haploid_samples(species, use_HMP_freqs = True)\n",
    "    \n",
    "    #change map\n",
    "    intersample_change_map = load_intersample_change_map(species)\n",
    "\n",
    "    intersample_change_map_within_host = {}\n",
    "    \n",
    "    if within_host_changes_condition:\n",
    "\n",
    "        for subject in subject_sample_map:\n",
    "            host_samples = subject_sample_map[subject].keys()\n",
    "            within_host_sample_combos = list(itertools.combinations(host_samples, 2))\n",
    "\n",
    "            for sample_combo in within_host_sample_combos:\n",
    "                if sample_combo in intersample_change_map:\n",
    "                    intersample_change_map_within_host[sample_combo] = intersample_change_map[sample_combo]\n",
    "                elif (sample_combo[1], sample_combo[0]) in within_host_sample_combos:\n",
    "                    sample_combo = (sample_combo[1], sample_combo[0])\n",
    "                    intersample_change_map_within_host[sample_combo] = intersample_change_map[sample_combo]\n",
    "                else: \n",
    "                    continue\n",
    "\n",
    "        intersample_change_map = intersample_change_map_within_host\n",
    "    \n",
    "    #Intersample change map subsetting\n",
    "#     ICMs = {key: intersample_change_map[key]['snps'] for key in intersample_change_map.keys() if len(intersample_change_map[key]['snps'][2]) > 0} #do we actually want this?\n",
    "    ICMs = {key: intersample_change_map[key]['snps'] for key in intersample_change_map.keys()}\n",
    "    ICMs = {key: ICMs[key] for key in ICMs.keys() if ((key[0] in haploid_samples) & (key[1] in haploid_samples))}\n",
    "    \n",
    "    \n",
    "    if len(ICMs) != 0:\n",
    "        for key in ICMs.keys():\n",
    "            for snp in ICMs[key][2]:\n",
    "                accession_1.append(key[0])\n",
    "                accession_2.append(key[1])\n",
    "                species_vec.append(species)\n",
    "                contig.append(snp[1])\n",
    "                site_pos.append(snp[2])\n",
    "                gene.append(snp[0])\n",
    "                variant_type.append(snp[3])\n",
    "                sample1.append(key[0])\n",
    "                sample2.append(key[1])\n",
    "                alternate_freq_1.append(snp[4])\n",
    "                depth_1.append(snp[5])\n",
    "                alternate_freq_2.append(snp[6])\n",
    "                depth_2.append(snp[7])\n",
    "                if snp[0] in gene_descriptions:\n",
    "                    gene_description_vec.append(gene_descriptions[snp[0]])\n",
    "                elif snp[0] in centroid_gene_map:\n",
    "                    if centroid_gene_map[snp[0]] in gene_descriptions:\n",
    "                        gene_description_vec.append(gene_descriptions[centroid_gene_map[snp[0]]])\n",
    "                else:\n",
    "                    gene_description_vec.append(\"\")\n",
    "                \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "snp_changes_df['accession_1'] = accession_1\n",
    "snp_changes_df['accession_2'] = accession_2\n",
    "snp_changes_df['species'] = species_vec\n",
    "snp_changes_df['contig'] = contig\n",
    "snp_changes_df['site_pos'] = site_pos\n",
    "snp_changes_df['gene'] = gene\n",
    "snp_changes_df['variant_type'] = variant_type\n",
    "snp_changes_df['sample1'] = sample1\n",
    "snp_changes_df['sample2'] = sample2\n",
    "snp_changes_df['alternate_freq_1'] = alternate_freq_1\n",
    "snp_changes_df['depth_1'] = depth_1\n",
    "snp_changes_df['alternate_freq_2'] = alternate_freq_2\n",
    "snp_changes_df['depth_2'] = depth_2\n",
    "snp_changes_df['gene_description'] = gene_description_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate snp_changes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map = parse_midas_data.parse_sample_metadata_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if within_host_changes:\n",
    "    snp_changes_df['subject'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][0])\n",
    "else:\n",
    "    snp_changes_df['subject_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][0])\n",
    "    snp_changes_df['subject_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][0])\n",
    "\n",
    "snp_changes_df['type_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][2])\n",
    "snp_changes_df['type_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][2])\n",
    "\n",
    "snp_changes_df['swallow_date_time_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][3])\n",
    "snp_changes_df['swallow_date_time_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][3])\n",
    "\n",
    "snp_changes_df['sample_set_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][8])\n",
    "snp_changes_df['sample_set_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][8])\n",
    "\n",
    "snp_changes_df['sample_type_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][9])\n",
    "snp_changes_df['sample_type_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][9])\n",
    "\n",
    "snp_changes_df['location_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][10])\n",
    "snp_changes_df['location_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][10])\n",
    "\n",
    "snp_changes_df['day_1'] = snp_changes_df.sample1.apply(lambda sample: metadata_map[sample][11])\n",
    "snp_changes_df['day_2'] = snp_changes_df.sample2.apply(lambda sample: metadata_map[sample][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving full snv dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_folder = \"%s%s\" % (config.project_directory, \"evolutionary_changes/\")\n",
    "if not os.path.exists(evolution_folder):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(evolution_folder)\n",
    "    print(\"Directory '{}' created successfully.\".format(evolution_folder))\n",
    "else:\n",
    "    print(\"Directory '{}' already exists.\".format(evolution_folder))\n",
    "if within_host_changes_condition:\n",
    "    snv_changes_df_output = \"%s%s\" % (evolution_folder, \"snp_changes_WithinHost.txt.bz2\")\n",
    "else:\n",
    "    snv_changes_df_output = \"%s%s\" % (evolution_folder, \"snp_changes.txt.bz2\")\n",
    "snp_changes_df.to_csv(snv_changes_df_output, index = False, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading snv dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_folder = \"%s%s\" % (config.project_directory, \"evolutionary_changes/\")\n",
    "if within_host_changes_condition:\n",
    "    snv_changes_df_output = \"%s%s\" % (evolution_folder, \"snp_changes_WithinHost.txt.bz2\")\n",
    "else:\n",
    "    snv_changes_df_output = \"%s%s\" % (evolution_folder, \"snp_changes.txt.bz2\")\n",
    "snp_changes_df = pd.read_csv(snv_changes_df_output, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the opportunities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map = parse_midas_data.parse_sample_metadata_map()\n",
    "opportunities_df = pd.DataFrame(columns = ['species', 'accession_1', 'accession_2', 'opportunities'])\n",
    "\n",
    "species_vec = []\n",
    "sample_1 = []\n",
    "sample_2 = []\n",
    "opportunity_vec = []\n",
    "\n",
    "for species in good_species:\n",
    "    print \"Processing \" + species\n",
    "    intersample_change_map = load_intersample_change_map(species)\n",
    "    \n",
    "    haploid_samples = diversity_utils.calculate_haploid_samples(species, use_HMP = True) #MW: 06/08: Changed to true\n",
    "    sample_pairs = list(itertools.combinations(haploid_samples, 2))\n",
    "\n",
    "\n",
    "    for sample_pair in sample_pairs:\n",
    "        if sample_pair in intersample_change_map:\n",
    "            opportunities = intersample_change_map[sample_pair]['snps'][0]\n",
    "        elif (sample_pair[1], sample_pair[0]) in intersample_change_map:\n",
    "            sample_pair = (sample_pair[1], sample_pair[0])\n",
    "            opportunities = intersample_change_map[sample_pair]['snps'][0]\n",
    "        else: \n",
    "            print \"Error: \" + str(sample_pair) + \"not found.\"\n",
    "            continue\n",
    "\n",
    "        species_vec.append(species)\n",
    "        sample_1.append(sample_pair[0])\n",
    "        sample_2.append(sample_pair[1])\n",
    "        opportunity_vec.append(opportunities)\n",
    "        #snv_changes\n",
    "\n",
    "opportunities_df['species'] = species_vec\n",
    "opportunities_df['accession_1'] = sample_1\n",
    "opportunities_df['accession_2'] = sample_2\n",
    "opportunities_df['opportunities'] = opportunity_vec\n",
    "\n",
    "#Annotation\n",
    "opportunities_df['subject_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][0])\n",
    "opportunities_df['subject_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][0])\n",
    "\n",
    "opportunities_df['type_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][2])\n",
    "opportunities_df['type_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][2])\n",
    "\n",
    "opportunities_df['swallow_date_time_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][3])\n",
    "opportunities_df['swallow_date_time_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][3])\n",
    "\n",
    "opportunities_df['sample_set_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][8])\n",
    "opportunities_df['sample_set_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][8])\n",
    "\n",
    "opportunities_df['sample_type_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][9])\n",
    "opportunities_df['sample_type_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][9])\n",
    "\n",
    "opportunities_df['location_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][10])\n",
    "opportunities_df['location_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][10])\n",
    "\n",
    "opportunities_df['day_1'] = opportunities_df.accession_1.apply(lambda sample: metadata_map[sample][11])\n",
    "opportunities_df['day_2'] = opportunities_df.accession_2.apply(lambda sample: metadata_map[sample][11])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving opportunities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_folder = \"%s%s\" % (config.project_directory, \"evolutionary_changes/\")\n",
    "if not os.path.exists(evolution_folder):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(evolution_folder)\n",
    "    print(\"Directory '{}' created successfully.\".format(evolution_folder))\n",
    "else:\n",
    "    print(\"Directory '{}' already exists.\".format(evolution_folder))\n",
    "# if within_host_changes:\n",
    "#     snv_changes_df_output = \"%s%s\" % (evolution_folder, \"snp_changes_WithinHost.txt.bz2\")\n",
    "# else:\n",
    "#     snv_changes_df_output = \"%s%s\" % (evolution_folder, \"snp_changes.txt.bz2\")\n",
    "opportunities_df_output = \"%s%s\" % (evolution_folder, \"opportunities.txt.bz2\")\n",
    "opportunities_df.to_csv(opportunities_df_output, index = False, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading opportunities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_folder = \"%s%s\" % (config.project_directory, \"evolutionary_changes/\")\n",
    "opportunities_df_output = \"%s%s\" % (evolution_folder, \"opportunities.txt.bz2\")\n",
    "opportunities_df = pd.read_csv(opportunities_df_output, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_1 = (snp_changes_df.sample_type_1 == \"Capsule\") & (snp_changes_df.sample_type_2 == \"Capsule\")\n",
    "condition_2 = (snp_changes_df.sample_set_1 == snp_changes_df.sample_set_2)\n",
    "condition_3 = [True if sample_set in [\"1\",\"2\",\"3\",\"4\",\"5\"] else False for sample_set in snp_changes_df.sample_set_1]\n",
    "\n",
    "within_set_changes = snp_changes_df[condition_1 & condition_2].sort_values(by = ['species', 'contig','gene'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within host data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_changes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_1 = (snp_changes_df.sample_type_1 == \"Capsule\") & (snp_changes_df.sample_type_2 == \"Capsule\")\n",
    "\n",
    "within_host_changes = snp_changes_df[condition_1].sort_values(by = ['species', 'contig','gene'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME ANALYSIS - WITHIN SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject 9\n",
    "**Species:** Desulfovibrio_piger_61475\n",
    "- **Gene of interest:** 411464.8.peg.774\n",
    "- **Annotation:** Formate dehydrogenase O alpha subunit (EC 1.2.1.2) @ selenocysteine-containing\n",
    "\n",
    "## Subject 11\n",
    "**Species:** Bacteroides_vulgatus_57955\n",
    "- **Region of interest:** NC_009614, 363467 - 5148355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Set variables\n",
    "species = \"Anaerostipes_hadrus_55206\"\n",
    "subject = \"8\"\n",
    "subject_integer = int(subject)\n",
    "##Extract haploid subject samples\n",
    "subject_samples = parse_midas_data.parse_subject_sample_map()[subject].keys()\n",
    "haploid_samples = diversity_utils.calculate_haploid_samples(species, min_coverage=10)\n",
    "subject_samples = [sample for sample in subject_samples if sample in haploid_samples]\n",
    "##Make a metadataframe\n",
    "metadata_map = parse_midas_data.parse_sample_metadata_map()\n",
    "subject_sample_metadata = pd.DataFrame()\n",
    "subject_sample_metadata['sample'] = subject_samples\n",
    "subject_sample_metadata['sample_type'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][9])\n",
    "subject_sample_metadata['sample_set'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][8])\n",
    "subject_sample_metadata['location'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Extract contig and locus information \n",
    "loci_of_interest = set(within_set_changes[within_set_changes['subject'] == subject].sort_values(by = ['site_pos'])[['contig', 'site_pos']].drop_duplicates().to_records(index=False).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Pull out allele frequencies of variants of interest in subject samples\n",
    "\n",
    "\n",
    "## Load the bz2 file\n",
    "data_directory = config.data_directory\n",
    "snp_file =  bz2.BZ2File(\"%ssnps/%s/annotated_snps.txt.bz2\" % (data_directory, species),\"r\")\n",
    "\n",
    "## Setting up the loop\n",
    "num_sites_processed = 0\n",
    "num_extracted_sites = 0\n",
    "line_number = -1\n",
    "final_line_number = -1\n",
    "initial_line_number = -100\n",
    "previous_gene_name = \"\"\n",
    "gene_name = \"\"\n",
    "chunk_size = 20000000\n",
    "\n",
    "## Header info\n",
    "line = snp_file.readline() \n",
    "items = line.split()[1:]    \n",
    "samples_in_file = sample_utils.parse_merged_sample_names(items)\n",
    "\n",
    "# Sample indices\n",
    "desired_sample_idxs = []\n",
    "for sample in subject_samples:\n",
    "    desired_sample_idxs.append( numpy.nonzero(samples_in_file==sample)[0][0] )\n",
    "desired_sample_idxs = numpy.array(desired_sample_idxs)    \n",
    "desired_samples = samples_in_file[desired_sample_idxs]\n",
    "\n",
    "## Initializing\n",
    "chrom_vec = []\n",
    "location_vec = []\n",
    "loci_extracted = 0\n",
    "\n",
    "#pd.DataFrame(columns = desired_samples)\n",
    "\n",
    "for line in snp_file: \n",
    "    \n",
    "    line_number += 1\n",
    "    \n",
    "    previous_gene_name = gene_name\n",
    "    \n",
    "    if line_number%100000==0:\n",
    "        sys.stderr.write(\"%dk sites processed...\\n\" % (line_number/1000)) \n",
    "    \n",
    "    items = line.split()\n",
    "    # Load information about site\n",
    "    info_items = items[0].split(\"|\")\n",
    "    chromosome = info_items[0]\n",
    "    location = long(info_items[1])\n",
    "    gene_name = info_items[2]\n",
    "    variant_type = info_items[3]\n",
    "    location_tuple = (chromosome, int(location))\n",
    "    #If it's not one of the sites of interest, move to the next line\n",
    "    \n",
    "    if line_number >= chunk_size and gene_name!=previous_gene_name:\n",
    "        # We are done for now!\n",
    "        final_line_number = line_number\n",
    "        sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \"\\n\")\n",
    "        break\n",
    "        \n",
    "    if location_tuple not in loci_of_interest:\n",
    "        continue\n",
    "    else:\n",
    "        sys.stderr.write(\"EXTRACTING: \" + str(location_tuple) + \"\\n\")\n",
    "        chrom_vec.append(chromosome)\n",
    "        location_vec.append(location)\n",
    "        loci_extracted += 1\n",
    "    \n",
    "    ## Getting alts and depths for sites of interest\n",
    "    alts = []\n",
    "    depths = []\n",
    "\n",
    "    for idx in desired_sample_idxs:    \n",
    "        item = items[1+idx]\n",
    "        subitems = item.split(\",\")\n",
    "        alts.append(float(subitems[0]))\n",
    "        depths.append(float(subitems[1]))\n",
    "    \n",
    "    if loci_extracted == 1:\n",
    "        alts_total = numpy.array(alts)\n",
    "        depths_total = numpy.array(depths)\n",
    "    else:\n",
    "        alts_total = numpy.vstack((alts_total, numpy.array(alts)))\n",
    "        depths_total = numpy.vstack((depths_total, numpy.array(depths)))\n",
    "    \n",
    "    \n",
    "    num_sites_processed += 1\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "snp_file.close()\n",
    "\n",
    "## creating dataframe with alts, depths, and freqs for each sample\n",
    "alts_df = pd.DataFrame(data = alts_total, columns = desired_samples)\n",
    "depths_df = pd.DataFrame(data = depths_total, columns = desired_samples)\n",
    "alts_df['contig'] = chrom_vec\n",
    "alts_df['site_pos'] = location_vec\n",
    "depths_df['contig'] = chrom_vec\n",
    "depths_df['site_pos'] = location_vec\n",
    "alts_df = alts_df.melt(var_name='sample', value_name='alt', id_vars = ['contig', 'site_pos'])\n",
    "depths_df = depths_df.melt(var_name='sample', value_name='depth', id_vars = ['contig', 'site_pos'])\n",
    "freq_df = pd.merge(alts_df, depths_df, on=['sample', 'site_pos', 'contig'])\n",
    "freq_df['allele_frequency'] = freq_df['alt']/freq_df['depth']\n",
    "## Merge with subject_sample map\n",
    "freq_df = pd.merge(freq_df, subject_sample_metadata, on = 'sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Repolarize\n",
    "polarize = True\n",
    "\n",
    "if species == \"Desulfovibrio_piger_61475\":\n",
    "    if polarize:\n",
    "        sites_to_repolarize = [147085, 147116]\n",
    "        freq_df['polarized_af'] = freq_df.apply(lambda row: 1-row['allele_frequency'] if row.site_pos in sites_to_repolarize else row['allele_frequency'], axis = 1)\n",
    "\n",
    "if species == \"Bacteroides_vulgatus_57955\":\n",
    "    if polarize:\n",
    "        sites_to_repolarize = freq_df[(freq_df.location == \"Small intestine 2\") & (freq_df.sample_set == \"3\") & (freq_df.allele_frequency < 0.4)].site_pos.values.tolist()\n",
    "        freq_df['polarized_af'] = freq_df.apply(lambda row: 1-row['allele_frequency'] if row.site_pos in sites_to_repolarize else row['allele_frequency'], axis = 1)\n",
    "\n",
    "if species == \"Anaerostipes_hadrus_55206\":\n",
    "    if polarize:\n",
    "        sites_to_repolarize = freq_df[(freq_df.location == \"Ascending colon\") & (freq_df.sample_set == \"3\") & (freq_df.allele_frequency < 0.6)].site_pos.values.tolist()\n",
    "        freq_df['polarized_af'] = freq_df.apply(lambda row: 1-row['allele_frequency'] if row.site_pos in sites_to_repolarize else row['allele_frequency'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Saving\n",
    "output_path = \"%sevolutionary_changes/%s_withinsetchanges.csv\" % (config.project_directory, species)\n",
    "freq_df.to_csv(output_path, index = False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Plotting\n",
    "sns.set(font_scales = 5)\n",
    "\n",
    "plotting_df = freq_df[freq_df.sample_set != \"5\"]\n",
    "\n",
    "g = sns.FacetGrid(data=plotting_df, col = \"sample_set\", sharex=False, height = 10, aspect=1, legend_out=True)\n",
    "g.map(sns.lineplot, 'location', 'polarized_af', 'site_pos',palette=sns.color_palette(\"Set1\", plotting_df.site_pos.nunique()))\n",
    "g.set_titles(fontsize=20)\n",
    "\n",
    "# Set labels and title\n",
    "g.set_axis_labels(\"Location\", \"Allele Frequency\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Plotting individual sample sets\n",
    "plotting_df = freq_df[freq_df.sample_set == \"3\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "# g = sns.FacetGrid(data=plotting_df, col = \"sample_set\")\n",
    "# g.map(sns.lineplot, 'location', 'allele_frequency', 'site_pos',palette=sns.color_palette(\"Set1\", plotting_df.site_pos.nunique()))\n",
    "\n",
    "sns.lineplot(data=plotting_df, x='location', y='allele_frequency', hue='site_pos',palette=sns.color_palette(\"Set1\", plotting_df.site_pos.nunique()), ax = ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME ANALYSIS - WITHIN HOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Identify species that display within host changes\n",
    "within_host_species = within_host_changes.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Identify subjects in which species displays evolutionary changes\n",
    "species = \"Bacteroides_vulgatus_57955\"\n",
    "subjects = list(within_host_changes[(within_host_changes['species'] == species)].subject.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. For the subject in the current loop, pull out relevant samples and create a metadata table\n",
    "subject = \"8\"\n",
    "subject_integer = int(subject)\n",
    "##Extract haploid subject samples\n",
    "subject_samples = parse_midas_data.parse_subject_sample_map()[subject].keys()\n",
    "haploid_samples = diversity_utils.calculate_haploid_samples(species, min_coverage=10)\n",
    "subject_samples = [sample for sample in subject_samples if sample in haploid_samples]\n",
    "##Make a metadataframe\n",
    "metadata_map = parse_midas_data.parse_sample_metadata_map()\n",
    "subject_sample_metadata = pd.DataFrame()\n",
    "subject_sample_metadata['sample'] = subject_samples\n",
    "subject_sample_metadata['sample_type'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][9])\n",
    "subject_sample_metadata['sample_set'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][8])\n",
    "subject_sample_metadata['location'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][10])\n",
    "subject_sample_metadata['swallow_date'] = subject_sample_metadata['sample'].apply(lambda sample: '' if metadata_map[sample][3] == '' else datetime.strptime(metadata_map[sample][3], \"%Y-%m-%dT%H:%M:%SZ\").date())\n",
    "subject_sample_metadata['collection_date'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][6])\n",
    "subject_sample_metadata['subject'] = subject\n",
    "subject_sample_metadata['species'] = species\n",
    "\n",
    "#4. Extract contig and locus information \n",
    "loci_of_interest = set(within_host_changes[(within_host_changes['subject'] == subject_integer) & (within_host_changes['species'] == species)].sort_values(by = ['site_pos'])[['contig', 'site_pos']].drop_duplicates().to_records(index=False).tolist())\n",
    "no_of_loci = len(loci_of_interest)\n",
    "\n",
    "#5. Pull out allele frequencies of variants of interest in subject samples\n",
    "\n",
    "\n",
    "## Load the bz2 file\n",
    "data_directory = config.data_directory\n",
    "snp_file =  bz2.BZ2File(\"%ssnps/%s/annotated_snps.txt.bz2\" % (data_directory, species),\"r\")\n",
    "\n",
    "## Setting up the loop\n",
    "num_sites_processed = 0\n",
    "num_extracted_sites = 0\n",
    "line_number = -1\n",
    "final_line_number = -1\n",
    "initial_line_number = -100\n",
    "previous_gene_name = \"\"\n",
    "gene_name = \"\"\n",
    "chunk_size = 20000000\n",
    "\n",
    "## Header info\n",
    "line = snp_file.readline() \n",
    "items = line.split()[1:]    \n",
    "samples_in_file = sample_utils.parse_merged_sample_names(items)\n",
    "\n",
    "# Sample indices\n",
    "desired_sample_idxs = []\n",
    "for sample in subject_samples:\n",
    "    desired_sample_idxs.append( numpy.nonzero(samples_in_file==sample)[0][0] )\n",
    "desired_sample_idxs = numpy.array(desired_sample_idxs)    \n",
    "desired_samples = samples_in_file[desired_sample_idxs]\n",
    "\n",
    "## Gene descriptions\n",
    "genome_ids = parse_midas_data.get_ref_genome_ids(species)\n",
    "non_shared_genes = core_gene_utils.parse_non_shared_reference_genes(species)\n",
    "gene_descriptions = parse_patric.load_patric_gene_descriptions(genome_ids, non_shared_genes)\n",
    "centroid_gene_map = parse_midas_data.load_centroid_gene_map(species)\n",
    "\n",
    "## Initializing\n",
    "chrom_vec = []\n",
    "location_vec = []\n",
    "gene_descriptions_vec = []\n",
    "loci_extracted = 0\n",
    "\n",
    "#pd.DataFrame(columns = desired_samples)\n",
    "\n",
    "for line in snp_file: \n",
    "    \n",
    "    line_number += 1\n",
    "    \n",
    "    previous_gene_name = gene_name\n",
    "    \n",
    "    if line_number%100000==0:\n",
    "        sys.stderr.write(\"%dk sites processed...\\n\" % (line_number/1000)) \n",
    "    \n",
    "    items = line.split()\n",
    "    # Load information about site\n",
    "    info_items = items[0].split(\"|\")\n",
    "    chromosome = info_items[0]\n",
    "    location = long(info_items[1])\n",
    "    gene_name = info_items[2]\n",
    "    variant_type = info_items[3]\n",
    "    location_tuple = (chromosome, int(location))\n",
    "    #If it's not one of the sites of interest, move to the next line\n",
    "    \n",
    "    if line_number >= chunk_size and gene_name!=previous_gene_name:\n",
    "        # We are done for now!\n",
    "        final_line_number = line_number\n",
    "        sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \"\\n\")\n",
    "        break\n",
    "        \n",
    "    if location_tuple not in loci_of_interest:\n",
    "        continue\n",
    "    else:\n",
    "        sys.stderr.write(\"EXTRACTING: \" + str(location_tuple) + \"\\n\")\n",
    "        chrom_vec.append(chromosome)\n",
    "        location_vec.append(location)\n",
    "        loci_extracted += 1\n",
    "    \n",
    "    ## Getting alts and depths for sites of interest\n",
    "    alts = []\n",
    "    depths = []\n",
    "\n",
    "    for idx in desired_sample_idxs:    \n",
    "        item = items[1+idx]\n",
    "        subitems = item.split(\",\")\n",
    "        alts.append(float(subitems[0]))\n",
    "        depths.append(float(subitems[1]))\n",
    "    \n",
    "    if loci_extracted == 1:\n",
    "        alts_total = numpy.array(alts)\n",
    "        depths_total = numpy.array(depths)\n",
    "    else:\n",
    "        alts_total = numpy.vstack((alts_total, numpy.array(alts)))\n",
    "        depths_total = numpy.vstack((depths_total, numpy.array(depths)))\n",
    "    \n",
    "    if gene_name in gene_descriptions:\n",
    "        gene_descriptions_vec.append(gene_descriptions[gene_name])\n",
    "    elif gene_name in centroid_gene_map:\n",
    "        if centroid_gene_map[gene_name] in gene_descriptions:\n",
    "            gene_descriptions_vec.append(gene_descriptions[centroid_gene_map[gene_name]])\n",
    "    else:\n",
    "        gene_descriptions_vec.append(\"\")\n",
    "    \n",
    "    num_sites_processed += 1\n",
    "    \n",
    "    if num_sites_processed == no_of_loci:\n",
    "        sys.stderr.write(\"Successfully extracted all sites.\\n\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "snp_file.close()\n",
    "\n",
    "## creating dataframe with alts, depths, and freqs for each sample\n",
    "if no_of_loci == 1:\n",
    "    alts_df = pd.DataFrame(data = [alts_total], columns = desired_samples)\n",
    "    depths_df = pd.DataFrame(data = [depths_total], columns = desired_samples)\n",
    "else:\n",
    "    alts_df = pd.DataFrame(data = alts_total, columns = desired_samples)\n",
    "    depths_df = pd.DataFrame(data = depths_total, columns = desired_samples)\n",
    "alts_df['contig'] = chrom_vec\n",
    "alts_df['site_pos'] = location_vec\n",
    "alts_df['gene_descriptions'] = gene_descriptions_vec\n",
    "depths_df['contig'] = chrom_vec\n",
    "depths_df['site_pos'] = location_vec\n",
    "alts_df = alts_df.melt(var_name='sample', value_name='alt', id_vars = ['contig', 'site_pos', 'gene_descriptions'])\n",
    "depths_df = depths_df.melt(var_name='sample', value_name='depth', id_vars = ['contig', 'site_pos'])\n",
    "freq_df = pd.merge(alts_df, depths_df, on=['sample', 'site_pos', 'contig'])\n",
    "freq_df['allele_frequency'] = freq_df['alt']/freq_df['depth']\n",
    "# ## Merge with subject_sample map\n",
    "freq_df = pd.merge(freq_df, subject_sample_metadata, on = 'sample')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sys.stderr.write(\"\\nDONE! with %s\\n\" % (species))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. for loop version\n",
    "\n",
    "subjects = list(within_host_changes[(within_host_changes['species'] == species)].subject.unique())\n",
    "freq_df_final = pd.DataFrame()\n",
    "\n",
    "for subject in subjects:\n",
    "    sys.stderr.write(\"Processing subject %s in %s.\\n\" % (subject, species))\n",
    "    #3. For the subject in the current loop, pull out relevant samples and create a metadata table\n",
    "    subject = str(subject)\n",
    "    subject_integer = int(subject)\n",
    "    ##Extract haploid subject samples\n",
    "    subject_samples = parse_midas_data.parse_subject_sample_map()[subject].keys()\n",
    "    haploid_samples = diversity_utils.calculate_haploid_samples(species, min_coverage=10)\n",
    "    subject_samples = [sample for sample in subject_samples if sample in haploid_samples]\n",
    "    ##Make a metadataframe\n",
    "    metadata_map = parse_midas_data.parse_sample_metadata_map()\n",
    "    subject_sample_metadata = pd.DataFrame()\n",
    "    subject_sample_metadata['sample'] = subject_samples\n",
    "    subject_sample_metadata['sample_type'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][9])\n",
    "    subject_sample_metadata['sample_set'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][8])\n",
    "    subject_sample_metadata['location'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][10])\n",
    "    subject_sample_metadata['swallow_date'] = subject_sample_metadata['sample'].apply(lambda sample: '' if metadata_map[sample][3] == '' else datetime.strptime(metadata_map[sample][3], \"%Y-%m-%dT%H:%M:%SZ\").date())\n",
    "    subject_sample_metadata['collection_date'] = subject_sample_metadata['sample'].apply(lambda sample: metadata_map[sample][6])\n",
    "    subject_sample_metadata['subject'] = subject\n",
    "    subject_sample_metadata['species'] = species\n",
    "\n",
    "    #4. Extract contig and locus information \n",
    "    loci_of_interest = set(within_host_changes[(within_host_changes['subject'] == subject_integer) & (within_host_changes['species'] == species)].sort_values(by = ['site_pos'])[['contig', 'site_pos']].drop_duplicates().to_records(index=False).tolist())\n",
    "    no_of_loci = len(loci_of_interest)\n",
    "\n",
    "    #5. Pull out allele frequencies of variants of interest in subject samples\n",
    "\n",
    "\n",
    "    ## Load the bz2 file\n",
    "    data_directory = config.data_directory\n",
    "    snp_file =  bz2.BZ2File(\"%ssnps/%s/annotated_snps.txt.bz2\" % (data_directory, species),\"r\")\n",
    "\n",
    "    ## Setting up the loop\n",
    "    num_sites_processed = 0\n",
    "    num_extracted_sites = 0\n",
    "    line_number = -1\n",
    "    final_line_number = -1\n",
    "    initial_line_number = -100\n",
    "    previous_gene_name = \"\"\n",
    "    gene_name = \"\"\n",
    "    chunk_size = 20000000\n",
    "\n",
    "    ## Header info\n",
    "    line = snp_file.readline() \n",
    "    items = line.split()[1:]    \n",
    "    samples_in_file = sample_utils.parse_merged_sample_names(items)\n",
    "\n",
    "    # Sample indices\n",
    "    desired_sample_idxs = []\n",
    "    for sample in subject_samples:\n",
    "        desired_sample_idxs.append( numpy.nonzero(samples_in_file==sample)[0][0] )\n",
    "    desired_sample_idxs = numpy.array(desired_sample_idxs)    \n",
    "    desired_samples = samples_in_file[desired_sample_idxs]\n",
    "\n",
    "    ## Initializing\n",
    "    chrom_vec = []\n",
    "    location_vec = []\n",
    "    loci_extracted = 0\n",
    "\n",
    "    #pd.DataFrame(columns = desired_samples)\n",
    "\n",
    "    for line in snp_file: \n",
    "\n",
    "        line_number += 1\n",
    "\n",
    "        previous_gene_name = gene_name\n",
    "\n",
    "        if line_number%1000000==0:\n",
    "            sys.stderr.write(\"%dk sites processed...\\n\" % (line_number/1000)) \n",
    "\n",
    "        items = line.split()\n",
    "        # Load information about site\n",
    "        info_items = items[0].split(\"|\")\n",
    "        chromosome = info_items[0]\n",
    "        location = long(info_items[1])\n",
    "        gene_name = info_items[2]\n",
    "        variant_type = info_items[3]\n",
    "        location_tuple = (chromosome, int(location))\n",
    "        #If it's not one of the sites of interest, move to the next line\n",
    "\n",
    "        if line_number >= chunk_size and gene_name!=previous_gene_name:\n",
    "            # We are done for now!\n",
    "            final_line_number = line_number\n",
    "            sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \"\\n\")\n",
    "            break\n",
    "\n",
    "        if location_tuple not in loci_of_interest:\n",
    "            continue\n",
    "        else:\n",
    "            sys.stderr.write(\"EXTRACTING: \" + str(location_tuple) + \"\\n\")\n",
    "            chrom_vec.append(chromosome)\n",
    "            location_vec.append(location)\n",
    "            loci_extracted += 1\n",
    "\n",
    "        ## Getting alts and depths for sites of interest\n",
    "        alts = []\n",
    "        depths = []\n",
    "\n",
    "        for idx in desired_sample_idxs:    \n",
    "            item = items[1+idx]\n",
    "            subitems = item.split(\",\")\n",
    "            alts.append(float(subitems[0]))\n",
    "            depths.append(float(subitems[1]))\n",
    "\n",
    "        if loci_extracted == 1:\n",
    "            alts_total = numpy.array(alts)\n",
    "            depths_total = numpy.array(depths)\n",
    "        else:\n",
    "            alts_total = numpy.vstack((alts_total, numpy.array(alts)))\n",
    "            depths_total = numpy.vstack((depths_total, numpy.array(depths)))\n",
    "\n",
    "\n",
    "        num_sites_processed += 1\n",
    "        \n",
    "        if num_sites_processed == no_of_loci:\n",
    "            sys.stderr.write(\"Successfully extracted all sites.\\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    snp_file.close()\n",
    "\n",
    "    ## creating dataframe with alts, depths, and freqs for each sample\n",
    "    if no_of_loci == 1:\n",
    "        alts_df = pd.DataFrame(data = [alts_total], columns = desired_samples)\n",
    "        depths_df = pd.DataFrame(data = [depths_total], columns = desired_samples)\n",
    "    else:\n",
    "        alts_df = pd.DataFrame(data = alts_total, columns = desired_samples)\n",
    "        depths_df = pd.DataFrame(data = depths_total, columns = desired_samples)\n",
    "    alts_df['contig'] = chrom_vec\n",
    "    alts_df['site_pos'] = location_vec\n",
    "    depths_df['contig'] = chrom_vec\n",
    "    depths_df['site_pos'] = location_vec\n",
    "    alts_df = alts_df.melt(var_name='sample', value_name='alt', id_vars = ['contig', 'site_pos'])\n",
    "    depths_df = depths_df.melt(var_name='sample', value_name='depth', id_vars = ['contig', 'site_pos'])\n",
    "    freq_df = pd.merge(alts_df, depths_df, on=['sample', 'site_pos', 'contig'])\n",
    "    freq_df['allele_frequency'] = freq_df['alt']/freq_df['depth']\n",
    "    # ## Merge with subject_sample map\n",
    "    freq_df = pd.merge(freq_df, subject_sample_metadata, on = 'sample')\n",
    "    \n",
    "    freq_df_final = freq_df_final.append(freq_df)\n",
    "\n",
    "    sys.stderr.write(\"Done with subject %s!\\n\\n\" % (subject))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_folder = \"%s%s\" % (config.project_directory, \"evolutionary_changes/\")\n",
    "if not os.path.exists(evolution_folder):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(evolution_folder)\n",
    "    print(\"Directory '{}' created successfully.\".format(evolution_folder))\n",
    "else:\n",
    "    print(\"Directory '{}' already exists.\".format(evolution_folder))\n",
    "snv_frequencies_output = \"%s%s\" % (evolution_folder, \"snv_frequencies.txt.bz2\")\n",
    "freq_df_final.to_csv(snv_frequencies_output, index = False, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENOMIC LOCI ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Get genome length\n",
    "species = \"Bacteroides_vulgatus_57955\"\n",
    "species_length_file_path = \"%smerged_data/snps/%s/snps_summary.txt\" % (config.project_folder, species)\n",
    "species_length = pd.read_csv(species_length_file_path, sep = \"\\t\")['genome_length'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Get SNV positions and contigs\n",
    "snv_contigs = within_set_changes.loc[within_set_changes['species'] == species, 'contig']\n",
    "snv_positions = within_set_changes.loc[within_set_changes['species'] == species, 'site_pos']\n",
    "snv_positions_df = pd.DataFrame({'contigs': snv_contigs, 'positions': snv_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming snv_positions_df is your DataFrame with 'positions' column\n",
    "positions = snv_positions_df['positions']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 1))\n",
    "plt.vlines(positions, 0, 1, colors='red', linewidth=0.5)  # Plotting vertical red lines\n",
    "plt.xlim(0, species_length)  # Setting x-axis limits\n",
    "plt.ylim(0, 1)  # Setting y-axis limits\n",
    "plt.gca().axes.get_yaxis().set_visible(False)  # Hide y-axis labels\n",
    "plt.title('{} SNV positions along the genome'.format(species))\n",
    "plt.xlabel('Position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"%sfigures/evolutionary_changes/%s_genomic_loci.png\" % (config.analysis_directory,species)\n",
    "fig.savefig(out_path, dpi=300, bbox_inches='tight')  # Save as PNG file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_set_changes.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max number of differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts = snp_changes_df.groupby(['species', 'subject', 'sample1', 'sample2']).size().reset_index(name='row_count').sort_values(by = ['row_count'], ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map = parse_midas_data.parse_sample_metadata_map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_map['SRR18585059']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individually per species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall directories\n",
    "metadata_directory = \"%s%s\" % (config.metadata_directory, \"shalon_metadata.txt\")\n",
    "gene_info_directory = \"%s%s\" % (config.project_folder, \"gene_descriptions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_directory, sep = \"\\t\")#.rename(columns = {'Sample Name': 'sample_alias'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_sample_map = parse_midas_data.parse_subject_sample_map()\n",
    "sample_metadata_map = parse_midas_data.parse_sample_order_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load intersample changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Anaerostipes_hadrus_55206\"\n",
    "intersample_change_map = load_intersample_change_map(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate within-host changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersample_change_map_within_host = {}\n",
    "\n",
    "for subject in subject_sample_map:\n",
    "    host_samples = subject_sample_map[subject].keys()\n",
    "    within_host_sample_combos = list(itertools.combinations(host_samples, 2))\n",
    "    \n",
    "    for sample_combo in within_host_sample_combos:\n",
    "        if sample_combo in intersample_change_map:\n",
    "            intersample_change_map_within_host[sample_combo] = intersample_change_map[sample_combo]\n",
    "        elif (sample_combo[1], sample_combo[0]) in within_host_sample_combos:\n",
    "            sample_combo = (sample_combo[1], sample_combo[0])\n",
    "            intersample_change_map_within_host[sample_combo] = intersample_change_map[sample_combo]\n",
    "        else: \n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersample_change_map_within_host"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python27_env",
   "language": "python",
   "name": "python27_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
