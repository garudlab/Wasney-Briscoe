{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import string\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "# plotting functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# predefined functions\n",
    "os.sys.path.append('/u/project/ngarud/michaelw/microbiome_evolution/py3.8/microbiome_evolution_SHALON/postprocessing_scripts/')\n",
    "from parse_midas_data import *\n",
    "from diversity_utils import *\n",
    "from calculate_intersample_changes import *\n",
    "from core_gene_utils import *\n",
    "from parse_patric import *\n",
    "import parse_patric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load species list\n",
    "species_list_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/metadata/species_snps.txt\"\n",
    "\n",
    "with open(species_list_path, 'r') as file:\n",
    "    species_list = file.readlines()\n",
    "species_list = [species.strip() for species in species_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load maps\n",
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "sample_list = list(sample_metadata_map.keys())\n",
    "subject_sample_map = parse_subject_sample_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcuating opportunities, rates of change etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNV change rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load species list\n",
    "species_list_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/metadata/species_snps.txt\"\n",
    "\n",
    "with open(species_list_path, 'r') as file:\n",
    "    species_list = file.readlines()\n",
    "species_list = [species.strip() for species in species_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SNV changes dataframe\n",
    "in_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/snp_changes.txt.bz2\"\n",
    "snv_changes_df = pd.read_csv(in_path, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_summary_df = pd.DataFrame(snv_changes_df.groupby(['species', 'sample1', 'sample2']).size()).reset_index().rename(columns = {0:\"no_of_changes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QP pairs (even with 0 changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preexisting QP summary file\n"
     ]
    }
   ],
   "source": [
    "# path to output\n",
    "haploid_summary_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/QP_status.csv\"\n",
    "generate = False\n",
    "save = True\n",
    "if  os.path.exists(haploid_summary_path) and (not generate):\n",
    "    print(\"Loading preexisting QP summary file\")\n",
    "    samples_df = pd.read_csv(haploid_summary_path, sep = \",\")\n",
    "\n",
    "    haploid_samples = samples_df[samples_df['haploid']]\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Creating QP summary file from scratch.\")\n",
    "    # load species list\n",
    "    species_list_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/metadata/species_snps.txt\"\n",
    "\n",
    "    with open(species_list_path, 'r') as file:\n",
    "        species_list = file.readlines()\n",
    "    species_list = [species.strip() for species in species_list]\n",
    "\n",
    "    metadata_map = parse_midas_data.parse_sample_metadata_map()\n",
    "    samples_df = pd.DataFrame(columns = ['accession_id', 'species', 'haploid', 'subject_id'])\n",
    "\n",
    "    accession_id_vec = []\n",
    "    species_vec = []\n",
    "    haploid_vec = []\n",
    "    subject_id_vec = []\n",
    "\n",
    "    print(\"\\nCalculating lineage structure.\")\n",
    "\n",
    "    counter = 0\n",
    "    no_of_species = len(species_list)\n",
    "    for species in species_list:\n",
    "        counter += 1\n",
    "        print(\"Processing \" + species + \" ({}/{})\".format(counter, no_of_species))\n",
    "        high_coverage_samples = diversity_utils.calculate_highcoverage_samples(species)\n",
    "        haploid_samples = diversity_utils.calculate_haploid_samples(species, use_HMP_freqs = True)\n",
    "        \n",
    "        haploid_boolean_vec = [True if sample in haploid_samples else False for sample in high_coverage_samples]\n",
    "        subject_id_vec_temp = [metadata_map[sample][0] for sample in high_coverage_samples]\n",
    "        \n",
    "        species_vec = species_vec + [species]*len(high_coverage_samples)\n",
    "        accession_id_vec = accession_id_vec + list(high_coverage_samples)\n",
    "        haploid_vec = haploid_vec + haploid_boolean_vec\n",
    "        subject_id_vec = subject_id_vec + subject_id_vec_temp\n",
    "\n",
    "    samples_df['accession_id'] = accession_id_vec\n",
    "    samples_df['species'] = species_vec\n",
    "    samples_df['haploid'] = haploid_vec\n",
    "    samples_df['subject_id'] = subject_id_vec\n",
    "\n",
    "    if save:\n",
    "        samples_df.to_csv(haploid_summary_path, sep = \",\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preexisting change summary file\n"
     ]
    }
   ],
   "source": [
    "# path to output\n",
    "change_summary_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/change_summary_Full.csv\"\n",
    "generate = False\n",
    "save = True\n",
    "\n",
    "if os.path.exists(change_summary_path) and (not generate):\n",
    "    print(\"Loading preexisting change summary file\")\n",
    "    change_summary_df = pd.read_csv(change_summary_path, sep = \",\")\n",
    "    \n",
    "else:\n",
    "\n",
    "    haploid_samples = samples_df[samples_df['haploid']]\n",
    "        \n",
    "    change_summary_array = []\n",
    "    haploid_species = haploid_samples.species.unique()\n",
    "\n",
    "    print(\"\\nCalculating change rate.\")\n",
    "\n",
    "    for i,species in enumerate(haploid_species):\n",
    "        print(\"Processing %s (%d/%d)\" % (species, i+1, len(haploid_species)))\n",
    "        intersample_change_map = load_intersample_change_map(species)\n",
    "        samples = haploid_samples[haploid_samples['species'] == species]['accession_id'].unique()\n",
    "        sample_pairs = list(itertools.combinations(samples, 2))\n",
    "        for sample_pair in sample_pairs:\n",
    "            if (sample_pair[1], sample_pair[0]) in intersample_change_map:\n",
    "                sample_pair = (sample_pair[1], sample_pair[0])\n",
    "\n",
    "            if sample_pair not in intersample_change_map:\n",
    "                print(\"%s not in intersample change map for %s\" % (str(sample_pair), species))\n",
    "                continue\n",
    "\n",
    "            opportunities = intersample_change_map[sample_pair]['snps'][0]\n",
    "            gene_opportunities = intersample_change_map[sample_pair]['genes'][0]\n",
    "            snv_changes = len(intersample_change_map[sample_pair]['snps'][2])\n",
    "            gene_changes = len(intersample_change_map[sample_pair]['genes'][2])\n",
    "            rate_of_change = snv_changes/opportunities\n",
    "            gene_rate_of_change = gene_changes/gene_opportunities\n",
    "\n",
    "            change_summary_array.append([species, sample_pair[0], sample_pair[1], snv_changes, opportunities, rate_of_change, gene_changes, gene_opportunities, gene_rate_of_change])\n",
    "            \n",
    "\n",
    "    change_summary_df = pd.DataFrame(change_summary_array, columns=['species', 'sample_1', 'sample_2', 'snv_changes', 'opportunities', 'rate_of_change', 'gene_changes', 'gene_opportunities', 'gene_rate_of_change'])\n",
    "\n",
    "    # annotate\n",
    "    print(\"\\n\\nAnnotating.\")\n",
    "    change_summary_df['subject_1'] = change_summary_df['sample_1'].apply(lambda x: sample_metadata_map[x][0])\n",
    "    change_summary_df['subject_2'] = change_summary_df['sample_2'].apply(lambda x: sample_metadata_map[x][0])\n",
    "    change_summary_df['device_type_1'] = change_summary_df['sample_1'].apply(lambda x: sample_metadata_map[x][2])\n",
    "    change_summary_df['device_type_2'] = change_summary_df['sample_2'].apply(lambda x: sample_metadata_map[x][2])\n",
    "    timestamp_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    change_summary_df['day_1'] = change_summary_df[['sample_1','device_type_1']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample_1']][5], timestamp_format).strftime('%Y-%m-%d') if row['device_type_1'] == \"Stool\" or row['device_type_1'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample_1']][3], timestamp_format).strftime('%Y-%m-%d'), axis = 1)\n",
    "    change_summary_df['day_2'] = change_summary_df[['sample_2','device_type_2']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample_2']][5], timestamp_format).strftime('%Y-%m-%d') if row['device_type_2'] == \"Stool\" or row['device_type_2'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample_2']][3], timestamp_format).strftime('%Y-%m-%d'), axis = 1)\n",
    "    change_summary_df['time_1'] = change_summary_df[['sample_1','device_type_1']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample_1']][5], timestamp_format).strftime('%H:%M:%S') if row['device_type_1'] == \"Stool\" or row['device_type_1'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample_1']][3], timestamp_format).strftime('%H:%M:%S'), axis = 1)\n",
    "    change_summary_df['time_2'] = change_summary_df[['sample_2','device_type_2']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample_2']][5], timestamp_format).strftime('%H:%M:%S') if row['device_type_2'] == \"Stool\" or row['device_type_2'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample_2']][3], timestamp_format).strftime('%H:%M:%S'), axis = 1)\n",
    "\n",
    "    # Label as within timepoint or between time point\n",
    "    change_summary_df['timepoint_orientation'] = change_summary_df.apply(lambda row: \"Within timepoint\" if row['day_1'] == row['day_2'] and row['time_1'] == row['time_2'] else \"Between timepoint\", axis = 1)\n",
    "    change_summary_df['device_orientation'] = change_summary_df.apply(lambda row: \"Same device\" if row['device_type_1'] == row['device_type_2'] else \"Different device\", axis = 1)\n",
    "    change_summary_df['datetime_1'] = pd.to_datetime(change_summary_df['day_1'] + ' ' + change_summary_df['time_1'])\n",
    "    change_summary_df['datetime_2'] = pd.to_datetime(change_summary_df['day_2'] + ' ' + change_summary_df['time_2'])\n",
    "    change_summary_df['time_difference_hours'] = (change_summary_df['datetime_2'] - change_summary_df['datetime_1']).dt.total_seconds() / 3600\n",
    "\n",
    "    if save:\n",
    "        change_summary_df.to_csv(change_summary_path, sep = \",\", index=False)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for <= 20 SNV changes\n",
    "change_summary_df_full = change_summary_df.copy()\n",
    "change_summary_df = change_summary_df[change_summary_df['snv_changes'] <= 20]\n",
    "# Filter for within host\n",
    "change_summary_df = change_summary_df[change_summary_df['subject_1'] == change_summary_df['subject_2']]\n",
    "# Filter for between capsule\n",
    "change_summary_df = change_summary_df[(change_summary_df['device_type_1'] != \"Stool\") &\n",
    "                                      (change_summary_df['device_type_1'] != \"Saliva\") &\n",
    "                                      (change_summary_df['device_type_2'] != \"Stool\") &\n",
    "                                      (change_summary_df['device_type_2'] != \"Saliva\")]\n",
    "# Filter for not the same device in the same timepoint (which only happens in subject 1)\n",
    "change_summary_df = change_summary_df[~((change_summary_df['timepoint_orientation'] == \"Within timepoint\") &\n",
    "                                      (change_summary_df['device_orientation'] == \"Same device\"))]\n",
    "# Reset index\n",
    "change_summary_df = change_summary_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving change summary\n",
    "# change_summary_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/change_summary_CapsulesOnly.csv\"\n",
    "\n",
    "# change_summary_df.to_csv(change_summary_path, sep = \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating text numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate 1: How many species-subjects show along the gut differentiation comparisons have changes (i.e., > 0 changes).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_timepoint_change_pairs = change_summary_df[change_summary_df.timepoint_orientation == \"Within timepoint\"][['species','subject_1']].drop_duplicates().rename(columns = {\"subject_1\":\"subject\"}).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(within_timepoint_change_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_timepoint_change_pairs_with_changes = change_summary_df[(change_summary_df.timepoint_orientation == \"Within timepoint\") & (change_summary_df.snv_changes > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>sample_1</th>\n",
       "      <th>sample_2</th>\n",
       "      <th>snv_changes</th>\n",
       "      <th>opportunities</th>\n",
       "      <th>rate_of_change</th>\n",
       "      <th>gene_changes</th>\n",
       "      <th>gene_opportunities</th>\n",
       "      <th>gene_rate_of_change</th>\n",
       "      <th>subject_1</th>\n",
       "      <th>...</th>\n",
       "      <th>device_type_2</th>\n",
       "      <th>day_1</th>\n",
       "      <th>day_2</th>\n",
       "      <th>time_1</th>\n",
       "      <th>time_2</th>\n",
       "      <th>timepoint_orientation</th>\n",
       "      <th>device_orientation</th>\n",
       "      <th>datetime_1</th>\n",
       "      <th>datetime_2</th>\n",
       "      <th>time_difference_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Anaerostipes_hadrus_55206</td>\n",
       "      <td>SRR18585057</td>\n",
       "      <td>SRR18585058</td>\n",
       "      <td>8</td>\n",
       "      <td>875267.0</td>\n",
       "      <td>9.140068e-06</td>\n",
       "      <td>3</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 3</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-18 05:00:00</td>\n",
       "      <td>2020-08-18 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Bacteroides_massiliensis_44749</td>\n",
       "      <td>SRR18585022</td>\n",
       "      <td>SRR18585025</td>\n",
       "      <td>2</td>\n",
       "      <td>2419000.0</td>\n",
       "      <td>8.267879e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>06:55:00</td>\n",
       "      <td>06:55:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-06-27 06:55:00</td>\n",
       "      <td>2020-06-27 06:55:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>SRR18585193</td>\n",
       "      <td>SRR18585194</td>\n",
       "      <td>5</td>\n",
       "      <td>2738090.0</td>\n",
       "      <td>1.826090e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>8473.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 3</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>04:15:00</td>\n",
       "      <td>04:15:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-09-16 04:15:00</td>\n",
       "      <td>2020-09-16 04:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>SRR18585218</td>\n",
       "      <td>SRR18585220</td>\n",
       "      <td>5</td>\n",
       "      <td>2820210.0</td>\n",
       "      <td>1.772918e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>8473.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>05:15:00</td>\n",
       "      <td>05:15:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-19 05:15:00</td>\n",
       "      <td>2020-08-19 05:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Blautia_wexlerae_56130</td>\n",
       "      <td>SRR18584991</td>\n",
       "      <td>SRR18584993</td>\n",
       "      <td>3</td>\n",
       "      <td>1300190.0</td>\n",
       "      <td>2.307355e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>3628.0</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>22:30:00</td>\n",
       "      <td>22:30:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-30 22:30:00</td>\n",
       "      <td>2020-08-30 22:30:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>Desulfovibrio_piger_61475</td>\n",
       "      <td>SRR18585200</td>\n",
       "      <td>SRR18585202</td>\n",
       "      <td>6</td>\n",
       "      <td>660160.0</td>\n",
       "      <td>9.088706e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>2476.0</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-25 23:45:00</td>\n",
       "      <td>2020-08-25 23:45:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>Ruminococcus_gnavus_57638</td>\n",
       "      <td>SRR18585019</td>\n",
       "      <td>SRR18585020</td>\n",
       "      <td>5</td>\n",
       "      <td>1453270.0</td>\n",
       "      <td>3.440517e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>4633.0</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 3</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-06-30 21:30:00</td>\n",
       "      <td>2020-06-30 21:30:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             species     sample_1     sample_2  snv_changes  \\\n",
       "231        Anaerostipes_hadrus_55206  SRR18585057  SRR18585058            8   \n",
       "396   Bacteroides_massiliensis_44749  SRR18585022  SRR18585025            2   \n",
       "889       Bacteroides_vulgatus_57955  SRR18585193  SRR18585194            5   \n",
       "909       Bacteroides_vulgatus_57955  SRR18585218  SRR18585220            5   \n",
       "1069          Blautia_wexlerae_56130  SRR18584991  SRR18584993            3   \n",
       "1111       Desulfovibrio_piger_61475  SRR18585200  SRR18585202            6   \n",
       "1472       Ruminococcus_gnavus_57638  SRR18585019  SRR18585020            5   \n",
       "\n",
       "      opportunities  rate_of_change  gene_changes  gene_opportunities  \\\n",
       "231        875267.0    9.140068e-06             3              2360.0   \n",
       "396       2419000.0    8.267879e-07             0              2963.0   \n",
       "889       2738090.0    1.826090e-06             0              8473.0   \n",
       "909       2820210.0    1.772918e-06             0              8473.0   \n",
       "1069      1300190.0    2.307355e-06             1              3628.0   \n",
       "1111       660160.0    9.088706e-06             1              2476.0   \n",
       "1472      1453270.0    3.440517e-06             1              4633.0   \n",
       "\n",
       "      gene_rate_of_change  subject_1  ...  device_type_2       day_1  \\\n",
       "231              0.001271          8  ...      Capsule 3  2020-08-18   \n",
       "396              0.000000          2  ...      Capsule 1  2020-06-27   \n",
       "889              0.000000         11  ...      Capsule 3  2020-09-16   \n",
       "909              0.000000          8  ...      Capsule 1  2020-08-19   \n",
       "1069             0.000276         10  ...      Capsule 1  2020-08-30   \n",
       "1111             0.000404          9  ...      Capsule 1  2020-08-25   \n",
       "1472             0.000216          2  ...      Capsule 3  2020-06-30   \n",
       "\n",
       "           day_2    time_1    time_2 timepoint_orientation device_orientation  \\\n",
       "231   2020-08-18  05:00:00  05:00:00      Within timepoint   Different device   \n",
       "396   2020-06-27  06:55:00  06:55:00      Within timepoint   Different device   \n",
       "889   2020-09-16  04:15:00  04:15:00      Within timepoint   Different device   \n",
       "909   2020-08-19  05:15:00  05:15:00      Within timepoint   Different device   \n",
       "1069  2020-08-30  22:30:00  22:30:00      Within timepoint   Different device   \n",
       "1111  2020-08-25  23:45:00  23:45:00      Within timepoint   Different device   \n",
       "1472  2020-06-30  21:30:00  21:30:00      Within timepoint   Different device   \n",
       "\n",
       "               datetime_1           datetime_2 time_difference_hours  \n",
       "231   2020-08-18 05:00:00  2020-08-18 05:00:00                   0.0  \n",
       "396   2020-06-27 06:55:00  2020-06-27 06:55:00                   0.0  \n",
       "889   2020-09-16 04:15:00  2020-09-16 04:15:00                   0.0  \n",
       "909   2020-08-19 05:15:00  2020-08-19 05:15:00                   0.0  \n",
       "1069  2020-08-30 22:30:00  2020-08-30 22:30:00                   0.0  \n",
       "1111  2020-08-25 23:45:00  2020-08-25 23:45:00                   0.0  \n",
       "1472  2020-06-30 21:30:00  2020-06-30 21:30:00                   0.0  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_timepoint_change_pairs_with_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>sample_1</th>\n",
       "      <th>sample_2</th>\n",
       "      <th>snv_changes</th>\n",
       "      <th>opportunities</th>\n",
       "      <th>rate_of_change</th>\n",
       "      <th>gene_changes</th>\n",
       "      <th>gene_opportunities</th>\n",
       "      <th>gene_rate_of_change</th>\n",
       "      <th>subject_1</th>\n",
       "      <th>...</th>\n",
       "      <th>device_type_2</th>\n",
       "      <th>day_1</th>\n",
       "      <th>day_2</th>\n",
       "      <th>time_1</th>\n",
       "      <th>time_2</th>\n",
       "      <th>timepoint_orientation</th>\n",
       "      <th>device_orientation</th>\n",
       "      <th>datetime_1</th>\n",
       "      <th>datetime_2</th>\n",
       "      <th>time_difference_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Anaerostipes_hadrus_55206</td>\n",
       "      <td>SRR18585057</td>\n",
       "      <td>SRR18585058</td>\n",
       "      <td>8</td>\n",
       "      <td>875267.0</td>\n",
       "      <td>9.140068e-06</td>\n",
       "      <td>3</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 3</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-18 05:00:00</td>\n",
       "      <td>2020-08-18 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Bacteroides_massiliensis_44749</td>\n",
       "      <td>SRR18585022</td>\n",
       "      <td>SRR18585025</td>\n",
       "      <td>2</td>\n",
       "      <td>2419000.0</td>\n",
       "      <td>8.267879e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>06:55:00</td>\n",
       "      <td>06:55:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-06-27 06:55:00</td>\n",
       "      <td>2020-06-27 06:55:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>SRR18585193</td>\n",
       "      <td>SRR18585194</td>\n",
       "      <td>5</td>\n",
       "      <td>2738090.0</td>\n",
       "      <td>1.826090e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>8473.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 3</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>04:15:00</td>\n",
       "      <td>04:15:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-09-16 04:15:00</td>\n",
       "      <td>2020-09-16 04:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>SRR18585218</td>\n",
       "      <td>SRR18585220</td>\n",
       "      <td>5</td>\n",
       "      <td>2820210.0</td>\n",
       "      <td>1.772918e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>8473.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>05:15:00</td>\n",
       "      <td>05:15:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-19 05:15:00</td>\n",
       "      <td>2020-08-19 05:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Blautia_wexlerae_56130</td>\n",
       "      <td>SRR18584991</td>\n",
       "      <td>SRR18584993</td>\n",
       "      <td>3</td>\n",
       "      <td>1300190.0</td>\n",
       "      <td>2.307355e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>3628.0</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>22:30:00</td>\n",
       "      <td>22:30:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-30 22:30:00</td>\n",
       "      <td>2020-08-30 22:30:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>Desulfovibrio_piger_61475</td>\n",
       "      <td>SRR18585200</td>\n",
       "      <td>SRR18585202</td>\n",
       "      <td>6</td>\n",
       "      <td>660160.0</td>\n",
       "      <td>9.088706e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>2476.0</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 1</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-08-25 23:45:00</td>\n",
       "      <td>2020-08-25 23:45:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>Ruminococcus_gnavus_57638</td>\n",
       "      <td>SRR18585019</td>\n",
       "      <td>SRR18585020</td>\n",
       "      <td>5</td>\n",
       "      <td>1453270.0</td>\n",
       "      <td>3.440517e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>4633.0</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Capsule 3</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>Within timepoint</td>\n",
       "      <td>Different device</td>\n",
       "      <td>2020-06-30 21:30:00</td>\n",
       "      <td>2020-06-30 21:30:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             species     sample_1     sample_2  snv_changes  \\\n",
       "231        Anaerostipes_hadrus_55206  SRR18585057  SRR18585058            8   \n",
       "396   Bacteroides_massiliensis_44749  SRR18585022  SRR18585025            2   \n",
       "889       Bacteroides_vulgatus_57955  SRR18585193  SRR18585194            5   \n",
       "909       Bacteroides_vulgatus_57955  SRR18585218  SRR18585220            5   \n",
       "1069          Blautia_wexlerae_56130  SRR18584991  SRR18584993            3   \n",
       "1111       Desulfovibrio_piger_61475  SRR18585200  SRR18585202            6   \n",
       "1472       Ruminococcus_gnavus_57638  SRR18585019  SRR18585020            5   \n",
       "\n",
       "      opportunities  rate_of_change  gene_changes  gene_opportunities  \\\n",
       "231        875267.0    9.140068e-06             3              2360.0   \n",
       "396       2419000.0    8.267879e-07             0              2963.0   \n",
       "889       2738090.0    1.826090e-06             0              8473.0   \n",
       "909       2820210.0    1.772918e-06             0              8473.0   \n",
       "1069      1300190.0    2.307355e-06             1              3628.0   \n",
       "1111       660160.0    9.088706e-06             1              2476.0   \n",
       "1472      1453270.0    3.440517e-06             1              4633.0   \n",
       "\n",
       "      gene_rate_of_change  subject_1  ...  device_type_2       day_1  \\\n",
       "231              0.001271          8  ...      Capsule 3  2020-08-18   \n",
       "396              0.000000          2  ...      Capsule 1  2020-06-27   \n",
       "889              0.000000         11  ...      Capsule 3  2020-09-16   \n",
       "909              0.000000          8  ...      Capsule 1  2020-08-19   \n",
       "1069             0.000276         10  ...      Capsule 1  2020-08-30   \n",
       "1111             0.000404          9  ...      Capsule 1  2020-08-25   \n",
       "1472             0.000216          2  ...      Capsule 3  2020-06-30   \n",
       "\n",
       "           day_2    time_1    time_2 timepoint_orientation device_orientation  \\\n",
       "231   2020-08-18  05:00:00  05:00:00      Within timepoint   Different device   \n",
       "396   2020-06-27  06:55:00  06:55:00      Within timepoint   Different device   \n",
       "889   2020-09-16  04:15:00  04:15:00      Within timepoint   Different device   \n",
       "909   2020-08-19  05:15:00  05:15:00      Within timepoint   Different device   \n",
       "1069  2020-08-30  22:30:00  22:30:00      Within timepoint   Different device   \n",
       "1111  2020-08-25  23:45:00  23:45:00      Within timepoint   Different device   \n",
       "1472  2020-06-30  21:30:00  21:30:00      Within timepoint   Different device   \n",
       "\n",
       "               datetime_1           datetime_2 time_difference_hours  \n",
       "231   2020-08-18 05:00:00  2020-08-18 05:00:00                   0.0  \n",
       "396   2020-06-27 06:55:00  2020-06-27 06:55:00                   0.0  \n",
       "889   2020-09-16 04:15:00  2020-09-16 04:15:00                   0.0  \n",
       "909   2020-08-19 05:15:00  2020-08-19 05:15:00                   0.0  \n",
       "1069  2020-08-30 22:30:00  2020-08-30 22:30:00                   0.0  \n",
       "1111  2020-08-25 23:45:00  2020-08-25 23:45:00                   0.0  \n",
       "1472  2020-06-30 21:30:00  2020-06-30 21:30:00                   0.0  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_timepoint_change_pairs_with_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(within_timepoint_change_pairs_with_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_timepoint_change_pairs = change_summary_df[change_summary_df.timepoint_orientation == \"Between timepoint\"][['species','subject_1']].drop_duplicates().rename(columns = {\"subject_1\":\"subject\"}).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(between_timepoint_change_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(between_timepoint_change_pairs.subject.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_timepoint_change_pairs_with_changes = change_summary_df[(change_summary_df.timepoint_orientation == \"Between timepoint\") & (change_summary_df.snv_changes > 0)][['species','subject_1']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(between_timepoint_change_pairs_with_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32608695652173914"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30/92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate 2: How many species-subjects meet the following requirements:  \n",
    "- at least two timepoints with at least two capsule devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load maps\n",
    "sample_metadata_map = parse_sample_metadata_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36962/2083326937.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  haploid_samples.loc[:, 'device_type'] = haploid_samples['accession_id'].apply(lambda x: sample_metadata_map[x][2])\n",
      "/tmp/ipykernel_36962/2083326937.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  haploid_samples.loc[:, 'day'] = haploid_samples[['accession_id', 'device_type']].apply(\n",
      "/tmp/ipykernel_36962/2083326937.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  haploid_samples.loc[:, 'time'] = haploid_samples[['accession_id', 'device_type']].apply(\n",
      "/tmp/ipykernel_36962/2083326937.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  haploid_samples['datetime'] = pd.to_datetime(haploid_samples['day'] + ' ' + haploid_samples['time'])\n"
     ]
    }
   ],
   "source": [
    "# annotate haploid_samples\n",
    "## device type\n",
    "haploid_samples.loc[:, 'device_type'] = haploid_samples['accession_id'].apply(lambda x: sample_metadata_map[x][2])\n",
    "timestamp_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "haploid_samples.loc[:, 'day'] = haploid_samples[['accession_id', 'device_type']].apply(\n",
    "    lambda row: datetime.strptime(sample_metadata_map[row['accession_id']][5], timestamp_format).strftime('%Y-%m-%d')\n",
    "    if row['device_type'] == \"Stool\" or row['device_type'] == \"Saliva\"\n",
    "    else datetime.strptime(sample_metadata_map[row['accession_id']][3], timestamp_format).strftime('%Y-%m-%d'),\n",
    "    axis=1\n",
    ")\n",
    "haploid_samples.loc[:, 'time'] = haploid_samples[['accession_id', 'device_type']].apply(\n",
    "    lambda row: datetime.strptime(sample_metadata_map[row['accession_id']][5], timestamp_format).strftime('%H:%M:%S')\n",
    "    if row['device_type'] == \"Stool\" or row['device_type'] == \"Saliva\"\n",
    "    else datetime.strptime(sample_metadata_map[row['accession_id']][3], timestamp_format).strftime('%H:%M:%S'),\n",
    "    axis=1\n",
    ")\n",
    "haploid_samples['datetime'] = pd.to_datetime(haploid_samples['day'] + ' ' + haploid_samples['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique device types per haploid samples \n",
    "device_types_per_tmpt = haploid_samples.groupby(['species', 'subject_id', 'datetime'])['device_type'].nunique().reset_index()\n",
    "\n",
    "# Filter timepoints with more than 2 unique device types\n",
    "timepoints_with_gt2 = device_types_per_tmpt[device_types_per_tmpt['device_type'] >= 2]\n",
    "\n",
    "# Count species-subject pairs with at least two such timepoints\n",
    "species_subject_pairs = timepoints_with_gt2.groupby(['species', 'subject_id']).filter(lambda x: len(x) >= 2).reset_index(drop = True)\n",
    "\n",
    "# Optionally, re-group the filtered data for further analysis\n",
    "grouped_species_subject_pairs = species_subject_pairs[['species', 'subject_id']].drop_duplicates().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alistipes_onderdonkii_55464</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alistipes_onderdonkii_55464</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alistipes_putredinis_61533</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alistipes_putredinis_61533</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bacteroides_massiliensis_44749</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bifidobacterium_adolescentis_56815</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bilophila_wadsworthia_57364</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bilophila_wadsworthia_57364</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Catenibacterium_mitsuokai_61547</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Catenibacterium_mitsuokai_61547</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Desulfovibrio_piger_61475</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Escherichia_coli_58110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Eubacterium_hallii_61477</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Guyana_massiliensis_60772</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Parabacteroides_distasonis_56985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Phascolarctobacterium_sp_59817</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ruminococcus_bromii_62047</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ruminococcus_torques_62045</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Streptococcus_thermophilus_54772</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               species  subject_id\n",
       "0          Alistipes_onderdonkii_55464           9\n",
       "1          Alistipes_onderdonkii_55464          11\n",
       "2           Alistipes_putredinis_61533           9\n",
       "3           Alistipes_putredinis_61533          11\n",
       "4       Bacteroides_massiliensis_44749           2\n",
       "5           Bacteroides_vulgatus_57955           2\n",
       "6           Bacteroides_vulgatus_57955           8\n",
       "7           Bacteroides_vulgatus_57955          11\n",
       "8   Bifidobacterium_adolescentis_56815          12\n",
       "9          Bilophila_wadsworthia_57364          11\n",
       "10         Bilophila_wadsworthia_57364          12\n",
       "11     Catenibacterium_mitsuokai_61547           9\n",
       "12     Catenibacterium_mitsuokai_61547          13\n",
       "13           Desulfovibrio_piger_61475           9\n",
       "14              Escherichia_coli_58110           3\n",
       "15            Eubacterium_hallii_61477           9\n",
       "16           Guyana_massiliensis_60772           6\n",
       "17    Parabacteroides_distasonis_56985           1\n",
       "18      Phascolarctobacterium_sp_59817           8\n",
       "19           Ruminococcus_bromii_62047          12\n",
       "20          Ruminococcus_torques_62045          12\n",
       "21    Streptococcus_thermophilus_54772          13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_species_subject_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate 3: How many of these have (1) temporal changes (2) within host changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_vec = []\n",
    "between_vec = []\n",
    "for i,row in grouped_species_subject_pairs.iterrows():\n",
    "    species = row[\"species\"]\n",
    "    subject_id = row[\"subject_id\"]\n",
    "\n",
    "    orientations_with_changes = change_summary_df[(change_summary_df['species'] == species) & (change_summary_df['subject_1'] == subject_id) & (change_summary_df['snv_changes'] > 0)].timepoint_orientation.unique().tolist()\n",
    "\n",
    "    if \"Within timepoint\" in orientations_with_changes:\n",
    "        within_vec.append(True)\n",
    "    else:\n",
    "        within_vec.append(False)\n",
    "    \n",
    "    if \"Between timepoint\" in orientations_with_changes:\n",
    "        between_vec.append(True)\n",
    "    else:\n",
    "        between_vec.append(False)\n",
    "\n",
    "\n",
    "change_type_summary_df = grouped_species_subject_pairs.copy()\n",
    "\n",
    "change_type_summary_df[\"within_timepoint_changes\"] = within_vec\n",
    "change_type_summary_df[\"between_timepoint_changes\"] = between_vec\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>within_timepoint_changes</th>\n",
       "      <th>between_timepoint_changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alistipes_onderdonkii_55464</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alistipes_putredinis_61533</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bacteroides_massiliensis_44749</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilophila_wadsworthia_57364</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Desulfovibrio_piger_61475</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Escherichia_coli_58110</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Parabacteroides_distasonis_56985</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            species  subject_id  within_timepoint_changes  \\\n",
       "0       Alistipes_onderdonkii_55464          11                     False   \n",
       "1        Alistipes_putredinis_61533          11                     False   \n",
       "2    Bacteroides_massiliensis_44749           2                      True   \n",
       "3        Bacteroides_vulgatus_57955           8                      True   \n",
       "4        Bacteroides_vulgatus_57955          11                      True   \n",
       "5       Bilophila_wadsworthia_57364          11                     False   \n",
       "6         Desulfovibrio_piger_61475           9                      True   \n",
       "7            Escherichia_coli_58110           3                     False   \n",
       "8  Parabacteroides_distasonis_56985           1                     False   \n",
       "\n",
       "   between_timepoint_changes  \n",
       "0                       True  \n",
       "1                       True  \n",
       "2                       True  \n",
       "3                       True  \n",
       "4                       True  \n",
       "5                       True  \n",
       "6                       True  \n",
       "7                       True  \n",
       "8                       True  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_type_summary_df[change_type_summary_df.between_timepoint_changes].reset_index(drop = True\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>within_timepoint_changes</th>\n",
       "      <th>between_timepoint_changes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alistipes_onderdonkii_55464</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alistipes_onderdonkii_55464</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alistipes_putredinis_61533</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alistipes_putredinis_61533</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bacteroides_massiliensis_44749</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bacteroides_vulgatus_57955</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bifidobacterium_adolescentis_56815</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bilophila_wadsworthia_57364</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bilophila_wadsworthia_57364</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Catenibacterium_mitsuokai_61547</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Catenibacterium_mitsuokai_61547</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Desulfovibrio_piger_61475</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Escherichia_coli_58110</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Eubacterium_hallii_61477</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Guyana_massiliensis_60772</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Parabacteroides_distasonis_56985</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Phascolarctobacterium_sp_59817</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ruminococcus_bromii_62047</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ruminococcus_torques_62045</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Streptococcus_thermophilus_54772</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               species  subject_id  within_timepoint_changes  \\\n",
       "0          Alistipes_onderdonkii_55464           9                     False   \n",
       "1          Alistipes_onderdonkii_55464          11                     False   \n",
       "2           Alistipes_putredinis_61533           9                     False   \n",
       "3           Alistipes_putredinis_61533          11                     False   \n",
       "4       Bacteroides_massiliensis_44749           2                      True   \n",
       "5           Bacteroides_vulgatus_57955           2                     False   \n",
       "6           Bacteroides_vulgatus_57955           8                      True   \n",
       "7           Bacteroides_vulgatus_57955          11                      True   \n",
       "8   Bifidobacterium_adolescentis_56815          12                     False   \n",
       "9          Bilophila_wadsworthia_57364          11                     False   \n",
       "10         Bilophila_wadsworthia_57364          12                     False   \n",
       "11     Catenibacterium_mitsuokai_61547           9                     False   \n",
       "12     Catenibacterium_mitsuokai_61547          13                     False   \n",
       "13           Desulfovibrio_piger_61475           9                      True   \n",
       "14              Escherichia_coli_58110           3                     False   \n",
       "15            Eubacterium_hallii_61477           9                     False   \n",
       "16           Guyana_massiliensis_60772           6                     False   \n",
       "17    Parabacteroides_distasonis_56985           1                     False   \n",
       "18      Phascolarctobacterium_sp_59817           8                     False   \n",
       "19           Ruminococcus_bromii_62047          12                     False   \n",
       "20          Ruminococcus_torques_62045          12                     False   \n",
       "21    Streptococcus_thermophilus_54772          13                     False   \n",
       "\n",
       "    between_timepoint_changes  \n",
       "0                       False  \n",
       "1                        True  \n",
       "2                       False  \n",
       "3                        True  \n",
       "4                        True  \n",
       "5                       False  \n",
       "6                        True  \n",
       "7                        True  \n",
       "8                       False  \n",
       "9                        True  \n",
       "10                      False  \n",
       "11                      False  \n",
       "12                      False  \n",
       "13                       True  \n",
       "14                       True  \n",
       "15                      False  \n",
       "16                      False  \n",
       "17                       True  \n",
       "18                      False  \n",
       "19                      False  \n",
       "20                      False  \n",
       "21                      False  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_type_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate 4: getting gene descriptsion of certain species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parse_patric\n",
    "species = \"Bacteroides_vulgatus_57955\"\n",
    "genome_ids = parse_midas_data.get_ref_genome_ids(species)\n",
    "non_shared_genes = core_gene_utils.parse_non_shared_reference_genes(species)\n",
    "gene_descriptions = parse_patric.load_patric_gene_descriptions(genome_ids, non_shared_genes)\n",
    "centroid_gene_map = parse_midas_data.load_centroid_gene_map(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TPR-domain-containing protein'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_descriptions['435590.9.peg.777']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNV_freq changes for within-host changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id_gene_dict = dict({})\n",
    "\n",
    "only_haploid = False\n",
    "only_high_coverage = False\n",
    "min_depth = 1\n",
    "final_line_number = 1e10\n",
    "timestamp_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "subject_sample_map = parse_subject_sample_map()\n",
    "\n",
    "\n",
    "# for species_subject_i,row in change_type_summary_df[change_type_summary_df.within_timepoint_changes].reset_index(drop = False).iterrows():\n",
    "for species_subject_i,row in within_timepoint_change_pairs_with_changes.rename(columns={\"subject_1\":\"subject_id\"}).reset_index(drop = False).iterrows():\n",
    "    species = row['species']\n",
    "    subject_id = row['subject_id']\n",
    "\n",
    "    # sys.stderr.write(\"Processing species %s in subject %d (%d / %d species-subject pairs)\\n\" % (species, subject_id, species_subject_i + 1, len(change_type_summary_df[change_type_summary_df.within_timepoint_changes])))\n",
    "    sys.stderr.write(\"Processing species %s in subject %d (%d / %d species-subject pairs)\\n\" % (species, subject_id, species_subject_i + 1, len(within_timepoint_change_pairs_with_changes)))\n",
    "\n",
    "    # Identify sample pair\n",
    "    species_subject_withinhost_sample_pair = change_summary_df[(change_summary_df['species'] == species) & (change_summary_df['subject_1'] == subject_id) & (change_summary_df['timepoint_orientation'] == \"Within timepoint\") & (change_summary_df['snv_changes'] > 0)][['sample_1', 'sample_2', 'snv_changes']].sort_values(by = ['snv_changes'], ascending = False)\n",
    "    sample_pair = (species_subject_withinhost_sample_pair.iloc[0,0], species_subject_withinhost_sample_pair.iloc[0,1])\n",
    "\n",
    "    # Extract SNVs\n",
    "    intersample_change_map = load_intersample_change_map(species)\n",
    "\n",
    "    if sample_pair in intersample_change_map:\n",
    "        snvs = intersample_change_map[sample_pair]['snps'][2]\n",
    "    elif (sample_pair[1], sample_pair[0]) in intersample_change_map:\n",
    "        snvs = intersample_change_map[(sample_pair[1], sample_pair[0])]['snps'][2]\n",
    "    else:\n",
    "        sys.stderr.write(\"%s not in intersample change map for %s\\n\" % (str(sample_pair), species))\n",
    "        continue\n",
    "\n",
    "    snvs = [(snv[1], snv[2]) for snv in snvs]\n",
    "    \n",
    "    # Extract SNV frequencies\n",
    "    ## Define paths\n",
    "    snps_freq_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_ref_freq.txt.bz2\")\n",
    "    snps_depth_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_depth.txt.bz2\")\n",
    "    snps_info_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_info.txt.bz2\")\n",
    "\n",
    "    ## define desired samples\n",
    "\n",
    "    if only_haploid:\n",
    "        sys.stderr.write(\"Only considering haploid samples\\n\")\n",
    "        haploid_samples = calculate_haploid_samples(species, use_HMP_freqs=True)\n",
    "        desired_samples = [desired_sample for desired_sample in desired_samples if (desired_sample in haploid_samples) & (sample_metadata_map[desired_sample][0] == str(subject_id))]\n",
    "    elif only_high_coverage:\n",
    "        sys.stderr.write(\"Only considering high coverage samples\\n\")\n",
    "        high_coverage_samples = calculate_highcoverage_samples(species)\n",
    "        desired_samples = [desired_sample for desired_sample in desired_samples if (desired_sample in high_coverage_samples) & (sample_metadata_map[desired_sample][0] == str(subject_id))]\n",
    "    else:\n",
    "        sys.stderr.write(\"Considering all samples\\n\")\n",
    "        desired_samples = list(subject_sample_map[str(subject_id)].keys())\n",
    "\n",
    "    \n",
    "    # Looping through the files\n",
    "    line_counter = 0\n",
    "    loci_extracted = 0\n",
    "    site_id_vec = []\n",
    "    allele_frequencies = []\n",
    "\n",
    "    with bz2.open(snps_freq_path, 'rt') as file,  bz2.open(snps_depth_path, 'rt') as depth_file, bz2.open(snps_info_path, 'rt') as info_file:\n",
    "        for line, depth_line, info_line in zip(file, depth_file, info_file):\n",
    "            if line_counter == 0:\n",
    "                items = line.split()[1:]    \n",
    "                samples_in_file = parse_merged_sample_names(items)\n",
    "                desired_sample_idxs = []\n",
    "                for sample in desired_samples:\n",
    "                    if sample not in samples_in_file:\n",
    "                        continue\n",
    "                    desired_sample_idxs.append( np.nonzero(samples_in_file==sample)[0][0] )\n",
    "                desired_sample_idxs = np.array(desired_sample_idxs)    \n",
    "                desired_samples = samples_in_file[desired_sample_idxs]\n",
    "\n",
    "                line_counter += 1\n",
    "            else:\n",
    "                if line_counter == final_line_number:\n",
    "                    sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \". \" + str(loci_extracted)+ \" loci extracted.\\n\")\n",
    "                    break\n",
    "\n",
    "                items = line.split()\n",
    "                depths =  depth_line.split()\n",
    "                infos = info_line.split()\n",
    "                if len(infos) >= 7:\n",
    "                    gene_id = infos[6]\n",
    "                else: \n",
    "                    gene_id = \"\"\n",
    "                site_id_original = items[0]\n",
    "                site_id = items[0].split(\"|\")\n",
    "                site_id = (site_id[0], int(site_id[1]))\n",
    "                items = items[1:]\n",
    "                depths = [int(depth) for depth in depths[1:]]     \n",
    "\n",
    "                if site_id in snvs:\n",
    "                    sys.stderr.write(\"EXTRACTING: \" + site_id[0] + \"|\" + str(site_id[1]) + \"\\n\")\n",
    "                    \n",
    "                    if species not in site_id_gene_dict:\n",
    "                        site_id_gene_dict[species] = dict({})\n",
    "                    \n",
    "                    if site_id not in site_id_gene_dict[species]:\n",
    "                        site_id_gene_dict[species][site_id] = gene_id\n",
    "                    \n",
    "                    site_id_vec.append(site_id_original)\n",
    "                    loci_extracted += 1\n",
    "                    af = []\n",
    "                    dp = [] \n",
    "                    for idx in desired_sample_idxs:    \n",
    "                        item = items[idx]\n",
    "                        depth = depths[idx]\n",
    "                        if depth < min_depth:\n",
    "                            subitems = [np.nan]\n",
    "                        else:\n",
    "                            subitems = item.split(\",\")\n",
    "                        af.append(subitems[0])\n",
    "                        dp.append(depth)\n",
    "\n",
    "                    if loci_extracted == 1:\n",
    "                        allele_frequencies = np.array(af)\n",
    "                        site_depths = np.array(dp)\n",
    "                        if len(snvs) == 1:\n",
    "                            allele_frequencies = allele_frequencies[np.newaxis, :]\n",
    "                            site_depths = site_depths[np.newaxis, :]\n",
    "                    else:\n",
    "                        allele_frequencies = np.vstack((allele_frequencies, np.array(af)))\n",
    "                        site_depths = np.vstack((site_depths, np.array(dp)))\n",
    "                        \n",
    "                    \n",
    "                if (loci_extracted == len(snvs)):\n",
    "                    sys.stderr.write(\"Extracted all loci. Annotating.\\n\")\n",
    "                    break\n",
    "    ## ANNOTATION\n",
    "    ### FREQS\n",
    "    snv_freqs = pd.DataFrame(columns = desired_samples, index=site_id_vec, data = allele_frequencies)\n",
    "    snv_freqs = snv_freqs.reset_index().rename(columns = {\"index\":\"site_id\"})\n",
    "    snv_freqs[['contig', 'site_pos', 'nucleotide']] = snv_freqs['site_id'].str.split(\"|\", expand=True)\n",
    "    ### DEPTHS\n",
    "    snv_depths = pd.DataFrame(columns=desired_samples,index=site_id_vec,data=site_depths)\n",
    "    snv_depths = snv_depths.reset_index().rename(columns={\"index\": \"site_id\"})\n",
    "    snv_freqs[['contig', 'site_pos', 'nucleotide']] = snv_freqs['site_id'].str.split(\"|\", expand=True)\n",
    "    snv_depths[['contig', 'site_pos', 'nucleotide']] = snv_depths['site_id'].str.split(\"|\", expand=True)\n",
    "    # convert allele frequency to numeric\n",
    "    snv_freqs[list(desired_samples)] = snv_freqs[list(desired_samples)].apply(pd.to_numeric, errors='coerce')\n",
    "    snv_depths[list(desired_samples)] = snv_depths[list(desired_samples)].apply(pd.to_numeric, errors='coerce')\n",
    "    # convert to longform\n",
    "    snv_freqs =pd.melt(snv_freqs,\n",
    "                    id_vars=['site_id', 'contig', 'site_pos', 'nucleotide'],  # columns to keep as identifier variables\n",
    "                    var_name='sample',  # the name for the sample column\n",
    "                    value_name='allele_frequency'  # the name for the values from sample columns\n",
    "                    )\n",
    "    snv_depths = pd.melt(\n",
    "        snv_depths,\n",
    "        id_vars=['site_id'],\n",
    "        var_name='sample',\n",
    "        value_name='depth'\n",
    "    )\n",
    "    # Merge depth and AFs\n",
    "    snv_freqs = snv_freqs.merge(snv_depths[['site_id', 'sample', 'depth']],\n",
    "                                on=['site_id', 'sample'],\n",
    "                                how='left')\n",
    "\n",
    "    # species and subject\n",
    "    snv_freqs['species'] = species\n",
    "    snv_freqs['subject_id'] = subject_id\n",
    "    # annotate with sample type (e.g., \"Stool\", \"Saliva\", \"Capsule 2\", etc.)\n",
    "    snv_freqs['sample_type'] = snv_freqs['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "    # annotate with date\n",
    "    snv_freqs['date'] = snv_freqs[['sample','sample_type']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample']][5], timestamp_format).strftime('%Y-%m-%d') if row['sample_type'] == \"Stool\" or row['sample_type'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample']][3], timestamp_format).strftime('%Y-%m-%d'), axis = 1)\n",
    "    # annotate with time\n",
    "    snv_freqs['time'] = snv_freqs[['sample','sample_type']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample']][5], timestamp_format).strftime('%H:%M:%S') if row['sample_type'] == \"Stool\" or row['sample_type'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample']][3], timestamp_format).strftime('%H:%M:%S'), axis = 1)\n",
    "    # annotate with timepoint\n",
    "    snv_freqs['timepoint'] = pd.to_datetime(snv_freqs['date'] + ' ' + snv_freqs['time'])\n",
    "    if species_subject_i == 0:\n",
    "        snv_freqs_combined = snv_freqs.copy()\n",
    "    else:\n",
    "        snv_freqs_combined = pd.concat([snv_freqs_combined, snv_freqs], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "snv_freqs_combined = snv_freqs_combined[['species', \n",
    "                                         'subject_id', \n",
    "                                         'sample_type', \n",
    "                                         'date',\n",
    "                                         'time', \n",
    "                                         'timepoint',\n",
    "                                         'site_id', \n",
    "                                         'contig', \n",
    "                                         'site_pos', \n",
    "                                         'nucleotide', \n",
    "                                         'sample',\n",
    "                                         'allele_frequency',\n",
    "                                         'depth']]\n",
    "\n",
    "sys.stderr.write(\"\\n\\nFinished. Combined output in snv_freqs_combined\\n\")\n",
    "\n",
    "                \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_freqs_WithinTimepoint.tsv\"\n",
    "# out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_freqs_WithinTimepoint_atleast1timepoint.tsv\"\n",
    "snv_freqs_combined.to_csv(out_path, sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_summary_df[change_type_summary_df.between_timepoint_changes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_haploid = False\n",
    "only_high_coverage = False\n",
    "min_depth = 1\n",
    "final_line_number = 1e10\n",
    "timestamp_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "subject_sample_map = parse_subject_sample_map()\n",
    "\n",
    "\n",
    "for species_subject_i,row in change_type_summary_df[change_type_summary_df.between_timepoint_changes].reset_index(drop = False).iterrows():\n",
    "    species = row['species']\n",
    "    subject_id = row['subject_id']\n",
    "\n",
    "    sys.stderr.write(\"Processing species %s in subject %d (%d / %d species-subject pairs)\\n\" % (species, subject_id, species_subject_i + 1, len(change_type_summary_df[change_type_summary_df.between_timepoint_changes])))\n",
    "\n",
    "    # Identify sample pair\n",
    "    species_subject_between_sample_pair = change_summary_df[(change_summary_df['species'] == species) & (change_summary_df['subject_1'] == subject_id) & (change_summary_df['timepoint_orientation'] == \"Between timepoint\") & (change_summary_df['snv_changes'] > 0)][['sample_1', 'sample_2', 'snv_changes']].sort_values(by = ['snv_changes'], ascending = False)\n",
    "    sample_pair = (species_subject_between_sample_pair.iloc[0,0], species_subject_between_sample_pair.iloc[0,1])\n",
    "\n",
    "    # Extract SNVs\n",
    "    intersample_change_map = load_intersample_change_map(species)\n",
    "\n",
    "    snvs = []\n",
    "\n",
    "    # Loop through all sample pairs\n",
    "    for _, row in species_subject_between_sample_pair.iterrows():\n",
    "        sample_pair = (row['sample_1'], row['sample_2'])\n",
    "\n",
    "        # Try both orientations\n",
    "        if sample_pair in intersample_change_map:\n",
    "            snvs_temp = intersample_change_map[sample_pair]['snps'][2]\n",
    "        elif (sample_pair[1], sample_pair[0]) in intersample_change_map:\n",
    "            snvs_temp = intersample_change_map[(sample_pair[1], sample_pair[0])]['snps'][2]\n",
    "        else:\n",
    "            sys.stderr.write(\n",
    "                f\"{sample_pair} not in intersample change map for {species}\\n\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Append raw SNV list to final list\n",
    "        snvs = snvs + snvs_temp\n",
    "\n",
    "    snvs = list(set([(snv[1], snv[2]) for snv in snvs]))\n",
    "    \n",
    "    # Extract SNV frequencies\n",
    "    ## Define paths\n",
    "    snps_freq_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_ref_freq.txt.bz2\")\n",
    "    snps_depth_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_depth.txt.bz2\")\n",
    "    snps_info_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_info.txt.bz2\")\n",
    "\n",
    "    ## define desired samples\n",
    "\n",
    "    if only_haploid:\n",
    "        sys.stderr.write(\"Only considering haploid samples\\n\")\n",
    "        haploid_samples = calculate_haploid_samples(species, use_HMP_freqs=True)\n",
    "        desired_samples = [desired_sample for desired_sample in desired_samples if (desired_sample in haploid_samples) & (sample_metadata_map[desired_sample][0] == str(subject_id))]\n",
    "    elif only_high_coverage:\n",
    "        sys.stderr.write(\"Only considering high coverage samples\\n\")\n",
    "        high_coverage_samples = calculate_highcoverage_samples(species)\n",
    "        desired_samples = [desired_sample for desired_sample in desired_samples if (desired_sample in high_coverage_samples) & (sample_metadata_map[desired_sample][0] == str(subject_id))]\n",
    "    else:\n",
    "        sys.stderr.write(\"Considering all samples\\n\")\n",
    "        desired_samples = list(subject_sample_map[str(subject_id)].keys())\n",
    "\n",
    "    \n",
    "    # Looping through the files\n",
    "    line_counter = 0\n",
    "    loci_extracted = 0\n",
    "    site_id_vec = []\n",
    "    allele_frequencies = []\n",
    "\n",
    "    with bz2.open(snps_freq_path, 'rt') as file,  bz2.open(snps_depth_path, 'rt') as depth_file, bz2.open(snps_info_path, 'rt') as info_file:\n",
    "        for line, depth_line, info_line in zip(file, depth_file, info_file):\n",
    "            if line_counter == 0:\n",
    "                items = line.split()[1:]    \n",
    "                samples_in_file = parse_merged_sample_names(items)\n",
    "                desired_sample_idxs = []\n",
    "                for sample in desired_samples:\n",
    "                    if sample not in samples_in_file:\n",
    "                        continue\n",
    "                    desired_sample_idxs.append( np.nonzero(samples_in_file==sample)[0][0] )\n",
    "                desired_sample_idxs = np.array(desired_sample_idxs)    \n",
    "                desired_samples = samples_in_file[desired_sample_idxs]\n",
    "\n",
    "                line_counter += 1\n",
    "            else:\n",
    "                if line_counter == final_line_number:\n",
    "                    sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \". \" + str(loci_extracted)+ \" loci extracted.\\n\")\n",
    "                    break\n",
    "\n",
    "                items = line.split()\n",
    "                depths =  depth_line.split()\n",
    "                infos = info_line.split()\n",
    "                if len(infos) >= 7:\n",
    "                    gene_id = infos[6]\n",
    "                else: \n",
    "                    gene_id = \"\"\n",
    "                site_id_original = items[0]\n",
    "                site_id = items[0].split(\"|\")\n",
    "                site_id = (site_id[0], int(site_id[1]))\n",
    "                items = items[1:]\n",
    "                depths = [int(depth) for depth in depths[1:]]     \n",
    "\n",
    "                if site_id in snvs:\n",
    "                    sys.stderr.write(\"EXTRACTING: \" + site_id[0] + \"|\" + str(site_id[1]) + \"\\n\")\n",
    "                    site_id_vec.append(site_id_original)\n",
    "                    loci_extracted += 1\n",
    "                    af = []\n",
    "                    dp = [] \n",
    "                    for idx in desired_sample_idxs:    \n",
    "                        item = items[idx]\n",
    "                        depth = depths[idx]\n",
    "                        if depth < min_depth:\n",
    "                            subitems = [np.nan]\n",
    "                        else:\n",
    "                            subitems = item.split(\",\")\n",
    "                        af.append(subitems[0])\n",
    "                        dp.append(depth)\n",
    "\n",
    "                    if loci_extracted == 1:\n",
    "                        allele_frequencies = np.array(af)\n",
    "                        site_depths = np.array(dp)\n",
    "                        if len(snvs) == 1:\n",
    "                            allele_frequencies = allele_frequencies[np.newaxis, :]\n",
    "                            site_depths = site_depths[np.newaxis, :]\n",
    "                    else:\n",
    "                        allele_frequencies = np.vstack((allele_frequencies, np.array(af)))\n",
    "                        site_depths = np.vstack((site_depths, np.array(dp)))\n",
    "                        \n",
    "                    \n",
    "                if (loci_extracted == len(snvs)):\n",
    "                    sys.stderr.write(\"Extracted all loci. Annotating.\\n\")\n",
    "                    break\n",
    "    ## ANNOTATION\n",
    "    ### FREQS\n",
    "    snv_freqs = pd.DataFrame(columns = desired_samples, index=site_id_vec, data = allele_frequencies)\n",
    "    snv_freqs = snv_freqs.reset_index().rename(columns = {\"index\":\"site_id\"})\n",
    "    snv_freqs[['contig', 'site_pos', 'nucleotide']] = snv_freqs['site_id'].str.split(\"|\", expand=True)\n",
    "    ### DEPTHS\n",
    "    snv_depths = pd.DataFrame(columns=desired_samples,index=site_id_vec,data=site_depths)\n",
    "    snv_depths = snv_depths.reset_index().rename(columns={\"index\": \"site_id\"})\n",
    "    snv_freqs[['contig', 'site_pos', 'nucleotide']] = snv_freqs['site_id'].str.split(\"|\", expand=True)\n",
    "    snv_depths[['contig', 'site_pos', 'nucleotide']] = snv_depths['site_id'].str.split(\"|\", expand=True)\n",
    "    # convert allele frequency to numeric\n",
    "    snv_freqs[list(desired_samples)] = snv_freqs[list(desired_samples)].apply(pd.to_numeric, errors='coerce')\n",
    "    snv_depths[list(desired_samples)] = snv_depths[list(desired_samples)].apply(pd.to_numeric, errors='coerce')\n",
    "    # convert to longform\n",
    "    snv_freqs =pd.melt(snv_freqs,\n",
    "                    id_vars=['site_id', 'contig', 'site_pos', 'nucleotide'],  # columns to keep as identifier variables\n",
    "                    var_name='sample',  # the name for the sample column\n",
    "                    value_name='allele_frequency'  # the name for the values from sample columns\n",
    "                    )\n",
    "    snv_depths = pd.melt(\n",
    "        snv_depths,\n",
    "        id_vars=['site_id'],\n",
    "        var_name='sample',\n",
    "        value_name='depth'\n",
    "    )\n",
    "    # Merge depth and AFs\n",
    "    snv_freqs = snv_freqs.merge(snv_depths[['site_id', 'sample', 'depth']],\n",
    "                                on=['site_id', 'sample'],\n",
    "                                how='left')\n",
    "\n",
    "    # species and subject\n",
    "    snv_freqs['species'] = species\n",
    "    snv_freqs['subject_id'] = subject_id\n",
    "    # annotate with sample type (e.g., \"Stool\", \"Saliva\", \"Capsule 2\", etc.)\n",
    "    snv_freqs['sample_type'] = snv_freqs['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "    # annotate with date\n",
    "    snv_freqs['date'] = snv_freqs[['sample','sample_type']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample']][5], timestamp_format).strftime('%Y-%m-%d') if row['sample_type'] == \"Stool\" or row['sample_type'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample']][3], timestamp_format).strftime('%Y-%m-%d'), axis = 1)\n",
    "    # annotate with time\n",
    "    snv_freqs['time'] = snv_freqs[['sample','sample_type']].apply(lambda row: datetime.strptime(sample_metadata_map[row['sample']][5], timestamp_format).strftime('%H:%M:%S') if row['sample_type'] == \"Stool\" or row['sample_type'] == \"Saliva\" else datetime.strptime(sample_metadata_map[row['sample']][3], timestamp_format).strftime('%H:%M:%S'), axis = 1)\n",
    "    # annotate with timepoint\n",
    "    snv_freqs['timepoint'] = pd.to_datetime(snv_freqs['date'] + ' ' + snv_freqs['time'])\n",
    "    if species_subject_i == 0:\n",
    "        snv_freqs_combined = snv_freqs.copy()\n",
    "    else:\n",
    "        snv_freqs_combined = pd.concat([snv_freqs_combined, snv_freqs], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "snv_freqs_combined = snv_freqs_combined[['species', \n",
    "                                         'subject_id', \n",
    "                                         'sample_type', \n",
    "                                         'date',\n",
    "                                         'time', \n",
    "                                         'timepoint',\n",
    "                                         'site_id', \n",
    "                                         'contig', \n",
    "                                         'site_pos', \n",
    "                                         'nucleotide', \n",
    "                                         'sample',\n",
    "                                         'allele_frequency',\n",
    "                                         'depth']]\n",
    "\n",
    "sys.stderr.write(\"\\n\\nFinished. Combined output in snv_freqs_combined\\n\")\n",
    "\n",
    "                \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_freqs_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_freqs_BetweenTimepoint.tsv\"\n",
    "snv_freqs_combined.to_csv(out_path, sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNV annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METHOD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = change_summary_df[change_summary_df.snv_changes > 0][['species','sample_1','sample_2']].drop_duplicates().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_vec = []\n",
    "contig_vec = []\n",
    "site_pos_vec = []\n",
    "contig_site_pos_vec = []\n",
    "variant_type_vec = []\n",
    "gene_id_vec = []\n",
    "gene_description_vec = []\n",
    "\n",
    "last_species = \"\"\n",
    "\n",
    "\n",
    "for i,row in species_list.iterrows():\n",
    "    \n",
    "    species = row['species']\n",
    "    sample_1 = row['sample_1']\n",
    "    sample_2 = row['sample_2']\n",
    "    \n",
    "    sys.stderr.write(\"Processing %s in %s - %s sample pair (%d / %d species)\\n\" % (species, sample_1, sample_2, i+1, len(species_list)))\n",
    "\n",
    "    if species != last_species:\n",
    "        last_species = species\n",
    "        intersample_change_map = load_intersample_change_map(species)\n",
    "    \n",
    "\n",
    "    if (sample_1,sample_2) in intersample_change_map:\n",
    "        sample_pair = (sample_1,sample_2)\n",
    "        snvs = intersample_change_map[sample_pair]['snps'][2]\n",
    "    elif (sample_2,sample_1) in intersample_change_map:\n",
    "        sample_pair = (sample_2,sample_1)\n",
    "        snvs = intersample_change_map[sample_pair]['snps'][2]\n",
    "    else:\n",
    "        sys.stderr.write(\"%s not in intersample change map for %s\\n\" % (str(sample_pair), species))\n",
    "        continue\n",
    "\n",
    "\n",
    "    for snv in snvs:\n",
    "        gene_id = snv[0]\n",
    "        contig = snv[1]\n",
    "        site_pos = snv[2]\n",
    "        variant_type = snv[3]\n",
    "        contig_site_pos = contig + \"|\" + str(site_pos)\n",
    "        if contig_site_pos not in contig_site_pos_vec:\n",
    "\n",
    "            species_vec.append(species)\n",
    "            contig_vec.append(contig)\n",
    "            site_pos_vec.append(site_pos)\n",
    "            contig_site_pos_vec.append(contig_site_pos)\n",
    "            gene_id_vec.append(gene_id)\n",
    "            variant_type_vec.append(variant_type)\n",
    "\n",
    "            genome_ids = parse_midas_data.get_ref_genome_ids(species)\n",
    "            non_shared_genes = core_gene_utils.parse_non_shared_reference_genes(species)\n",
    "            gene_descriptions = parse_patric.load_patric_gene_descriptions(genome_ids, non_shared_genes)\n",
    "            centroid_gene_map = parse_midas_data.load_centroid_gene_map(species)\n",
    "\n",
    "            if gene_id in gene_descriptions:\n",
    "                gene_description_vec.append(gene_descriptions[gene_id])\n",
    "            elif gene_id in centroid_gene_map:\n",
    "                if centroid_gene_map[gene_id] in gene_descriptions:\n",
    "                    gene_description_vec.append(gene_descriptions[centroid_gene_map[gene_id]])\n",
    "            else:\n",
    "                gene_description_vec.append(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_gene_descriptions.tsv\"\n",
    "snv_gene_descriptions_df.to_csv(out_path, sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METHOD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataframes\n",
    "between_timepoint_freqs_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_freqs_BetweenTimepoint.tsv\"\n",
    "within_timepoint_freqs_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_freqs_WithinTimepoint.tsv\"\n",
    "snv_freqs_1timepoint_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/evolutionary_changes/SNV_freqs_WithinTimepoint_atleast1timepoint.tsv\"\n",
    "\n",
    "df_1 = pd.read_csv(between_timepoint_freqs_path, sep = \"\\t\")\n",
    "df_2 = pd.read_csv(within_timepoint_freqs_path, sep = \"\\t\")\n",
    "df_3 = pd.read_csv(snv_freqs_1timepoint_path, sep = \"\\t\")\n",
    "\n",
    "df_all = pd.concat([df_1, df_2, df_3], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['species','site_id','contig','site_pos']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $F_{st}$ pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all within-host sample qp pairs\n",
    "haploid_sample_dict = dict({})\n",
    "for species in species_list:\n",
    "    haploid_samples = calculate_haploid_samples(species)\n",
    "    sample_pairs = list(itertools.combinations(haploid_samples, 2))\n",
    "\n",
    "    sample_pairs = [sample_pair for sample_pair in sample_pairs if sample_metadata_map[sample_pair[0]][0] == sample_metadata_map[sample_pair[1]][0]]\n",
    "\n",
    "    # Store the sample pairs in the dictionary\n",
    "    if len(sample_pairs) > 0:\n",
    "        haploid_sample_dict[species] = sample_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make fst dataframe\n",
    "fst_processing = []\n",
    "for species in haploid_sample_dict.keys():\n",
    "    \n",
    "    for sample_pair in haploid_sample_dict[species]:\n",
    "        \n",
    "        sample_1 = sample_pair[0]\n",
    "        sample_2 = sample_pair[1]\n",
    "\n",
    "        fst_processing.append([species,sample_1,sample_2])\n",
    "\n",
    "fst_processing_df = pd.DataFrame(fst_processing, columns = ['species','sample_1', 'sample_2'])\n",
    "\n",
    "# saving \n",
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/metadata/Fst_processing_df.txt\"\n",
    "fst_processing_df.to_csv(out_path, sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing pre-identified changes\n",
    "\n",
    "[Within-host change](https://diversityalonggutshared.blogspot.com/2024/04/shalon-results-all-within-host-snvs.html)  \n",
    "[Within-set changes](https://diversityalonggutshared.blogspot.com/2024/04/do-within-host-changes-spread-shalon-et.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_species_subject_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Alistipes_onderdonkii_55464\"\n",
    "subject_id = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load intersample changes\n",
    "intersample_changes = load_intersample_change_map(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all within-host SNVs\n",
    "\n",
    "subject_samples = list(subject_sample_map[str(subject_id)].keys())\n",
    "haploid_samples = calculate_haploid_samples(species, use_HMP_freqs=True)\n",
    "subject_samples = [sample_ for sample_ in subject_samples if sample_ in haploid_samples]\n",
    "# sample_pair_list = list(itertools.combinations(subject_samples, 2))\n",
    "sample_pair_list = list(change_summary_df.loc[(change_summary_df['species'] == species) & (change_summary_df['subject_1'] == str(subject_id)),['sample_1', 'sample_2']].itertuples(index=False, name=None))\n",
    "\n",
    "# paring intersample changes to only include within-host snvs\n",
    "intersample_changes = {key: value for key, value in intersample_changes.items() if (key in sample_pair_list) or ((key[1], key[0]) in sample_pair_list)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNV trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snvs = set({})\n",
    "# for key in intersample_changes.keys():\n",
    "for key in sample_pair_list:\n",
    "    snv_list = set(intersample_changes[key]['snps'][2])\n",
    "    snvs.update(snv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for core or non-shared genes\n",
    "core_genes_only = False\n",
    "\n",
    "if core_genes_only:\n",
    "    sys.stderr.write(\"Filtering for core genes\\n\")\n",
    "else:\n",
    "    sys.stderr.write(\"Filtering for non-shared genes\\n\")\n",
    "    \n",
    "\n",
    "if core_genes_only:\n",
    "    core_genes = parse_core_genes(species)\n",
    "    centroid_gene_map = load_centroid_gene_map(species)\n",
    "    desired_gene_ids = {value if key in core_genes else key for key, value in centroid_gene_map.items() if key in core_genes or value in core_genes}.union(core_genes)\n",
    "    snvs = [snv for snv in snvs if snv[0] in desired_gene_ids]\n",
    "else:\n",
    "    non_shared_genes = parse_non_shared_reference_genes(species)\n",
    "    centroid_gene_map = load_centroid_gene_map(species)\n",
    "    desired_gene_ids = {value if key in non_shared_genes else key for key, value in centroid_gene_map.items() if key in non_shared_genes or value in non_shared_genes}.union(non_shared_genes)\n",
    "    snvs = [snv for snv in snvs if snv[0] in desired_gene_ids]\n",
    "\n",
    "# snvs = [(snv[1], snv[2], snv[0]) for snv in snvs]\n",
    "snvs = list(set([(snv[1], snv[2]) for snv in sorted(snvs, key=lambda x: (x[1], x[2]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate snvs into clusters\n",
    "\n",
    "clust_dist = 1e4\n",
    "contigs = {snv[0] for snv in snvs}\n",
    "snv_clusters = []\n",
    "start = 0\n",
    "for i,contig in enumerate(contigs):\n",
    "    snvs_contig = [snv for snv in snvs if snv[0] == contig]\n",
    "    snvs_contig = [snv for snv in sorted(snvs_contig, key=lambda x: (x[0], x[1]))]\n",
    "    loci = np.array([snv[1] for snv in snvs_contig])\n",
    "    gaps = np.diff(loci)\n",
    "    breaks = np.where(gaps > clust_dist)[0]\n",
    "    start = 0\n",
    "    for end in breaks:\n",
    "        end = end+1\n",
    "        snv_clusters.append(list(snvs_contig[start:end]))\n",
    "        start = end\n",
    "    snv_clusters.append(list(snvs_contig[start:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading subject samples\n",
    "subject_samples = list(subject_sample_map[str(subject_id)].keys())\n",
    "snps_freq_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_ref_freq.txt.bz2\")\n",
    "# Open the compressed file in binary mode\n",
    "with bz2.open(snps_freq_path, 'rb') as file:\n",
    "    first_line = file.readline().strip() \n",
    "snps_samples = first_line.decode('utf-8').split(\"\\t\")[1:]\n",
    "\n",
    "subject_samples = [sample_ for sample_ in subject_samples if sample_ in snps_samples]\n",
    "\n",
    "\n",
    "# Creating sample indices and sorting\n",
    "sample_type = [sample_metadata_map[f][2] for f in subject_samples]\n",
    "stool_idx = [i for i, element in enumerate(sample_type) if element == \"Stool\"]\n",
    "saliva_idx = [i for i, element in enumerate(sample_type) if element == \"Saliva\"]\n",
    "timestamp_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "# Create a list of tuples containing sorting keys\n",
    "sorting_keys = [\n",
    "(\n",
    "    datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%Y-%m-%d') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%Y-%m-%d'),\n",
    "    datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%H:%M:%S') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%H:%M:%S'),\n",
    "    sample_metadata_map[f][2]  # sample_type\n",
    ")\n",
    "for i, f in enumerate(subject_samples)\n",
    "]\n",
    "\n",
    "# Sort subject_samples based on the sorting keys\n",
    "desired_samples = [x for _, x in sorted(zip(sorting_keys, subject_samples))]\n",
    "\n",
    "# Creating sample indices with sorted\n",
    "sample_type = [sample_metadata_map[f][2] for f in desired_samples]\n",
    "stool_idx = [i for i, element in enumerate(sample_type) if element == \"Stool\"]\n",
    "saliva_idx = [i for i, element in enumerate(sample_type) if element == \"Saliva\"]\n",
    "\n",
    "# creating a dictionary\n",
    "sample_sorting_dict = {\n",
    "    f: (\n",
    "        datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%Y-%m-%d') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%Y-%m-%d'),\n",
    "        datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%H:%M:%S') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%H:%M:%S'),\n",
    "        sample_metadata_map[f][2],  # sample_type\n",
    "        i\n",
    "    )\n",
    "    for i, f in enumerate(desired_samples)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting SNV frequencies\n",
    "\n",
    "only_haploid = True\n",
    "only_high_coverage = False\n",
    "min_depth = 5\n",
    "\n",
    "x_axis_dictionary = {}\n",
    "\n",
    "snps_freq_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_ref_freq.txt.bz2\")\n",
    "snps_depth_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_depth.txt.bz2\")\n",
    "snps_info_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_info.txt.bz2\")\n",
    "\n",
    "\n",
    "final_line_number = 1e10\n",
    "\n",
    "if only_haploid:\n",
    "    haploid_samples = calculate_haploid_samples(species, use_HMP_freqs=True)\n",
    "    desired_samples = [desired_sample for desired_sample in desired_samples if desired_sample in haploid_samples]\n",
    "elif only_high_coverage:\n",
    "    high_coverage_samples = calculate_highcoverage_samples(species)\n",
    "    desired_samples = [desired_sample for desired_sample in desired_samples if desired_sample in high_coverage_samples]\n",
    "\n",
    "# if (len(baseline_sample) == 0) | (baseline_sample not in desired_samples):\n",
    "#     if len(recipient_samples) > 0:\n",
    "#         baseline_sample = recipient_samples[0]\n",
    "#     else:\n",
    "#         baseline_sample = desired_samples[0]\n",
    "\n",
    "line_counter = 0\n",
    "current_cluster = 0\n",
    "clusters_finished = False\n",
    "loci_extracted = [0]*len(snv_clusters)\n",
    "site_id_vec = [[] for _ in snv_clusters]\n",
    "allele_frequencies = [[] for _ in snv_clusters]\n",
    "\n",
    "with bz2.open(snps_freq_path, 'rt') as file,  bz2.open(snps_depth_path, 'rt') as depth_file, bz2.open(snps_info_path, 'rt') as info_file:\n",
    "    for line, depth_line, info_line in zip(file, depth_file, info_file):\n",
    "        if line_counter == 0:\n",
    "            items = line.split()[1:]    \n",
    "            samples_in_file = parse_merged_sample_names(items)\n",
    "            desired_sample_idxs = []\n",
    "            for sample in desired_samples:\n",
    "                if sample not in samples_in_file:\n",
    "                    continue\n",
    "                desired_sample_idxs.append( np.nonzero(samples_in_file==sample)[0][0] )\n",
    "                # if plot_strain_snvs:\n",
    "                #     strain_tracking_sample_idx = np.nonzero(samples_in_file==strain_tracking_sample)[0][0]\n",
    "                #     strain_tracking_sample_foil_idx = np.nonzero(samples_in_file==strain_tracking_sample_foil)[0][0]\n",
    "\n",
    "            desired_sample_idxs = np.array(desired_sample_idxs)    \n",
    "            desired_samples = samples_in_file[desired_sample_idxs]\n",
    "            sample_sorting_dict = {key:val for key,val in sample_sorting_dict.items() if key in desired_samples}\n",
    "            sample_sorting_dict = {k: sample_sorting_dict[k][:3] + (i,) for i, k in enumerate(sample_sorting_dict.keys())}\n",
    "\n",
    "            line_counter += 1\n",
    "        else:\n",
    "            if line_counter == final_line_number:\n",
    "                sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \"\\n\")\n",
    "                break\n",
    "            if (sum(loci_extracted) == len(snvs)):\n",
    "                sys.stderr.write(\"All loci extracted\\n\")\n",
    "                break\n",
    "            \n",
    "            items = line.split()\n",
    "            depths =  depth_line.split()\n",
    "            infos = info_line.split()\n",
    "            if len(infos) >= 7:\n",
    "                gene_id = infos[6]\n",
    "            else: \n",
    "                gene_id = \"\"\n",
    "            site_id_original = items[0]\n",
    "            site_id = items[0].split(\"|\")\n",
    "            site_id = (site_id[0], int(site_id[1]))\n",
    "            items = items[1:]\n",
    "            depths = [int(depth) for depth in depths[1:]]     \n",
    "\n",
    "            for cluster_i,cluster in enumerate(snv_clusters):\n",
    "                if site_id in cluster:\n",
    "                    sys.stderr.write(\"EXTRACTING: \" + site_id[0] + \"|\" + str(site_id[1]) + \" in cluster \" + str(cluster_i+1) + \"\\n\")\n",
    "                    site_id_vec[cluster_i].append(site_id_original)\n",
    "                    loci_extracted[cluster_i] += 1\n",
    "                    af = []\n",
    "                    for idx in desired_sample_idxs:    \n",
    "                        item = items[idx]\n",
    "                        depth = depths[idx]\n",
    "                        if depth < min_depth:\n",
    "                            subitems = [np.nan]\n",
    "                        else:\n",
    "                            subitems = item.split(\",\")\n",
    "                        af.append(subitems[0])\n",
    "                    if loci_extracted[cluster_i] == 1:\n",
    "                        current_cluster += 1\n",
    "                        allele_frequencies[cluster_i] = np.array(af)\n",
    "                        if len(snv_clusters[cluster_i]) == 1:\n",
    "                            allele_frequencies[cluster_i] = allele_frequencies[cluster_i][np.newaxis, :]\n",
    "                    else:\n",
    "                        allele_frequencies[cluster_i] = np.vstack((allele_frequencies[cluster_i], np.array(af)))\n",
    "                    break\n",
    "            if (loci_extracted[cluster_i] == len(cluster)) & (not clusters_finished):\n",
    "                if (sum(loci_extracted) == len(snvs)):\n",
    "                    clusters_finished = True\n",
    "                sys.stderr.write(\"Extracted \" + str(loci_extracted[cluster_i]) + \" SNVs in cluster \" + str(cluster_i+1) + \"\\n\")\n",
    "            \n",
    "            line_counter += 1\n",
    "            # if line_counter % 1000 == 0:\n",
    "            #     print(site_id)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANNOTATION\n",
    "snv_freqs = []\n",
    "sorting_dict = {key: idx for idx, key in enumerate(sample_sorting_dict.keys())}\n",
    "polarize_sample = [sample_ for sample_ in sample_sorting_dict.keys() if (sample_sorting_dict[sample_][2] != \"Stool\") & (sample_sorting_dict[sample_][2] != \"Saliva\")][0]\n",
    "for cluster_i, cluster in enumerate(snv_clusters):\n",
    "    # creating dataframe\n",
    "    snv_freqs.append(pd.DataFrame(columns = desired_samples, index=site_id_vec[cluster_i], data = allele_frequencies[cluster_i]))\n",
    "    snv_freqs[cluster_i] = snv_freqs[cluster_i].reset_index().rename(columns = {\"index\":\"site_id\"})\n",
    "    snv_freqs[cluster_i][['contig', 'site_pos', 'nucleotide']] = snv_freqs[cluster_i]['site_id'].str.split(\"|\", expand=True)\n",
    "    # convert allele frequency to numeric\n",
    "    snv_freqs[cluster_i][list(desired_samples)] = snv_freqs[cluster_i][list(desired_samples)].apply(pd.to_numeric, errors='coerce')\n",
    "    # polarize\n",
    "    snv_freqs[cluster_i][list(desired_samples)] = snv_freqs[cluster_i].apply(lambda row: 1 - row[list(desired_samples)] if row[polarize_sample] > 0.5 else row[list(desired_samples)], axis = 1)\n",
    "    # convert to longform\n",
    "    snv_freqs[cluster_i] =pd.melt(snv_freqs[cluster_i],\n",
    "                                  id_vars=['site_id', 'contig', 'site_pos', 'nucleotide'],  # columns to keep as identifier variables\n",
    "                                  var_name='sample',  # the name for the sample column\n",
    "                                  value_name='allele_frequency'  # the name for the values from sample columns\n",
    "                                  )\n",
    "\n",
    "    # annotate with sample type (e.g., \"Stool\", \"Saliva\", \"Capsule 2\", etc.)\n",
    "    snv_freqs[cluster_i]['sample_type'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][2])\n",
    "    # annotate with date\n",
    "    snv_freqs[cluster_i]['date'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][0])\n",
    "    # annotate with time\n",
    "    snv_freqs[cluster_i]['time'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][1])\n",
    "    # annotate with timepoint\n",
    "    snv_freqs[cluster_i]['timepoint'] = snv_freqs[cluster_i]['sample'].apply(lambda x: \"%s,\\n%s\" % (sample_sorting_dict[x][0],sample_sorting_dict[x][1]))\n",
    "    # sample order\n",
    "    # snv_freqs[cluster_i]['sample_order'] = snv_freqs[cluster_i]['sample'].map(sorting_dict)\n",
    "    snv_freqs[cluster_i]['sample_order'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][3])\n",
    "\n",
    "snv_freqs_combined = pd.concat(\n",
    "    [df.assign(cluster=i) for i, df in enumerate(snv_freqs)],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "\n",
    "color_palette = sns.color_palette(\"husl\", len(snv_freqs))\n",
    "\n",
    "\n",
    "for cluster_i,cluster in enumerate(snv_freqs):\n",
    "    for site_id in cluster['site_id'].unique():\n",
    "        snv_freq_individual = cluster[cluster['site_id'] == site_id]\n",
    "        sns.lineplot(data=snv_freq_individual, x = \"sample_order\", y = \"allele_frequency\", color = color_palette[cluster_i])\n",
    "\n",
    "# Add legend manually\n",
    "cluster_labels = [f\"Cluster {i}\" for i in range(len(snv_freqs))]\n",
    "handles = [plt.Line2D([0], [0], color=color_palette[i], lw=2) for i in range(len(snv_freqs))]\n",
    "ax.legend(\n",
    "    handles,\n",
    "    cluster_labels,\n",
    "    title=\"Genomic clusters\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),  # Position the legend to the right of the plot\n",
    "    ncol=1  # Arrange legend items in a single column\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xticks(range(len(sample_sorting_dict.keys())))\n",
    "\n",
    "major_ticks = []\n",
    "major_tick_labels = []\n",
    "minor_ticks = []\n",
    "minor_tick_labels = []\n",
    "time_point = \"\"\n",
    "x_ticks_loc = ax.get_xticks()\n",
    "vspan_counter = 0\n",
    "vspan_vec = []\n",
    "for i, sample_ in enumerate(sample_sorting_dict.keys()):\n",
    "    # Major ticks: timepoint\n",
    "    new_time_point = \"%s,\\n%s\" % (sample_sorting_dict[sample_][0],sample_sorting_dict[sample_][1])\n",
    "    if (time_point != new_time_point) & (i != len(sample_sorting_dict) - 1):\n",
    "        time_point = new_time_point\n",
    "        # major_ticks.append(x_ticks_loc[i])\n",
    "        major_tick_labels.append(time_point)\n",
    "\n",
    "        # add vspan\n",
    "        vspan_counter += 1\n",
    "\n",
    "        if vspan_counter == 1:\n",
    "            xmin = 0\n",
    "        else: \n",
    "            xmax = x_ticks_loc[i] - 0.5\n",
    "            vspan_vec.append([xmin, xmax])\n",
    "            xmin = xmax\n",
    "    elif (time_point != new_time_point) & (i == len(sample_sorting_dict) - 1):  \n",
    "        time_point = new_time_point    \n",
    "        major_tick_labels.append(time_point)\n",
    "        \n",
    "        xmax = x_ticks_loc[i] - 0.5\n",
    "        vspan_vec.append([xmin, xmax])\n",
    "        xmin = xmax\n",
    "        xmax = x_ticks_loc[i]\n",
    "        vspan_vec.append([xmin, xmax])\n",
    "    elif (i == len(sample_sorting_dict) - 1):\n",
    "        xmax = x_ticks_loc[i]\n",
    "        vspan_vec.append([xmin, xmax])\n",
    "\n",
    "    # Minor ticks: sample type\n",
    "    sample_type = sample_sorting_dict[sample_][2]\n",
    "    minor_ticks.append(x_ticks_loc[i])\n",
    "    minor_tick_labels.append(sample_type)\n",
    "\n",
    "# adding vspan and creating minor ticks\n",
    "for i,v in enumerate(vspan_vec):\n",
    "    # adding vspan\n",
    "    if i % 2 == 1:\n",
    "        ax.axvspan(v[0],v[1],alpha=.2,color='grey') \n",
    "    \n",
    "    if (i == 0) & (v[1] == 0.5):\n",
    "        major_ticks.append(0)\n",
    "    elif (i == len(vspan_vec) - 1):\n",
    "        major_ticks.append(np.mean([v[0] + 0.5, v[1]]))\n",
    "    else:\n",
    "        major_ticks.append(np.mean(v))\n",
    "\n",
    "\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticklabels(major_tick_labels)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_xticklabels(minor_tick_labels, minor=True)\n",
    "\n",
    "ax.xaxis.remove_overlapping_locs = False\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "# Tick parameters\n",
    "plt.tick_params(axis='x',which='major',bottom=False,left=False,top=False) \n",
    "ax.tick_params(axis='x', which='major', pad=15)  # Move major ticks down by increasing the padding\n",
    "ax.tick_params(axis='x', which='minor', pad=2) \n",
    "\n",
    "# Titles\n",
    "\n",
    "ax.set_title(\"%s%s%s%s\" % (\"SNV frequencies in \", species, \" in subject \", str(subject_id)), size = 20)\n",
    "ax.set_xlabel(\"Sample\", size = 20)\n",
    "ax.set_ylabel(\"Allele frequency\", size = 20)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_freqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/evolutionary_changes/SNV_freqs_%s_%s.png\" % (species,str(subject_id))\n",
    "fig.savefig(out_path, dpi = 300, facecolor = \"white\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/revisions/SNV_freqs_%s_%s.png\" % (species,str(subject_id))\n",
    "fig.savefig(out_path, dpi = 300, facecolor = \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving \n",
    "fst_processing_df.to_csv(out_path, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing pre-identified changes (for loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_haploid = True\n",
    "only_high_coverage = False\n",
    "core_genes_only = False\n",
    "min_depth = 10\n",
    "\n",
    "\n",
    "number_of_snvs_vec = []\n",
    "snvs_set_vec = []\n",
    "\n",
    "\n",
    "for species_subject_i,species_subject_row in grouped_species_subject_pairs.iterrows():\n",
    "    # Extracting species-subject pair\n",
    "    species = species_subject_row['species']\n",
    "    subject_id = int(species_subject_row['subject_id'])\n",
    "\n",
    "    sys.stderr.write(\"Processig %s in subject %s (%d / %d)\\n\" % (species, str(subject_id), species_subject_i + 1, len(grouped_species_subject_pairs)))\n",
    "\n",
    "    # load intersample changes\n",
    "    intersample_changes = load_intersample_change_map(species)\n",
    "\n",
    "    # Extracting all within-host SNVs\n",
    "\n",
    "    subject_samples = list(subject_sample_map[str(subject_id)].keys())\n",
    "    haploid_samples = calculate_haploid_samples(species, use_HMP_freqs=True)\n",
    "    # subject_samples = [sample_ for sample_ in subject_samples if sample_ in haploid_samples]\n",
    "    # sample_pair_list = list(itertools.combinations(subject_samples, 2))\n",
    "    sample_pair_list = list(change_summary_df.loc[(change_summary_df['species'] == species) & (change_summary_df['subject_1'] == str(subject_id)),['sample_1', 'sample_2']].itertuples(index=False, name=None))\n",
    "    \n",
    "    ## paring intersample changes to only include within-host snvs\n",
    "    intersample_changes = {key: value for key, value in intersample_changes.items() if (key in sample_pair_list) or ((key[1], key[0]) in sample_pair_list)}\n",
    "\n",
    "    ## building snv list \n",
    "    snvs = set({})\n",
    "    # for key in intersample_changes.keys():\n",
    "    for key in sample_pair_list:\n",
    "        snv_list = set(intersample_changes[key]['snps'][2])\n",
    "        snvs.update(snv_list)\n",
    "\n",
    "    # Filter for core or non-shared genes\n",
    "    \n",
    "    if core_genes_only:\n",
    "        sys.stderr.write(\"Filtering for core genes\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"Filtering for non-shared genes\\n\")\n",
    "        \n",
    "    if core_genes_only:\n",
    "        core_genes = parse_core_genes(species)\n",
    "        centroid_gene_map = load_centroid_gene_map(species)\n",
    "        desired_gene_ids = {value if key in core_genes else key for key, value in centroid_gene_map.items() if key in core_genes or value in core_genes}.union(core_genes)\n",
    "        snvs = [snv for snv in snvs if snv[0] in desired_gene_ids]\n",
    "    else:\n",
    "        non_shared_genes = parse_non_shared_reference_genes(species)\n",
    "        centroid_gene_map = load_centroid_gene_map(species)\n",
    "        desired_gene_ids = {value if key in non_shared_genes else key for key, value in centroid_gene_map.items() if key in non_shared_genes or value in non_shared_genes}.union(non_shared_genes)\n",
    "        snvs = [snv for snv in snvs if snv[0] in desired_gene_ids]\n",
    "\n",
    "    # snvs = [(snv[1], snv[2], snv[0]) for snv in snvs]\n",
    "    snvs = list(set([(snv[1], snv[2]) for snv in sorted(snvs, key=lambda x: (x[1], x[2]))]))\n",
    "\n",
    "    if len(snvs) == 0:\n",
    "        sys.stderr.write(\"No within-host SNVs in %s in %s \\n\" % (species, str(subject_id)))\n",
    "        number_of_snvs_vec.append(0)\n",
    "        snvs_set_vec.append(set())\n",
    "        continue\n",
    "    else:\n",
    "        number_of_snvs_vec.append(len(snvs))\n",
    "        snvs_set_vec.append(set(snvs))\n",
    "\n",
    "\n",
    "    # separate snvs into clusters\n",
    "\n",
    "    clust_dist = 1e4\n",
    "    contigs = {snv[0] for snv in snvs}\n",
    "    snv_clusters = []\n",
    "    start = 0\n",
    "    for i,contig in enumerate(contigs):\n",
    "        snvs_contig = [snv for snv in snvs if snv[0] == contig]\n",
    "        snvs_contig = [snv for snv in sorted(snvs_contig, key=lambda x: (x[0], x[1]))]\n",
    "        loci = np.array([snv[1] for snv in snvs_contig])\n",
    "        gaps = np.diff(loci)\n",
    "        breaks = np.where(gaps > clust_dist)[0]\n",
    "        start = 0\n",
    "        for end in breaks:\n",
    "            end = end+1\n",
    "            snv_clusters.append(list(snvs_contig[start:end]))\n",
    "            start = end\n",
    "        snv_clusters.append(list(snvs_contig[start:]))\n",
    "        \n",
    "\n",
    "    # reloading subject samples\n",
    "    subject_samples = list(subject_sample_map[str(subject_id)].keys())\n",
    "    snps_freq_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_ref_freq.txt.bz2\")\n",
    "    # Open the compressed file in binary mode\n",
    "    with bz2.open(snps_freq_path, 'rb') as file:\n",
    "        first_line = file.readline().strip() \n",
    "    snps_samples = first_line.decode('utf-8').split(\"\\t\")[1:]\n",
    "\n",
    "    subject_samples = [sample_ for sample_ in subject_samples if sample_ in snps_samples]\n",
    "\n",
    "\n",
    "    # Creating sample indices and sorting\n",
    "    sample_type = [sample_metadata_map[f][2] for f in subject_samples]\n",
    "    stool_idx = [i for i, element in enumerate(sample_type) if element == \"Stool\"]\n",
    "    saliva_idx = [i for i, element in enumerate(sample_type) if element == \"Saliva\"]\n",
    "    timestamp_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "    # Create a list of tuples containing sorting keys\n",
    "    sorting_keys = [\n",
    "    (\n",
    "        datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%Y-%m-%d') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%Y-%m-%d'),\n",
    "        datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%H:%M:%S') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%H:%M:%S'),\n",
    "        sample_metadata_map[f][2]  # sample_type\n",
    "    )\n",
    "    for i, f in enumerate(subject_samples)\n",
    "    ]\n",
    "\n",
    "    # Sort subject_samples based on the sorting keys\n",
    "    desired_samples = [x for _, x in sorted(zip(sorting_keys, subject_samples))]\n",
    "\n",
    "    # Creating sample indices with sorted\n",
    "    sample_type = [sample_metadata_map[f][2] for f in desired_samples]\n",
    "    stool_idx = [i for i, element in enumerate(sample_type) if element == \"Stool\"]\n",
    "    saliva_idx = [i for i, element in enumerate(sample_type) if element == \"Saliva\"]\n",
    "\n",
    "    # creating a dictionary\n",
    "    sample_sorting_dict = {\n",
    "        f: (\n",
    "            datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%Y-%m-%d') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%Y-%m-%d'),\n",
    "            datetime.strptime(sample_metadata_map[f][5], timestamp_format).strftime('%H:%M:%S') if (i in stool_idx) | (i in saliva_idx) else datetime.strptime(sample_metadata_map[f][3], timestamp_format).strftime('%H:%M:%S'),\n",
    "            sample_metadata_map[f][2],  # sample_type\n",
    "            i\n",
    "        )\n",
    "        for i, f in enumerate(desired_samples)\n",
    "    }\n",
    "\n",
    "        \n",
    "    # Extracting SNV frequencies\n",
    "    x_axis_dictionary = {}\n",
    "\n",
    "    snps_freq_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_ref_freq.txt.bz2\")\n",
    "    snps_depth_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_depth.txt.bz2\")\n",
    "    snps_info_path = \"%s%s%s%s\" % (config.data_directory,\"snps/\", species, \"/snps_info.txt.bz2\")\n",
    "\n",
    "\n",
    "    final_line_number = 1e10\n",
    "\n",
    "    if only_haploid:\n",
    "        haploid_samples = calculate_haploid_samples(species, use_HMP_freqs=True)\n",
    "        desired_samples = [desired_sample for desired_sample in desired_samples if desired_sample in haploid_samples]\n",
    "    elif only_high_coverage:\n",
    "        high_coverage_samples = calculate_highcoverage_samples(species)\n",
    "        desired_samples = [desired_sample for desired_sample in desired_samples if desired_sample in high_coverage_samples]\n",
    "\n",
    "    # if (len(baseline_sample) == 0) | (baseline_sample not in desired_samples):\n",
    "    #     if len(recipient_samples) > 0:\n",
    "    #         baseline_sample = recipient_samples[0]\n",
    "    #     else:\n",
    "    #         baseline_sample = desired_samples[0]\n",
    "\n",
    "    line_counter = 0\n",
    "    current_cluster = 0\n",
    "    clusters_finished = False\n",
    "    loci_extracted = [0]*len(snv_clusters)\n",
    "    site_id_vec = [[] for _ in snv_clusters]\n",
    "    allele_frequencies = [[] for _ in snv_clusters]\n",
    "\n",
    "    with bz2.open(snps_freq_path, 'rt') as file,  bz2.open(snps_depth_path, 'rt') as depth_file, bz2.open(snps_info_path, 'rt') as info_file:\n",
    "        for line, depth_line, info_line in zip(file, depth_file, info_file):\n",
    "            if line_counter == 0:\n",
    "                items = line.split()[1:]    \n",
    "                samples_in_file = parse_merged_sample_names(items)\n",
    "                desired_sample_idxs = []\n",
    "                for sample in desired_samples:\n",
    "                    if sample not in samples_in_file:\n",
    "                        continue\n",
    "                    desired_sample_idxs.append( np.nonzero(samples_in_file==sample)[0][0] )\n",
    "                    # if plot_strain_snvs:\n",
    "                    #     strain_tracking_sample_idx = np.nonzero(samples_in_file==strain_tracking_sample)[0][0]\n",
    "                    #     strain_tracking_sample_foil_idx = np.nonzero(samples_in_file==strain_tracking_sample_foil)[0][0]\n",
    "\n",
    "                desired_sample_idxs = np.array(desired_sample_idxs)    \n",
    "                desired_samples = samples_in_file[desired_sample_idxs]\n",
    "                sample_sorting_dict = {key:val for key,val in sample_sorting_dict.items() if key in desired_samples}\n",
    "                sample_sorting_dict = {k: sample_sorting_dict[k][:3] + (i,) for i, k in enumerate(sample_sorting_dict.keys())}\n",
    "\n",
    "                line_counter += 1\n",
    "            else:\n",
    "                if line_counter == final_line_number:\n",
    "                    sys.stderr.write(\"Breaking at line \" + str(final_line_number) + \"\\n\")\n",
    "                    break\n",
    "                if (sum(loci_extracted) == len(snvs)):\n",
    "                    sys.stderr.write(\"All loci extracted\\n\")\n",
    "                    break\n",
    "                \n",
    "                items = line.split()\n",
    "                depths =  depth_line.split()\n",
    "                infos = info_line.split()\n",
    "                if len(infos) >= 7:\n",
    "                    gene_id = infos[6]\n",
    "                else: \n",
    "                    gene_id = \"\"\n",
    "                site_id_original = items[0]\n",
    "                site_id = items[0].split(\"|\")\n",
    "                site_id = (site_id[0], int(site_id[1]))\n",
    "                items = items[1:]\n",
    "                depths = [int(depth) for depth in depths[1:]]     \n",
    "\n",
    "                for cluster_i,cluster in enumerate(snv_clusters):\n",
    "                    if site_id in cluster:\n",
    "                        sys.stderr.write(\"EXTRACTING: \" + site_id[0] + \"|\" + str(site_id[1]) + \" in cluster \" + str(cluster_i+1) + \"\\n\")\n",
    "                        site_id_vec[cluster_i].append(site_id_original)\n",
    "                        loci_extracted[cluster_i] += 1\n",
    "                        af = []\n",
    "                        for idx in desired_sample_idxs:    \n",
    "                            item = items[idx]\n",
    "                            depth = depths[idx]\n",
    "                            if depth < min_depth:\n",
    "                                subitems = [np.nan]\n",
    "                            else:\n",
    "                                subitems = item.split(\",\")\n",
    "                            af.append(subitems[0])\n",
    "                        if loci_extracted[cluster_i] == 1:\n",
    "                            current_cluster += 1\n",
    "                            allele_frequencies[cluster_i] = np.array(af)\n",
    "                            if len(snv_clusters[cluster_i]) == 1:\n",
    "                                allele_frequencies[cluster_i] = allele_frequencies[cluster_i][np.newaxis, :]\n",
    "                        else:\n",
    "                            allele_frequencies[cluster_i] = np.vstack((allele_frequencies[cluster_i], np.array(af)))\n",
    "                        break\n",
    "                if (loci_extracted[cluster_i] == len(cluster)) & (not clusters_finished):\n",
    "                    if (sum(loci_extracted) == len(snvs)):\n",
    "                        clusters_finished = True\n",
    "                    sys.stderr.write(\"Extracted \" + str(loci_extracted[cluster_i]) + \" SNVs in cluster \" + str(cluster_i+1) + \"\\n\")\n",
    "                \n",
    "                line_counter += 1\n",
    "                # if line_counter % 1000 == 0:\n",
    "                #     print(site_id)  \n",
    "\n",
    "\n",
    "\n",
    "    ## ANNOTATION\n",
    "    snv_freqs = []\n",
    "    sorting_dict = {key: idx for idx, key in enumerate(sample_sorting_dict.keys())}\n",
    "    polarize_sample = [sample_ for sample_ in sample_sorting_dict.keys() if (sample_sorting_dict[sample_][2] != \"Stool\") & (sample_sorting_dict[sample_][2] != \"Saliva\")][0]\n",
    "    for cluster_i, cluster in enumerate(snv_clusters):\n",
    "        # creating dataframe\n",
    "        snv_freqs.append(pd.DataFrame(columns = desired_samples, index=site_id_vec[cluster_i], data = allele_frequencies[cluster_i]))\n",
    "        snv_freqs[cluster_i] = snv_freqs[cluster_i].reset_index().rename(columns = {\"index\":\"site_id\"})\n",
    "        snv_freqs[cluster_i][['contig', 'site_pos', 'nucleotide']] = snv_freqs[cluster_i]['site_id'].str.split(\"|\", expand=True)\n",
    "        # convert allele frequency to numeric\n",
    "        snv_freqs[cluster_i][list(desired_samples)] = snv_freqs[cluster_i][list(desired_samples)].apply(pd.to_numeric, errors='coerce')\n",
    "        # polarize\n",
    "        snv_freqs[cluster_i][list(desired_samples)] = snv_freqs[cluster_i].apply(lambda row: 1 - row[list(desired_samples)] if row[polarize_sample] > 0.5 else row[list(desired_samples)], axis = 1)\n",
    "        # convert to longform\n",
    "        snv_freqs[cluster_i] =pd.melt(snv_freqs[cluster_i],\n",
    "                                    id_vars=['site_id', 'contig', 'site_pos', 'nucleotide'],  # columns to keep as identifier variables\n",
    "                                    var_name='sample',  # the name for the sample column\n",
    "                                    value_name='allele_frequency'  # the name for the values from sample columns\n",
    "                                    )\n",
    "\n",
    "        # annotate with sample type (e.g., \"Stool\", \"Saliva\", \"Capsule 2\", etc.)\n",
    "        snv_freqs[cluster_i]['sample_type'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][2])\n",
    "        # annotate with date\n",
    "        snv_freqs[cluster_i]['date'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][0])\n",
    "        # annotate with time\n",
    "        snv_freqs[cluster_i]['time'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][1])\n",
    "        # annotate with timepoint\n",
    "        snv_freqs[cluster_i]['timepoint'] = snv_freqs[cluster_i]['sample'].apply(lambda x: \"%s,\\n%s\" % (sample_sorting_dict[x][0],sample_sorting_dict[x][1]))\n",
    "        # sample order\n",
    "        # snv_freqs[cluster_i]['sample_order'] = snv_freqs[cluster_i]['sample'].map(sorting_dict)\n",
    "        snv_freqs[cluster_i]['sample_order'] = snv_freqs[cluster_i]['sample'].apply(lambda x: sample_sorting_dict[x][3])\n",
    "\n",
    "    snv_freqs_combined = pd.concat(\n",
    "        [df.assign(cluster=i) for i, df in enumerate(snv_freqs)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # plotting\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "\n",
    "    color_palette = sns.color_palette(\"husl\", len(snv_freqs))\n",
    "\n",
    "\n",
    "    for cluster_i,cluster in enumerate(snv_freqs):\n",
    "        for site_id in cluster['site_id'].unique():\n",
    "            snv_freq_individual = cluster[cluster['site_id'] == site_id]\n",
    "            sns.lineplot(data=snv_freq_individual, x = \"sample_order\", y = \"allele_frequency\", color = color_palette[cluster_i])\n",
    "\n",
    "    # Add legend manually\n",
    "    cluster_labels = [f\"Cluster {i}\" for i in range(len(snv_freqs))]\n",
    "    handles = [plt.Line2D([0], [0], color=color_palette[i], lw=2) for i in range(len(snv_freqs))]\n",
    "    ax.legend(\n",
    "        handles,\n",
    "        cluster_labels,\n",
    "        title=\"Genomic clusters\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.5),  # Position the legend to the right of the plot\n",
    "        ncol=1  # Arrange legend items in a single column\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_xticks(range(len(sample_sorting_dict.keys())))\n",
    "\n",
    "    major_ticks = []\n",
    "    major_tick_labels = []\n",
    "    minor_ticks = []\n",
    "    minor_tick_labels = []\n",
    "    time_point = \"\"\n",
    "    x_ticks_loc = ax.get_xticks()\n",
    "    vspan_counter = 0\n",
    "    vspan_vec = []\n",
    "    for i, sample_ in enumerate(sample_sorting_dict.keys()):\n",
    "        # Major ticks: timepoint\n",
    "        new_time_point = \"%s,\\n%s\" % (sample_sorting_dict[sample_][0],sample_sorting_dict[sample_][1])\n",
    "        if (time_point != new_time_point) & (i != len(sample_sorting_dict) - 1):\n",
    "            time_point = new_time_point\n",
    "            # major_ticks.append(x_ticks_loc[i])\n",
    "            major_tick_labels.append(time_point)\n",
    "\n",
    "            # add vspan\n",
    "            vspan_counter += 1\n",
    "\n",
    "            if vspan_counter == 1:\n",
    "                xmin = 0\n",
    "            else: \n",
    "                xmax = x_ticks_loc[i] - 0.5\n",
    "                vspan_vec.append([xmin, xmax])\n",
    "                xmin = xmax\n",
    "        elif (time_point != new_time_point) & (i == len(sample_sorting_dict) - 1):  \n",
    "            time_point = new_time_point    \n",
    "            major_tick_labels.append(time_point)\n",
    "            \n",
    "            xmax = x_ticks_loc[i] - 0.5\n",
    "            vspan_vec.append([xmin, xmax])\n",
    "            xmin = xmax\n",
    "            xmax = x_ticks_loc[i]\n",
    "            vspan_vec.append([xmin, xmax])\n",
    "        elif (i == len(sample_sorting_dict) - 1):\n",
    "            xmax = x_ticks_loc[i]\n",
    "            vspan_vec.append([xmin, xmax])\n",
    "\n",
    "        # Minor ticks: sample type\n",
    "        sample_type = sample_sorting_dict[sample_][2]\n",
    "        minor_ticks.append(x_ticks_loc[i])\n",
    "        minor_tick_labels.append(sample_type)\n",
    "\n",
    "    # adding vspan and creating minor ticks\n",
    "    for i,v in enumerate(vspan_vec):\n",
    "        # adding vspan\n",
    "        if i % 2 == 1:\n",
    "            ax.axvspan(v[0],v[1],alpha=.2,color='grey') \n",
    "        \n",
    "        if (i == 0) & (v[1] == 0.5):\n",
    "            major_ticks.append(0)\n",
    "        elif (i == len(vspan_vec) - 1):\n",
    "            major_ticks.append(np.mean([v[0] + 0.5, v[1]]))\n",
    "        else:\n",
    "            major_ticks.append(np.mean(v))\n",
    "\n",
    "\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_xticklabels(major_tick_labels)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_xticklabels(minor_tick_labels, minor=True)\n",
    "\n",
    "    ax.xaxis.remove_overlapping_locs = False\n",
    "\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "    # Tick parameters\n",
    "    plt.tick_params(axis='x',which='major',bottom=False,left=False,top=False) \n",
    "    ax.tick_params(axis='x', which='major', pad=15)  # Move major ticks down by increasing the padding\n",
    "    ax.tick_params(axis='x', which='minor', pad=2) \n",
    "\n",
    "    # Titles\n",
    "\n",
    "    ax.set_title(\"%s%s%s%s\" % (\"SNV frequencies in \", species, \" in subject \", str(subject_id)), size = 20)\n",
    "    ax.set_xlabel(\"Sample\", size = 20)\n",
    "    ax.set_ylabel(\"Allele frequency\", size = 20)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/evolutionary_changes/SNV_freqs_%s_%s_nov13.png\" % (species,str(subject_id))\n",
    "    fig.savefig(out_path, dpi = 300, facecolor = \"white\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_summary_df[(change_summary_df['species'] == \"Alistipes_putredinis_61533\") & ((change_summary_df['subject_1'] == \"9\"))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
