{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "# plotting functions\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# predefined functions\n",
    "from strain_phasing_functions import *\n",
    "from microbiome_evolution_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "strain_frequency_dir = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters/\"\n",
    "species_list_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/metadata/species_snps.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load species list\n",
    "with open(species_list_path, \"r\") as file:\n",
    "    species_list = file.readlines()  # Read all lines from the file\n",
    "    species_list = [line.strip() for line in species_list]  # Remove any leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acidaminococcus_intestini_54097',\n",
       " 'Actinomyces_graevenitzii_58300',\n",
       " 'Actinomyces_sp_57735',\n",
       " 'Actinomyces_sp_62581',\n",
       " 'Actinomyces_viscosus_57672',\n",
       " 'Adlercreutzia_equolifaciens_60310',\n",
       " 'Aggregatibacter_aphrophilus_58143',\n",
       " 'Akkermansia_muciniphila_55290',\n",
       " 'Alistipes_finegoldii_56071',\n",
       " 'Alistipes_indistinctus_62207',\n",
       " 'Alistipes_onderdonkii_55464',\n",
       " 'Alistipes_putredinis_61533',\n",
       " 'Alistipes_senegalensis_58364',\n",
       " 'Alistipes_shahii_62199',\n",
       " 'Anaerostipes_hadrus_55206',\n",
       " 'Atopobium_parvulum_59960',\n",
       " 'Atopobium_sp_59401',\n",
       " 'Bacteroidales_bacterium_58650',\n",
       " 'Bacteroides_caccae_53434',\n",
       " 'Bacteroides_cellulosilyticus_58046',\n",
       " 'Bacteroides_clarus_62282',\n",
       " 'Bacteroides_coprocola_61586',\n",
       " 'Bacteroides_eggerthii_54457',\n",
       " 'Bacteroides_finegoldii_57739',\n",
       " 'Bacteroides_fragilis_54507',\n",
       " 'Bacteroides_intestinalis_61596',\n",
       " 'Bacteroides_massiliensis_44749',\n",
       " 'Bacteroides_ovatus_58035',\n",
       " 'Bacteroides_pectinophilus_61619',\n",
       " 'Bacteroides_plebeius_61623',\n",
       " 'Bacteroides_salyersiae_54873',\n",
       " 'Bacteroides_stercoris_56735',\n",
       " 'Bacteroides_thetaiotaomicron_56941',\n",
       " 'Bacteroides_uniformis_57318',\n",
       " 'Bacteroides_vulgatus_57955',\n",
       " 'Bacteroides_xylanisolvens_57185',\n",
       " 'Barnesiella_intestinihominis_62208',\n",
       " 'Bifidobacterium_adolescentis_56815',\n",
       " 'Bifidobacterium_angulatum_45832',\n",
       " 'Bifidobacterium_animalis_58116',\n",
       " 'Bifidobacterium_bifidum_55065',\n",
       " 'Bifidobacterium_catenulatum_58257',\n",
       " 'Bifidobacterium_longum_57796',\n",
       " 'Bifidobacterium_pseudocatenulatum_57754',\n",
       " 'Bilophila_wadsworthia_57364',\n",
       " 'Blautia_producta_56315',\n",
       " 'Blautia_wexlerae_56130',\n",
       " 'Burkholderiales_bacterium_56577',\n",
       " 'Butyricimonas_virosa_58742',\n",
       " 'Capnocytophaga_gingivalis_61780',\n",
       " 'Capnocytophaga_granulosa_57613',\n",
       " 'Capnocytophaga_sputigena_61779',\n",
       " 'Catenibacterium_mitsuokai_61547',\n",
       " 'Citrobacter_braakii_56022',\n",
       " 'Clostridiales_bacterium_56470',\n",
       " 'Clostridiales_bacterium_61057',\n",
       " 'Clostridium_bolteae_57158',\n",
       " 'Clostridium_citroniae_62210',\n",
       " 'Clostridium_sp_61482',\n",
       " 'Clostridium_spiroforme_61500',\n",
       " 'Collinsella_aerofaciens_61484',\n",
       " 'Collinsella_intestinalis_61689',\n",
       " 'Collinsella_sp_62205',\n",
       " 'Coprococcus_catus_62200',\n",
       " 'Coprococcus_comes_61587',\n",
       " 'Coprococcus_sp_62244',\n",
       " 'Coriobacteriaceae_bacterium_58375',\n",
       " 'Desulfovibrio_piger_61475',\n",
       " 'Dialister_invisus_61905',\n",
       " 'Dorea_formicigenerans_56346',\n",
       " 'Dorea_longicatena_59913',\n",
       " 'Dorea_longicatena_61473',\n",
       " 'Eggerthella_lenta_56463',\n",
       " 'Enterobacter_aerogenes_55704',\n",
       " 'Enterobacter_cloacae_55011',\n",
       " 'Enterobacter_cloacae_55612',\n",
       " 'Enterobacter_cloacae_57303',\n",
       " 'Enterobacter_cloacae_58148',\n",
       " 'Enterobacter_cloacae_58287',\n",
       " 'Enterococcus_durans_52177',\n",
       " 'Enterococcus_faecalis_56297',\n",
       " 'Enterococcus_faecium_56947',\n",
       " 'Enterococcus_hirae_55131',\n",
       " 'Escherichia_coli_58110',\n",
       " 'Eubacterium_biforme_61684',\n",
       " 'Eubacterium_cylindroides_56237',\n",
       " 'Eubacterium_eligens_61678',\n",
       " 'Eubacterium_hallii_61477',\n",
       " 'Eubacterium_limosum_60946',\n",
       " 'Eubacterium_ramulus_59802',\n",
       " 'Eubacterium_rectale_56927',\n",
       " 'Eubacterium_siraeum_57634',\n",
       " 'Eubacterium_sulci_62452',\n",
       " 'Eubacterium_ventriosum_61474',\n",
       " 'Faecalibacterium_cf_62236',\n",
       " 'Faecalibacterium_prausnitzii_57453',\n",
       " 'Faecalibacterium_prausnitzii_61481',\n",
       " 'Faecalibacterium_prausnitzii_62201',\n",
       " 'Fusobacterium_periodonticum_58002',\n",
       " 'Gemella_haemolysans_61762',\n",
       " 'Gemella_morbillorum_61817',\n",
       " 'Gemella_sanguinis_57791',\n",
       " 'Gordonibacter_pamelaeae_62044',\n",
       " 'Granulicatella_elegans_61945',\n",
       " 'Guyana_massiliensis_60772',\n",
       " 'Haemophilus_haemolyticus_58348',\n",
       " 'Haemophilus_parainfluenzae_62356',\n",
       " 'Haemophilus_pittmaniae_58383',\n",
       " 'Haemophilus_sputorum_53575',\n",
       " 'Kingella_oralis_61952',\n",
       " 'Klebsiella_oxytoca_54123',\n",
       " 'Klebsiella_oxytoca_56762',\n",
       " 'Klebsiella_oxytoca_57801',\n",
       " 'Klebsiella_pneumoniae_54688',\n",
       " 'Klebsiella_pneumoniae_54788',\n",
       " 'Klebsiella_pneumoniae_57337',\n",
       " 'Lachnospiraceae_bacterium_51870',\n",
       " 'Lactobacillus_acidophilus_51143',\n",
       " 'Lactobacillus_delbrueckii_56845',\n",
       " 'Lactobacillus_rhamnosus_57549',\n",
       " 'Lactobacillus_ruminis_57020',\n",
       " 'Lactococcus_garvieae_57061',\n",
       " 'Lautropia_mirabilis_62440',\n",
       " 'Leptotrichia_sp_60101',\n",
       " 'Megamonas_hypermegale_57114',\n",
       " 'Megasphaera_micronuciformis_62167',\n",
       " 'Megasphaera_sp_50408',\n",
       " 'Mitsuokella_jalaludinii_60453',\n",
       " 'Mitsuokella_multacida_61656',\n",
       " 'Neisseria_elongata_61756',\n",
       " 'Neisseria_sicca_58189',\n",
       " 'Neisseria_subflava_61760',\n",
       " 'Odoribacter_splanchnicus_62174',\n",
       " 'Oribacterium_sp_54059',\n",
       " 'Oscillibacter_sp_60799',\n",
       " 'Oscillospiraceae_bacterium_54867',\n",
       " 'Oxalobacter_formigenes_61802',\n",
       " 'Parabacteroides_distasonis_56985',\n",
       " 'Parabacteroides_goldsteinii_56831',\n",
       " 'Parabacteroides_johnsonii_55217',\n",
       " 'Parabacteroides_merdae_56972',\n",
       " 'Paraprevotella_clara_33712',\n",
       " 'Pediococcus_acidilactici_57243',\n",
       " 'Pediococcus_pentosaceus_55856',\n",
       " 'Phascolarctobacterium_sp_59817',\n",
       " 'Phascolarctobacterium_sp_59818',\n",
       " 'Porphyromonas_sp_57899',\n",
       " 'Prevotella_copri_61740',\n",
       " 'Prevotella_histicola_56864',\n",
       " 'Prevotella_intermedia_57633',\n",
       " 'Prevotella_melaninogenica_58075',\n",
       " 'Prevotella_nanceiensis_44721',\n",
       " 'Prevotella_oris_57058',\n",
       " 'Prevotella_pallens_62692',\n",
       " 'Prevotella_salivae_57216',\n",
       " 'Prevotella_shahii_59695',\n",
       " 'Prevotella_sp_58138',\n",
       " 'Prevotella_sp_58566',\n",
       " 'Prevotella_sp_61818',\n",
       " 'Propionibacterium_freudenreichii_62075',\n",
       " 'Raoultella_planticola_57814',\n",
       " 'Roseburia_hominis_61877',\n",
       " 'Roseburia_intestinalis_56239',\n",
       " 'Roseburia_inulinivorans_61943',\n",
       " 'Rothia_dentocariosa_57938',\n",
       " 'Ruminococcus_bicirculans_59300',\n",
       " 'Ruminococcus_bromii_62047',\n",
       " 'Ruminococcus_callidus_61479',\n",
       " 'Ruminococcus_gnavus_57638',\n",
       " 'Ruminococcus_lactaris_55568',\n",
       " 'Ruminococcus_obeum_61472',\n",
       " 'Ruminococcus_obeum_62046',\n",
       " 'Ruminococcus_sp_55468',\n",
       " 'Ruminococcus_torques_62045',\n",
       " 'Serratia_marcescens_59790',\n",
       " 'Streptococcus_anginosus_58223',\n",
       " 'Streptococcus_australis_62469',\n",
       " 'Streptococcus_infantarius_58294',\n",
       " 'Streptococcus_infantis_58320',\n",
       " 'Streptococcus_infantis_62384',\n",
       " 'Streptococcus_infantis_62471',\n",
       " 'Streptococcus_mitis_58556',\n",
       " 'Streptococcus_mitis_60474',\n",
       " 'Streptococcus_mitis_62363',\n",
       " 'Streptococcus_oralis_58560',\n",
       " 'Streptococcus_parasanguinis_58126',\n",
       " 'Streptococcus_pseudopneumoniae_58459',\n",
       " 'Streptococcus_salivarius_58037',\n",
       " 'Streptococcus_sp_54717',\n",
       " 'Streptococcus_sp_58554',\n",
       " 'Streptococcus_thermophilus_54772',\n",
       " 'Streptococcus_vestibularis_56030',\n",
       " 'Sutterella_wadsworthensis_56828',\n",
       " 'Sutterella_wadsworthensis_62218',\n",
       " 'Veillonella_atypica_58169',\n",
       " 'Veillonella_dispar_61763',\n",
       " 'Veillonella_parvula_57794',\n",
       " 'Veillonella_parvula_58184',\n",
       " 'Veillonella_sp_62404',\n",
       " 'Weissella_confusa_59158']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load maps\n",
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "subject_sample_map = parse_subject_sample_map()\n",
    "## subject list\n",
    "subjects = [str(subject) for subject in sorted([int(subject) for subject in list(subject_sample_map.keys())])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRAIN ANOVA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strain files iteratively\n",
    "strain_freq_df_list = []\n",
    "\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df['sample_type_number'] = strain_freq_df['sample'].apply(lambda x: sample_metadata_map[x][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_freqs.csv\"\n",
    "strain_freq_df.to_csv(out_path, sep = \",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. have only 2 strains\n",
    "strain_freq_df = strain_freq_df.loc[strain_freq_df.groupby([\"species\", \"subject\"])['strain'].transform(\"nunique\") == 2,:]\n",
    "\n",
    "##### Filter to be only strain 1\n",
    "strain_freq_df_2 = strain_freq_df[strain_freq_df.strain == 2]\n",
    "strain_freq_df = strain_freq_df[strain_freq_df.strain == 1]\n",
    "\n",
    "\n",
    "##### Get stool groups\n",
    "stool_samples_df = strain_freq_df[strain_freq_df.sample_type == \"Stool\"].reset_index(drop=True)\n",
    "\n",
    "##### Filter for only capsule\n",
    "strain_freq_df = strain_freq_df[strain_freq_df.sample_type == \"Capsule\"].reset_index(drop=True)\n",
    "strain_freq_df_2 = strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"].reset_index(drop=True)\n",
    "\n",
    "## 2. Present in at least two sites (i.e., capsules in at least 1 host)\n",
    "### Identifying groups with spatial data (i.e., with unique sample_type_number)\n",
    "spatial_groups = strain_freq_df[strain_freq_df.sample_type == \"Capsule\"][\n",
    "    strain_freq_df[strain_freq_df.sample_type == \"Capsule\"].groupby([\"species\", \"subject\", \"date\", \"time\"])[\"sample_type_number\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True)\n",
    "spatial_groups_2 = strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"][\n",
    "    strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"].groupby([\"species\", \"subject\", \"date\", \"time\"])[\"sample_type_number\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True)\n",
    "### slicing stool based on spatial groups\n",
    "strain_freq_df = strain_freq_df.merge(spatial_groups, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "strain_freq_df_2 = strain_freq_df_2.merge(spatial_groups_2, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "\n",
    "\n",
    "## 3. Present in at least two timepoints \n",
    "###  adding timepoints\n",
    "strain_freq_df['timepoint'] = (\n",
    "    strain_freq_df.sort_values(['date', 'time'])\n",
    "    .groupby(['species', 'subject'])\n",
    "    .apply(lambda group: group.groupby(['date', 'time']).ngroup() + 1)\n",
    "    .reset_index(level=[0, 1], drop=True)\n",
    ")\n",
    "strain_freq_df_2['timepoint'] = (\n",
    "    strain_freq_df_2.sort_values(['date', 'time'])\n",
    "    .groupby(['species', 'subject'])\n",
    "    .apply(lambda group: group.groupby(['date', 'time']).ngroup() + 1)\n",
    "    .reset_index(level=[0, 1], drop=True)\n",
    ")\n",
    "### creating temporal groups\n",
    "temporal_groups = strain_freq_df[\n",
    "    strain_freq_df.groupby([\"species\", \"subject\", \"sample_type_number\"])[\"timepoint\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True) # consider only include capsules that are present in multiple timepoints\n",
    "temporal_groups_2 = strain_freq_df_2[\n",
    "    strain_freq_df_2.groupby([\"species\", \"subject\", \"sample_type_number\"])[\"timepoint\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True) # consider only include capsules that are present in multiple timepoints\n",
    "### slicing stool based on spatial groups\n",
    "strain_freq_df = strain_freq_df.merge(temporal_groups, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "strain_freq_df_2 = strain_freq_df_2.merge(temporal_groups_2, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "\n",
    "\n",
    "## 4. subset stool sampels to only include spatial and tempral samples\n",
    "stool_samples_df = stool_samples_df.merge(spatial_groups[['species', 'subject']].drop_duplicates().reset_index(drop = True), on=[\"species\", \"subject\"], how=\"inner\")\n",
    "stool_samples_df = stool_samples_df.merge(temporal_groups[['species', 'subject']].drop_duplicates().reset_index(drop = True), on=[\"species\", \"subject\"], how=\"inner\")\n",
    "\n",
    "##### Reordering columns\n",
    "strain_freq_df = strain_freq_df[['species', 'strain','subject', 'sample_type','sample_type_number','tissue', 'date', 'time','timepoint', 'sample','freq','quantile_25', 'quantile_75', 'upper_ci','lower_ci']]\n",
    "strain_freq_df = strain_freq_df.reset_index(drop = True)\n",
    "strain_freq_df_2 = strain_freq_df_2[['species', 'strain','subject', 'sample_type','sample_type_number','tissue', 'date', 'time','timepoint', 'sample','freq','quantile_25', 'quantile_75', 'upper_ci','lower_ci']]\n",
    "strain_freq_df_2 = strain_freq_df_2.reset_index(drop = True)\n",
    "stool_samples_df = stool_samples_df[['species', 'strain','subject', 'sample_type','sample_type_number','tissue', 'date', 'time', 'sample','freq','quantile_25', 'quantile_75', 'upper_ci','lower_ci']]\n",
    "stool_samples_df = stool_samples_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Getting strain freq to equal 1\n",
    "strain_freq_df_bothstrains = pd.concat([strain_freq_df, strain_freq_df_2], axis = 0)\n",
    "grouped_sum = strain_freq_df_bothstrains.groupby(\n",
    "    ['sample','species']\n",
    ")['freq'].sum().round(6)\n",
    "all_1_groups = grouped_sum[grouped_sum == 1.0].reset_index()\n",
    "strain_freq_df_bothstrains = strain_freq_df_bothstrains.merge(\n",
    "    all_1_groups[['sample','species']],\n",
    "    on=['sample','species'],\n",
    "    how='inner'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### minimum strain frequency\n",
    "msf = 0.05\n",
    "filtered_df = strain_freq_df_bothstrains.groupby(['species', 'subject', 'strain']).filter(\n",
    "    lambda group: ((group['freq'] > msf) & (group['freq'] < (1-msf))).any()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_freqs_filtered.csv\"\n",
    "filtered_df.to_csv(out_path, sep = \",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_freq_df_bothstrains.loc[(strain_freq_df_bothstrains.species == \"Anaerostipes_hadrus_55206\") & (strain_freq_df_bothstrains.subject == 9),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_freq_df_bothstrains[strain_freq_df_bothstrains.species == \"Ruminococcus_obeum_61472\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_freq_df.strain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_freq_df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species_subject_list =  strain_freq_df[['species','subject']].drop_duplicates().reset_index(drop = True)\n",
    "# out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/metadata/strain_species_subject_df.txt\"\n",
    "# species_subject_list.to_csv(out_path, sep = \"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_freq_GoodSpeciesSubjectPairs.csv\"\n",
    "strain_freq_df.to_csv(out_path, sep = \",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating a subject 1 only dataset\n",
    "strain_freq_df_S1 = strain_freq_df[strain_freq_df.subject == 1] # need to filter for species with multiple capsules for timepoint\n",
    "### creating a dataset with stool\n",
    "strain_freq_df_stool = pd.concat([strain_freq_df,stool_samples_df]).sort_values([\"species\", \"subject\", \"date\", \"time\", \"sample_type\", \"sample_type_number\"])\n",
    "strain_freq_df_stool = strain_freq_df_stool.drop(columns=['timepoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of species: %s\\n\" % (len(strain_freq_df.species.unique())))\n",
    "print(\"Number of subjects: %s\\n\" % (len(strain_freq_df.subject.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting data to an example species\n",
    "subset_data  = strain_freq_df.loc[(strain_freq_df.species == \"Bacteroides_vulgatus_57955\") & (strain_freq_df.subject == 11),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(\"freq ~ C(sample_type_number) + C(timepoint)\", data=subset_data).fit()\n",
    "anova_table = anova_lm(model, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explained variance:\n",
    "explained_variance_space = anova_table.loc[\"C(sample_type_number)\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "explained_variance_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explained variance:\n",
    "explained_variance_time = anova_table.loc[\"C(timepoint)\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "explained_variance_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strain files iteratively\n",
    "strain_freq_df_list = []\n",
    "\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df['sample_type_number'] = strain_freq_df['sample'].apply(lambda x: sample_metadata_map[x][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. have > 2 strains\n",
    "strain_freq_df = strain_freq_df.loc[strain_freq_df.groupby([\"species\", \"subject\"])['strain'].transform(\"nunique\") > 2,:]\n",
    "##### Filter for only capsule\n",
    "# strain_freq_df = strain_freq_df[strain_freq_df.sample_type == \"Capsule\"].reset_index(drop=True)\n",
    "# strain_freq_df_2 = strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strain_freq_df[['species','subject']].drop_duplicates().to_csv(\"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/tables/species_subjects_with_three_strains_FULL.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/tables/three_strain_frequencies.tsv\"\n",
    "strain_freq_df.to_csv(out_path, sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 strains that might be 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strain files iteratively\n",
    "strain_freq_df_list = []\n",
    "\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df['sample_type_number'] = strain_freq_df['sample'].apply(lambda x: sample_metadata_map[x][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. have only 2 strains\n",
    "strain_freq_df = strain_freq_df.loc[strain_freq_df.groupby([\"species\", \"subject\"])['strain'].transform(\"nunique\") == 2,:]\n",
    "\n",
    "##### Filter to be only strain 1\n",
    "strain_freq_df_2 = strain_freq_df[strain_freq_df.strain == 2]\n",
    "strain_freq_df = strain_freq_df[strain_freq_df.strain == 1]\n",
    "\n",
    "\n",
    "##### Filter for only capsule\n",
    "strain_freq_df = strain_freq_df[strain_freq_df.sample_type == \"Capsule\"].reset_index(drop=True)\n",
    "strain_freq_df_2 = strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"].reset_index(drop=True)\n",
    "\n",
    "\n",
    "## 2. Present in at least two sites (i.e., capsules in at least 1 host)\n",
    "### Identifying groups with spatial data (i.e., with unique sample_type_number)\n",
    "spatial_groups = strain_freq_df[strain_freq_df.sample_type == \"Capsule\"][\n",
    "    strain_freq_df[strain_freq_df.sample_type == \"Capsule\"].groupby([\"species\", \"subject\", \"date\", \"time\"])[\"sample_type_number\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True)\n",
    "spatial_groups_2 = strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"][\n",
    "    strain_freq_df_2[strain_freq_df_2.sample_type == \"Capsule\"].groupby([\"species\", \"subject\", \"date\", \"time\"])[\"sample_type_number\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "### slicing stool based on spatial groups\n",
    "strain_freq_df = strain_freq_df.merge(spatial_groups, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "strain_freq_df_2 = strain_freq_df_2.merge(spatial_groups_2, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "\n",
    "\n",
    "## 3. Present in at least two timepoints \n",
    "###  adding timepoints\n",
    "strain_freq_df['timepoint'] = (\n",
    "    strain_freq_df.sort_values(['date', 'time'])\n",
    "    .groupby(['species', 'subject'])\n",
    "    .apply(lambda group: group.groupby(['date', 'time']).ngroup() + 1)\n",
    "    .reset_index(level=[0, 1], drop=True)\n",
    ")\n",
    "strain_freq_df_2['timepoint'] = (\n",
    "    strain_freq_df_2.sort_values(['date', 'time'])\n",
    "    .groupby(['species', 'subject'])\n",
    "    .apply(lambda group: group.groupby(['date', 'time']).ngroup() + 1)\n",
    "    .reset_index(level=[0, 1], drop=True)\n",
    ")\n",
    "\n",
    "### creating temporal groups\n",
    "temporal_groups = strain_freq_df[\n",
    "    strain_freq_df.groupby([\"species\", \"subject\", \"sample_type_number\"])[\"timepoint\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True) # consider only include capsules that are present in multiple timepoints\n",
    "temporal_groups_2 = strain_freq_df_2[\n",
    "    strain_freq_df_2.groupby([\"species\", \"subject\", \"sample_type_number\"])[\"timepoint\"].transform(\"nunique\") >= 2\n",
    "][[\"species\", \"subject\"]].drop_duplicates().reset_index(drop = True) # consider only include capsules that are present in multiple timepoints\n",
    "\n",
    "### slicing stool based on spatial groups\n",
    "strain_freq_df = strain_freq_df.merge(temporal_groups, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "strain_freq_df_2 = strain_freq_df_2.merge(temporal_groups_2, on=[\"species\", \"subject\"], how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "##### Reordering columns\n",
    "strain_freq_df = strain_freq_df[['species', 'strain','subject', 'sample_type','sample_type_number','tissue', 'date', 'time','timepoint', 'sample','freq','quantile_25', 'quantile_75', 'upper_ci','lower_ci']]\n",
    "strain_freq_df = strain_freq_df.reset_index(drop = True)\n",
    "strain_freq_df_2 = strain_freq_df_2[['species', 'strain','subject', 'sample_type','sample_type_number','tissue', 'date', 'time','timepoint', 'sample','freq','quantile_25', 'quantile_75', 'upper_ci','lower_ci']]\n",
    "strain_freq_df_2 = strain_freq_df_2.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Getting strain freq to equal 1\n",
    "strain_freq_df_misphased = pd.concat([strain_freq_df, strain_freq_df_2], axis = 0)\n",
    "grouped_sum = strain_freq_df_misphased.groupby(\n",
    "    ['sample','species']\n",
    ")['freq'].sum()\n",
    "misphased_groups = grouped_sum[grouped_sum != 1.0].reset_index()\n",
    "# Need to basically \n",
    "strain_freq_df_misphased = strain_freq_df_misphased.merge(\n",
    "    misphased_groups[['sample','species']],\n",
    "    on=['sample','species'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misphased_groups[misphased_groups.species == \"Anaerostipes_hadrus_55206\"]['freq'][34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_species_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_freqs_filtered.csv\"\n",
    "filtered_species_df = pd.read_csv(filtered_species_path)\n",
    "filtered_species_df = filtered_species_df[['species','subject']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_to_remove = set(map(tuple, filtered_species_df[['species','subject']].values))\n",
    "\n",
    "strain_freq_df_misphased_filtered = strain_freq_df_misphased[\n",
    "    ~strain_freq_df_misphased.set_index(['species','subject']).index.isin(pairs_to_remove)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_freq_df_misphased_filtered[['species','subject']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop for calculating freq ~ C(sample_type_number) + C(timepoint) for species-host pairs\n",
    "\n",
    "ols_results = []\n",
    "\n",
    "for i,species in enumerate(strain_freq_df.species.unique()):\n",
    "    \n",
    "    # Extracting hosts\n",
    "    host_list = strain_freq_df[strain_freq_df['species'] == species].subject.unique()\n",
    "\n",
    "    for j,host in enumerate(host_list):\n",
    "        \n",
    "        # subsetting the data\n",
    "        subset_data  = strain_freq_df.loc[(strain_freq_df.species == species) & (strain_freq_df.subject == host),:].copy()\n",
    "        print(f\"Iteration: Species={species}, Host={host}, Data Size={subset_data.shape}\")\n",
    "        subset_data['sample_type_number'] = subset_data['sample_type_number'].astype('category')\n",
    "        subset_data['timepoint'] = subset_data['timepoint'].astype('category')\n",
    "        subset_data['freq_clt'] = np.log(subset_data['freq'] + 1)\n",
    "\n",
    "        # skip if necessary\n",
    "        if len(subset_data['sample_type_number'].unique()) <= 1 or len(subset_data['timepoint'].unique()) <= 1 or subset_data.shape[0] <= 2:\n",
    "            continue\n",
    "        \n",
    "        # building the model\n",
    "        # model = ols(\"freq ~ C(sample_type_number) + C(timepoint)\", data=subset_data).fit()\n",
    "        try:\n",
    "            # model = ols(\"freq_clt ~ C(sample_type_number) + C(timepoint)\", data=subset_data).fit()\n",
    "            model = ols(\"freq_clt ~ C(timepoint) + C(sample_type_number)\", data=subset_data).fit()\n",
    "            anova_table = anova_lm(model, typ=2)\n",
    "            \n",
    "            # explained variance\n",
    "            explained_variance_space = anova_table.loc[\"C(sample_type_number)\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "            explained_variance_time = anova_table.loc[\"C(timepoint)\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "            residual_variance = anova_table.loc[\"Residual\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "            \n",
    "            # adding to the array\n",
    "            ols_results.append([species, host, explained_variance_space, explained_variance_time, residual_variance])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for species: {species}, host: {host} - {e}\")\n",
    "\n",
    "# building the pandas df\n",
    "ols_results = np.array(ols_results)\n",
    "ols_results_df = pd.DataFrame(ols_results, columns=[\"species\", \n",
    "                                                    \"subject\", \n",
    "                                                    \"explained_variance_space\", \n",
    "                                                    \"explained_variance_time\", \n",
    "                                                    \"residual_variance\"])\n",
    "\n",
    "ols_results_df[\"explained_variance_space\"] = ols_results_df[\"explained_variance_space\"].astype(float)\n",
    "ols_results_df[\"explained_variance_time\"] = ols_results_df[\"explained_variance_time\"].astype(float)\n",
    "ols_results_df[\"residual_variance\"] = ols_results_df[\"residual_variance\"].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of species: \" + str(len(ols_results_df.species.unique())))\n",
    "print(\"Number of hosts: \" + str(len(ols_results_df.species.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ols_results_df to long-form\n",
    "ols_results_long = pd.melt(\n",
    "    ols_results_df,\n",
    "    id_vars=[\"species\", \"subject\"],  # Columns to keep as identifiers\n",
    "    value_vars=[\"explained_variance_space\", \"explained_variance_time\", \"residual_variance\"],  # Columns to unpivot\n",
    "    var_name=\"variance_type\",  # Name for the new column containing labels\n",
    "    value_name=\"value\"  # Name for the new column containing values\n",
    ")\n",
    "\n",
    "ols_results_long['variance_type'] = ols_results_long['variance_type'].apply(\n",
    "    lambda x: \"spatial effect\" if x == \"explained_variance_space\" \n",
    "    else \"temporal effect\" if x == \"explained_variance_time\" \n",
    "    else \"residual variance\" if x == \"residual_variance\" \n",
    "    else x\n",
    ")\n",
    "\n",
    "ols_results_long['value'] = ols_results_long['value']*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_anova.csv\"\n",
    "ols_results_long.to_csv(out_path, sep = \",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))  # Define figure and axes with a specific size\n",
    "\n",
    "sns.boxplot(data = ols_results_long, x = \"variance_type\", y = \"value\", \n",
    "            showfliers = False,\n",
    "            boxprops={'facecolor': 'none', 'alpha': 0.5}, \n",
    "            ax = ax)\n",
    "sns.stripplot(data = ols_results_long, x = \"variance_type\", y = \"value\", \n",
    "              hue = \"species\",\n",
    "              palette= \"colorblind\",\n",
    "              jitter = False, \n",
    "              size = 7,\n",
    "              ax = ax)\n",
    "\n",
    "for (species, subject), group in ols_results_long.groupby(['species', 'subject']):\n",
    "    x_positions = [list(ols_results_long['variance_type'].unique()).index(v) for v in group['variance_type']]\n",
    "    y_values = group['value']\n",
    "    ax.plot(x_positions, y_values, marker='', linestyle='-', color='gray', alpha=0.7)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"\") \n",
    "ax.set_ylabel(\"Percent variance explained\", fontsize=18)    \n",
    "# tick parameters\n",
    "ax.tick_params(axis='x', labelsize=14)  \n",
    "ax.tick_params(axis='y', labelsize=14) \n",
    "# Place the legend outside the right edge of the plot\n",
    "ax.legend(title=\"Species\", fontsize=14, title_fontsize=16, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/revisions/strain_ANOVA_species.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches=\"tight\", facecolor = \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))  # Define figure and axes with a specific size\n",
    "\n",
    "# Define a custom palette for the variance_type column\n",
    "custom_palette = {\"spatial effect\": \"purple\", \n",
    "                  \"temporal effect\": \"red\", \n",
    "                  \"residual variance\": \"gray\"}  \n",
    "\n",
    "sns.boxplot(data = ols_results_long, x = \"variance_type\", y = \"value\", \n",
    "            showfliers = False,\n",
    "            boxprops={'facecolor': 'none', 'alpha': 0.5}, \n",
    "            ax = ax)\n",
    "sns.stripplot(data = ols_results_long, x = \"variance_type\", y = \"value\", \n",
    "              palette=custom_palette, \n",
    "              jitter = False, \n",
    "              size = 7,\n",
    "              ax = ax)\n",
    "\n",
    "for (species, subject), group in ols_results_long.groupby(['species', 'subject']):\n",
    "    x_positions = [list(ols_results_long['variance_type'].unique()).index(v) for v in group['variance_type']]\n",
    "    y_values = group['value']\n",
    "    ax.plot(x_positions, y_values, marker='', linestyle='-', color='gray', alpha=0.7)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"\") \n",
    "ax.set_ylabel(\"Percent variance explained\", fontsize=18)    \n",
    "# tick parameters\n",
    "ax.tick_params(axis='x', labelsize=14)  \n",
    "ax.tick_params(axis='y', labelsize=14) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/revisions/strain_ANOVA.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches=\"tight\", facecolor = \"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject 1 (including technical variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = strain_freq_df_S1[strain_freq_df_S1.species == \"Bacteroides_ovatus_58035\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop for calculating freq ~ C(sample_type_number) + C(timepoint) for species-host pairs\n",
    "\n",
    "ols_results = []\n",
    "\n",
    "for i,species in enumerate(strain_freq_df_S1.species.unique()):\n",
    "    \n",
    "    # Extracting hosts\n",
    "    host = 1\n",
    "\n",
    "    print(f\"Iteration: Species={species}, Host={host}, Data Size={subset_data.shape}\")\n",
    "    \n",
    "    # subsetting the data\n",
    "    subset_data  = strain_freq_df_S1.loc[(strain_freq_df_S1.species == species) & (strain_freq_df_S1.timepoint >= 5),:].copy()\n",
    "    subset_data['sample_type_number'] = subset_data['sample_type_number'].astype('category')\n",
    "    subset_data['timepoint'] = subset_data['timepoint'].astype('category')\n",
    "    subset_data['freq_clt'] = np.log(subset_data['freq'] + 1)\n",
    "\n",
    "    # skip if necessary\n",
    "    if len(subset_data['sample_type_number'].unique()) <= 1 or len(subset_data['timepoint'].unique()) <= 1 or subset_data.shape[0] <= 2:\n",
    "        continue\n",
    "    \n",
    "    # building the model\n",
    "    # model = ols(\"freq ~ C(sample_type_number) + C(timepoint)\", data=subset_data).fit()\n",
    "    try:\n",
    "        # model = ols(\"freq_clt ~ C(sample_type_number) + C(timepoint)\", data=subset_data).fit()\n",
    "        model = ols(\"freq_clt ~ C(timepoint) + C(sample_type_number)\", data=subset_data).fit()\n",
    "        anova_table = anova_lm(model, typ=2)\n",
    "        \n",
    "        # explained variance\n",
    "        explained_variance_space = anova_table.loc[\"C(sample_type_number)\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "        explained_variance_time = anova_table.loc[\"C(timepoint)\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "        residual_variance = anova_table.loc[\"Residual\", \"sum_sq\"] / anova_table[\"sum_sq\"].sum()\n",
    "        \n",
    "        # adding to the array\n",
    "        ols_results.append([species, host, explained_variance_space, explained_variance_time, residual_variance])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error for species: {species}, host: {host} - {e}\")\n",
    "\n",
    "# building the pandas df\n",
    "ols_results = np.array(ols_results)\n",
    "ols_results_df = pd.DataFrame(ols_results, columns=[\"species\", \n",
    "                                                    \"subject\", \n",
    "                                                    \"explained_variance_space\", \n",
    "                                                    \"explained_variance_time\", \n",
    "                                                    \"residual_variance\"])\n",
    "\n",
    "ols_results_df[\"explained_variance_space\"] = ols_results_df[\"explained_variance_space\"].astype(float)\n",
    "ols_results_df[\"explained_variance_time\"] = ols_results_df[\"explained_variance_time\"].astype(float)\n",
    "ols_results_df[\"residual_variance\"] = ols_results_df[\"residual_variance\"].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ols_results_df to long-form\n",
    "ols_results_long = pd.melt(\n",
    "    ols_results_df,\n",
    "    id_vars=[\"species\", \"subject\"],  # Columns to keep as identifiers\n",
    "    value_vars=[\"explained_variance_space\", \"explained_variance_time\", \"residual_variance\"],  # Columns to unpivot\n",
    "    var_name=\"variance_type\",  # Name for the new column containing labels\n",
    "    value_name=\"value\"  # Name for the new column containing values\n",
    ")\n",
    "\n",
    "ols_results_long['variance_type'] = ols_results_long['variance_type'].apply(\n",
    "    lambda x: \"spatial effect\" if x == \"explained_variance_space\" \n",
    "    else \"temporal effect\" if x == \"explained_variance_time\" \n",
    "    else \"residual variance\" if x == \"residual_variance\" \n",
    "    else x\n",
    ")\n",
    "\n",
    "ols_results_long['value'] = ols_results_long['value']*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))  # Define figure and axes with a specific size\n",
    "\n",
    "# Define a custom palette for the variance_type column\n",
    "custom_palette = {\"spatial effect\": \"purple\", \n",
    "                  \"temporal effect\": \"red\", \n",
    "                  \"residual variance\": \"gray\"}  \n",
    "\n",
    "sns.boxplot(data = ols_results_long, x = \"variance_type\", y = \"value\", \n",
    "            showfliers = False,\n",
    "            boxprops={'facecolor': 'none', 'alpha': 0.5}, \n",
    "            ax = ax)\n",
    "sns.stripplot(data = ols_results_long, x = \"variance_type\", y = \"value\", \n",
    "              palette=custom_palette, \n",
    "              jitter = False, \n",
    "              size = 7,\n",
    "              ax = ax)\n",
    "\n",
    "for (species, subject), group in ols_results_long.groupby(['species', 'subject']):\n",
    "    x_positions = [list(ols_results_long['variance_type'].unique()).index(v) for v in group['variance_type']]\n",
    "    y_values = group['value']\n",
    "    ax.plot(x_positions, y_values, marker='', linestyle='-', color='gray', alpha=0.7)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"\") \n",
    "ax.set_ylabel(\"Percent variance explained\", fontsize=18)    \n",
    "# tick parameters\n",
    "ax.tick_params(axis='x', labelsize=14)  \n",
    "ax.tick_params(axis='y', labelsize=14) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/revisions/strain_ANOVA_technical.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches=\"tight\", facecolor = \"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does coverage impact the number of samples that make it through my filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strain files iteratively\n",
    "print(\"Processing normal strain clusters\")\n",
    "\n",
    "strain_freq_df_list = []\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df['sample_type_number'] = strain_freq_df['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "strain_freq_df['coverage_filters'] = \"average 5 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\\n(default)\"\n",
    "\n",
    "\n",
    "print(\"Processing strain clusters with average coverage of 4 and min per site coverage of 10\")\n",
    "\n",
    "strain_freq_df_list = []\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters_avg4_min10/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df_min4_cov10 = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df_min4_cov10['sample_type_number'] = strain_freq_df_min4_cov10['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "strain_freq_df_min4_cov10['coverage_filters'] = \"average 4 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\"\n",
    "\n",
    "\n",
    "print(\"Processing strain clusters with average coverage of 5 and min per site coverage of 5\")\n",
    "\n",
    "strain_freq_df_list = []\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters_avg5_min5/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df_min5_cov5 = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df_min5_cov5['sample_type_number'] = strain_freq_df_min5_cov5['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "strain_freq_df_min5_cov5['coverage_filters'] = \"average 5 reads,\\nper SNV minimum 5 reads,\\n100 SNVs per sample\"\n",
    "\n",
    "\n",
    "print(\"Processing strain clusters with average coverage of 4 and min per site coverage of 5\")\n",
    "\n",
    "strain_freq_df_list = []\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters_avg4_min5/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df_min4_cov5 = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df_min4_cov5['sample_type_number'] = strain_freq_df_min4_cov5['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "strain_freq_df_min4_cov5['coverage_filters'] = \"average 4 reads,\\nper SNV minimum 5 reads,\\n100 SNVs per sample\"\n",
    "\n",
    "print(\"Processing strain clusters with average coverage of 5 and min per site coverage of 5, and 100 SNVs per cluster\")\n",
    "\n",
    "strain_freq_df_list = []\n",
    "for species_i,species in enumerate(species_list):\n",
    "    for subject_i,subject_id in enumerate(subjects):\n",
    "        file_path = \"/u/project/ngarud/Garud_lab/metagenomic_fastq_files/Shalon_2023/strain_phasing/strain_clusters_avg5_min5_50snvs/%s/%s_subject_%s_strain_frequency.csv\" % (species, species, subject_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue \n",
    "    \n",
    "        strain_freq_df_list.append(pd.read_csv(file_path, sep = \"\\t\"))\n",
    "    \n",
    "strain_freq_df_min5_cov5_50snvs = pd.concat(strain_freq_df_list, ignore_index=True)\n",
    "strain_freq_df_min5_cov5_50snvs['sample_type_number'] = strain_freq_df_min5_cov5_50snvs['sample'].apply(lambda x: sample_metadata_map[x][2])\n",
    "strain_freq_df_min5_cov5_50snvs['coverage_filters'] = \"average 5 reads,\\nper SNV minimum 5 reads,\\n50 SNVs per sample\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all strain frequency DataFrames into a single DataFrame\n",
    "all_strain_freq_df = pd.concat(\n",
    "    [\n",
    "        strain_freq_df,\n",
    "        strain_freq_df_min4_cov10,\n",
    "        strain_freq_df_min5_cov5,\n",
    "        strain_freq_df_min4_cov5,\n",
    "        strain_freq_df_min5_cov5_50snvs\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_dataframe = pd.DataFrame(all_strain_freq_df.groupby(['species', 'subject', 'coverage_filters']).size()).reset_index().rename(columns={0:\"number_of_samples\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to wide format\n",
    "sample_no_dataframe_wide = sample_no_dataframe.pivot(index=['species', 'subject'], columns='coverage_filters', values='number_of_samples')\n",
    "sample_no_dataframe_wide.columns = sample_no_dataframe_wide.columns.astype(str)\n",
    "sample_no_dataframe_wide = sample_no_dataframe_wide.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_dataframe_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_1 = sample_no_dataframe_wide['average 5 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\\n(default)'] < sample_no_dataframe_wide['average 4 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample']\n",
    "condition_2 = sample_no_dataframe_wide['average 5 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\\n(default)'] < sample_no_dataframe_wide['average 5 reads,\\nper SNV minimum 5 reads,\\n100 SNVs per sample']\n",
    "condition_3 = sample_no_dataframe_wide['average 5 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\\n(default)'] < sample_no_dataframe_wide['average 4 reads,\\nper SNV minimum 5 reads,\\n100 SNVs per sample']\n",
    "condition_4 = sample_no_dataframe_wide['average 5 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\\n(default)'] < sample_no_dataframe_wide['average 5 reads,\\nper SNV minimum 5 reads,\\n50 SNVs per sample']\n",
    "\n",
    "conditions_with_an_increase = sample_no_dataframe_wide[condition_1 | condition_2 | condition_3 | condition_4]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_with_an_increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_1 = sample_no_dataframe_wide.species == \"Adlercreutzia_equolifaciens_60310\"\n",
    "condition_2 = sample_no_dataframe_wide.subject == 12\n",
    "sample_no_dataframe_wide[condition_1 & condition_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))  # Define figure and axes with a specific size\n",
    "\n",
    "desired_order = [\n",
    "    'average 5 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample\\n(default)',\n",
    "    'average 4 reads,\\nper SNV minimum 10 reads,\\n100 SNVs per sample',\n",
    "    'average 5 reads,\\nper SNV minimum 5 reads,\\n100 SNVs per sample',\n",
    "    'average 4 reads,\\nper SNV minimum 5 reads,\\n100 SNVs per sample',\n",
    "    'average 5 reads,\\nper SNV minimum 5 reads,\\n50 SNVs per sample'\n",
    "]\n",
    "\n",
    "sample_no_dataframe['coverage_filters'] = pd.Categorical(\n",
    "    sample_no_dataframe['coverage_filters'],\n",
    "    categories=desired_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "sns.boxplot(data=sample_no_dataframe, x=\"coverage_filters\", y=\"number_of_samples\", \n",
    "            showfliers=False,\n",
    "            palette = \"colorblind\",\n",
    "            boxprops={'alpha': 0.5},  # Correct transparency\n",
    "            ax=ax)\n",
    "sns.stripplot(data=sample_no_dataframe, x=\"coverage_filters\", y=\"number_of_samples\", \n",
    "              hue=\"coverage_filters\",\n",
    "              palette=\"colorblind\",\n",
    "              jitter=False, \n",
    "              size=7,\n",
    "              ax=ax)\n",
    "\n",
    "for (species, subject), group in sample_no_dataframe.groupby(['species', 'subject']):\n",
    "    x_positions = [list(sample_no_dataframe['coverage_filters'].unique()).index(v) for v in group['coverage_filters']]\n",
    "    y_values = group['number_of_samples']\n",
    "    ax.plot(x_positions, y_values, marker='', linestyle='-', color='gray', alpha=0.7)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"\") \n",
    "ax.set_ylabel(\"number of samples\", fontsize=18)    \n",
    "# Tick parameters\n",
    "ax.tick_params(axis='x', labelsize=14, rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "ax.tick_params(axis='y', labelsize=14) \n",
    "\n",
    "# Align x-axis labels to the right\n",
    "ax.set_xticklabels(ax.get_xticklabels(), ha='right')\n",
    "\n",
    "# Remove the legend\n",
    "ax.legend_.remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/figures/revisions/effect_of_coverage_on_the_number_of_samples.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches=\"tight\", facecolor = \"white\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
