{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6af3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting working directory\n",
    "import sys\n",
    "sys.path.insert(0, \"/u/project/ngarud/michaelw/Diversity-Along-Gut/Shalon_2023/scripts/strain_phasing/\")\n",
    "\n",
    "# Normal Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import random as rand\n",
    "from random import randint,sample,choices\n",
    "from math import log\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import figure_utils as fu\n",
    "from matplotlib import colormaps\n",
    "plasma_cmap = colormaps.get_cmap('plasma')\n",
    "\n",
    "# config\n",
    "import config\n",
    "\n",
    "# predefined functions\n",
    "from strain_phasing_functions import *\n",
    "from microbiome_evolution_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a693a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_path = \"%s%s\" % (config.analysis_directory, \"metadata/species_snps.txt\")\n",
    "with open(species_path, 'r') as file:\n",
    "    species_list = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55aab8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acidaminococcus_intestini_54097',\n",
       " 'Actinomyces_graevenitzii_58300',\n",
       " 'Actinomyces_sp_57735',\n",
       " 'Actinomyces_sp_62581',\n",
       " 'Actinomyces_viscosus_57672',\n",
       " 'Adlercreutzia_equolifaciens_60310',\n",
       " 'Aggregatibacter_aphrophilus_58143',\n",
       " 'Akkermansia_muciniphila_55290',\n",
       " 'Alistipes_finegoldii_56071',\n",
       " 'Alistipes_indistinctus_62207',\n",
       " 'Alistipes_onderdonkii_55464',\n",
       " 'Alistipes_putredinis_61533',\n",
       " 'Alistipes_senegalensis_58364',\n",
       " 'Alistipes_shahii_62199',\n",
       " 'Anaerostipes_hadrus_55206',\n",
       " 'Atopobium_parvulum_59960',\n",
       " 'Atopobium_sp_59401',\n",
       " 'Bacteroidales_bacterium_58650',\n",
       " 'Bacteroides_caccae_53434',\n",
       " 'Bacteroides_cellulosilyticus_58046',\n",
       " 'Bacteroides_clarus_62282',\n",
       " 'Bacteroides_coprocola_61586',\n",
       " 'Bacteroides_eggerthii_54457',\n",
       " 'Bacteroides_finegoldii_57739',\n",
       " 'Bacteroides_fragilis_54507',\n",
       " 'Bacteroides_intestinalis_61596',\n",
       " 'Bacteroides_massiliensis_44749',\n",
       " 'Bacteroides_ovatus_58035',\n",
       " 'Bacteroides_pectinophilus_61619',\n",
       " 'Bacteroides_plebeius_61623',\n",
       " 'Bacteroides_salyersiae_54873',\n",
       " 'Bacteroides_stercoris_56735',\n",
       " 'Bacteroides_thetaiotaomicron_56941',\n",
       " 'Bacteroides_uniformis_57318',\n",
       " 'Bacteroides_vulgatus_57955',\n",
       " 'Bacteroides_xylanisolvens_57185',\n",
       " 'Barnesiella_intestinihominis_62208',\n",
       " 'Bifidobacterium_adolescentis_56815',\n",
       " 'Bifidobacterium_angulatum_45832',\n",
       " 'Bifidobacterium_animalis_58116',\n",
       " 'Bifidobacterium_bifidum_55065',\n",
       " 'Bifidobacterium_catenulatum_58257',\n",
       " 'Bifidobacterium_longum_57796',\n",
       " 'Bifidobacterium_pseudocatenulatum_57754',\n",
       " 'Bilophila_wadsworthia_57364',\n",
       " 'Blautia_producta_56315',\n",
       " 'Blautia_wexlerae_56130',\n",
       " 'Burkholderiales_bacterium_56577',\n",
       " 'Butyricimonas_virosa_58742',\n",
       " 'Capnocytophaga_gingivalis_61780',\n",
       " 'Capnocytophaga_granulosa_57613',\n",
       " 'Capnocytophaga_sputigena_61779',\n",
       " 'Catenibacterium_mitsuokai_61547',\n",
       " 'Citrobacter_braakii_56022',\n",
       " 'Clostridiales_bacterium_56470',\n",
       " 'Clostridiales_bacterium_61057',\n",
       " 'Clostridium_bolteae_57158',\n",
       " 'Clostridium_citroniae_62210',\n",
       " 'Clostridium_sp_61482',\n",
       " 'Clostridium_spiroforme_61500',\n",
       " 'Collinsella_aerofaciens_61484',\n",
       " 'Collinsella_intestinalis_61689',\n",
       " 'Collinsella_sp_62205',\n",
       " 'Coprococcus_catus_62200',\n",
       " 'Coprococcus_comes_61587',\n",
       " 'Coprococcus_sp_62244',\n",
       " 'Coriobacteriaceae_bacterium_58375',\n",
       " 'Desulfovibrio_piger_61475',\n",
       " 'Dialister_invisus_61905',\n",
       " 'Dorea_formicigenerans_56346',\n",
       " 'Dorea_longicatena_59913',\n",
       " 'Dorea_longicatena_61473',\n",
       " 'Eggerthella_lenta_56463',\n",
       " 'Enterobacter_aerogenes_55704',\n",
       " 'Enterobacter_cloacae_55011',\n",
       " 'Enterobacter_cloacae_55612',\n",
       " 'Enterobacter_cloacae_57303',\n",
       " 'Enterobacter_cloacae_58148',\n",
       " 'Enterobacter_cloacae_58287',\n",
       " 'Enterococcus_durans_52177',\n",
       " 'Enterococcus_faecalis_56297',\n",
       " 'Enterococcus_faecium_56947',\n",
       " 'Enterococcus_hirae_55131',\n",
       " 'Escherichia_coli_58110',\n",
       " 'Eubacterium_biforme_61684',\n",
       " 'Eubacterium_cylindroides_56237',\n",
       " 'Eubacterium_eligens_61678',\n",
       " 'Eubacterium_hallii_61477',\n",
       " 'Eubacterium_limosum_60946',\n",
       " 'Eubacterium_ramulus_59802',\n",
       " 'Eubacterium_rectale_56927',\n",
       " 'Eubacterium_siraeum_57634',\n",
       " 'Eubacterium_sulci_62452',\n",
       " 'Eubacterium_ventriosum_61474',\n",
       " 'Faecalibacterium_cf_62236',\n",
       " 'Faecalibacterium_prausnitzii_57453',\n",
       " 'Faecalibacterium_prausnitzii_61481',\n",
       " 'Faecalibacterium_prausnitzii_62201',\n",
       " 'Fusobacterium_periodonticum_58002',\n",
       " 'Gemella_haemolysans_61762',\n",
       " 'Gemella_morbillorum_61817',\n",
       " 'Gemella_sanguinis_57791',\n",
       " 'Gordonibacter_pamelaeae_62044',\n",
       " 'Granulicatella_elegans_61945',\n",
       " 'Guyana_massiliensis_60772',\n",
       " 'Haemophilus_haemolyticus_58348',\n",
       " 'Haemophilus_parainfluenzae_62356',\n",
       " 'Haemophilus_pittmaniae_58383',\n",
       " 'Haemophilus_sputorum_53575',\n",
       " 'Kingella_oralis_61952',\n",
       " 'Klebsiella_oxytoca_54123',\n",
       " 'Klebsiella_oxytoca_56762',\n",
       " 'Klebsiella_oxytoca_57801',\n",
       " 'Klebsiella_pneumoniae_54688',\n",
       " 'Klebsiella_pneumoniae_54788',\n",
       " 'Klebsiella_pneumoniae_57337',\n",
       " 'Lachnospiraceae_bacterium_51870',\n",
       " 'Lactobacillus_acidophilus_51143',\n",
       " 'Lactobacillus_delbrueckii_56845',\n",
       " 'Lactobacillus_rhamnosus_57549',\n",
       " 'Lactobacillus_ruminis_57020',\n",
       " 'Lactococcus_garvieae_57061',\n",
       " 'Lautropia_mirabilis_62440',\n",
       " 'Leptotrichia_sp_60101',\n",
       " 'Megamonas_hypermegale_57114',\n",
       " 'Megasphaera_micronuciformis_62167',\n",
       " 'Megasphaera_sp_50408',\n",
       " 'Mitsuokella_jalaludinii_60453',\n",
       " 'Mitsuokella_multacida_61656',\n",
       " 'Neisseria_elongata_61756',\n",
       " 'Neisseria_sicca_58189',\n",
       " 'Neisseria_subflava_61760',\n",
       " 'Odoribacter_splanchnicus_62174',\n",
       " 'Oribacterium_sp_54059',\n",
       " 'Oscillibacter_sp_60799',\n",
       " 'Oscillospiraceae_bacterium_54867',\n",
       " 'Oxalobacter_formigenes_61802',\n",
       " 'Parabacteroides_distasonis_56985',\n",
       " 'Parabacteroides_goldsteinii_56831',\n",
       " 'Parabacteroides_johnsonii_55217',\n",
       " 'Parabacteroides_merdae_56972',\n",
       " 'Paraprevotella_clara_33712',\n",
       " 'Pediococcus_acidilactici_57243',\n",
       " 'Pediococcus_pentosaceus_55856',\n",
       " 'Phascolarctobacterium_sp_59817',\n",
       " 'Phascolarctobacterium_sp_59818',\n",
       " 'Porphyromonas_sp_57899',\n",
       " 'Prevotella_copri_61740',\n",
       " 'Prevotella_histicola_56864',\n",
       " 'Prevotella_intermedia_57633',\n",
       " 'Prevotella_melaninogenica_58075',\n",
       " 'Prevotella_nanceiensis_44721',\n",
       " 'Prevotella_oris_57058',\n",
       " 'Prevotella_pallens_62692',\n",
       " 'Prevotella_salivae_57216',\n",
       " 'Prevotella_shahii_59695',\n",
       " 'Prevotella_sp_58138',\n",
       " 'Prevotella_sp_58566',\n",
       " 'Prevotella_sp_61818',\n",
       " 'Propionibacterium_freudenreichii_62075',\n",
       " 'Raoultella_planticola_57814',\n",
       " 'Roseburia_hominis_61877',\n",
       " 'Roseburia_intestinalis_56239',\n",
       " 'Roseburia_inulinivorans_61943',\n",
       " 'Rothia_dentocariosa_57938',\n",
       " 'Ruminococcus_bicirculans_59300',\n",
       " 'Ruminococcus_bromii_62047',\n",
       " 'Ruminococcus_callidus_61479',\n",
       " 'Ruminococcus_gnavus_57638',\n",
       " 'Ruminococcus_lactaris_55568',\n",
       " 'Ruminococcus_obeum_61472',\n",
       " 'Ruminococcus_obeum_62046',\n",
       " 'Ruminococcus_sp_55468',\n",
       " 'Ruminococcus_torques_62045',\n",
       " 'Serratia_marcescens_59790',\n",
       " 'Streptococcus_anginosus_58223',\n",
       " 'Streptococcus_australis_62469',\n",
       " 'Streptococcus_infantarius_58294',\n",
       " 'Streptococcus_infantis_58320',\n",
       " 'Streptococcus_infantis_62384',\n",
       " 'Streptococcus_infantis_62471',\n",
       " 'Streptococcus_mitis_58556',\n",
       " 'Streptococcus_mitis_60474',\n",
       " 'Streptococcus_mitis_62363',\n",
       " 'Streptococcus_oralis_58560',\n",
       " 'Streptococcus_parasanguinis_58126',\n",
       " 'Streptococcus_pseudopneumoniae_58459',\n",
       " 'Streptococcus_salivarius_58037',\n",
       " 'Streptococcus_sp_54717',\n",
       " 'Streptococcus_sp_58554',\n",
       " 'Streptococcus_thermophilus_54772',\n",
       " 'Streptococcus_vestibularis_56030',\n",
       " 'Sutterella_wadsworthensis_56828',\n",
       " 'Sutterella_wadsworthensis_62218',\n",
       " 'Veillonella_atypica_58169',\n",
       " 'Veillonella_dispar_61763',\n",
       " 'Veillonella_parvula_57794',\n",
       " 'Veillonella_parvula_58184',\n",
       " 'Veillonella_sp_62404',\n",
       " 'Weissella_confusa_59158']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b6a939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Desulfovibrio_piger_61475\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster directory already exists.\n",
      "Figure directory already exists.\n"
     ]
    }
   ],
   "source": [
    "strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "\n",
    "strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "\n",
    "#Raw cluster\n",
    "# raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "species_raw_cluster_dir = \"%s%s/\" % (raw_cluster_path, species)\n",
    "if not os.path.exists(species_raw_cluster_dir):\n",
    "    os.makedirs(species_raw_cluster_dir)\n",
    "    print(\"Cluster directory created successfully!\")\n",
    "else:\n",
    "    print(\"Cluster directory already exists.\")\n",
    "\n",
    "strain_phasing_figures_dir = \"%s%s\" % (config.figure_directory, \"strain_phasing/\")\n",
    "if not os.path.exists(strain_phasing_figures_dir):\n",
    "    os.makedirs(strain_phasing_figures_dir)\n",
    "    print(\"Figure directory created successfully!\")\n",
    "else:\n",
    "    print(\"Figure directory already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25251fc4",
   "metadata": {},
   "source": [
    "## Selecting host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3896528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "subject_sample_map = parse_subject_sample_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67fa8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting median coverage\n",
    "## Loading coverage df\n",
    "coverage_df_path = \"%s%s\" % (config.data_directory, \"species/coverage.txt.bz2\")\n",
    "coverage_df = pd.read_csv(coverage_df_path, sep = \"\\t\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccd47a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvg_data = []\n",
    "for subject_id in subject_sample_map.keys():\n",
    "    samples_of_interest = [sample for sample in list(subject_sample_map[subject_id].keys())if sample in coverage_df.columns]\n",
    "    coverage_df_subject = coverage_df.loc[coverage_df.species_id == species, [\"species_id\"] + samples_of_interest]\n",
    "    median_coverage = coverage_df_subject[samples_of_interest].median(axis = 1).values[0]\n",
    "    cvg_data.append([subject_id, median_coverage])\n",
    "cvg_data = pd.DataFrame(data = cvg_data, columns = ['subject', 'median_coverage']).sort_values(by = 'median_coverage', ascending = False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68f67ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>median_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2.601905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.352792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>0.277947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.221344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  median_coverage\n",
       "0        9         2.601905\n",
       "1       12         0.352792\n",
       "2       14         0.277947\n",
       "3        2         0.221344\n",
       "4        1         0.000000\n",
       "5       10         0.000000\n",
       "6        3         0.000000\n",
       "7       11         0.000000\n",
       "8        8         0.000000\n",
       "9        7         0.000000\n",
       "10       6         0.000000\n",
       "11      15         0.000000\n",
       "12      13         0.000000\n",
       "13       5         0.000000\n",
       "14       4         0.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c486cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"9\"\n",
    "# samples_of_interest = [sample for sample in list(subject_sample_map[subject_id].keys())if sample in coverage_df.columns]\n",
    "samples_of_interest = list(subject_sample_map[subject_id].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "min_cluster_size = 1000\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "min_cluster_fraction = 1/10\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more   \n",
    "max_d = 3.5\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 10 \n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "poly_cov_frac = 1/5 #\n",
    "\n",
    "## Number of clusters to calculate\n",
    "n_clusters = 100\n",
    "\n",
    "#Minimum number of snvs per sample\n",
    "min_num_snvs_per_sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not using read support.\n"
     ]
    }
   ],
   "source": [
    "Fs,Ass,Dss = return_FAD(species, min_coverage=min_coverage, \n",
    "                        min_sample_coverage=min_sample_coverage, \n",
    "                        poly_cov_frac = poly_cov_frac, \n",
    "                        calculate_poly_cov_frac=False, \n",
    "                        read_support = False, # read_support = False,\n",
    "                        read_support_no = 2,\n",
    "                        subject_id=subject_id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out samples without an adequate number of SNVs when all is said and done\n",
    "sample_with_adequate_snv_count = ~((~np.isnan(Fs)).sum() < min_num_snvs_per_sample)\n",
    "\n",
    "Fs = Fs.loc[:,sample_with_adequate_snv_count]\n",
    "Ass = Ass.loc[:,sample_with_adequate_snv_count]\n",
    "Dss = Dss.loc[:,sample_with_adequate_snv_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 ms, sys: 5.86 ms, total: 21.9 ms\n",
      "Wall time: 15.9 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 250 SNVs"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fss = Ass.values/(Dss.values + (Dss.values == 0)) #This is so it doesn't produce a na (division by 0)\n",
    "\n",
    "cluster_As = Ass.values\n",
    "cluster_Ds = Dss.values\n",
    "cluster_fs = cluster_As/(cluster_Ds + (cluster_Ds == 0))\n",
    "\n",
    "## for compatibility in case of threshold number of SNVs\n",
    "num = min(max_num_snvs,Fs.shape[0])\n",
    "\n",
    "i_list = Dss.T.mean().sort_values(ascending=False).index[:num]\n",
    "\n",
    "sys.stderr.write(\"Processing %s SNVs\" % num)\n",
    "\n",
    "## simply shuffles indices if no threshold is specified\n",
    "#i_list = sample(range(Fs.shape[0]),num)\n",
    "i_list_idx = Fs.loc[i_list].index\n",
    "\n",
    "Ass_sub = Ass.loc[i_list_idx]\n",
    "Dss_sub = Dss.loc[i_list_idx]\n",
    "Fs_sub = Fs.loc[i_list_idx]\n",
    "\n",
    "fss_sub = Ass_sub.values/(Dss_sub.values + (Dss_sub.values == 0))\n",
    "\n",
    "cluster_As_sub = Ass_sub.values\n",
    "cluster_Ds_sub = Dss_sub.values\n",
    "cluster_fs_sub = cluster_As_sub/(cluster_Ds_sub + (cluster_Ds_sub == 0))\n",
    "\n",
    "D_mat = np.zeros([num,num])\n",
    "D_mat_1 = D_mat_fun1(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "D_mat = np.zeros([num,num]) \n",
    "D_mat_2 = D_mat_fun2(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "\n",
    "D_mat = np.fmin(D_mat_1,D_mat_2) #I believe this is filling in the minimum of the two polarizations\n",
    "D_mat = symmetrize(D_mat)\n",
    "\n",
    "D_mat_1 = pd.DataFrame(D_mat_1,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "D_mat_2 = pd.DataFrame(D_mat_2,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "\n",
    "D_mat_close = pd.DataFrame(D_mat < max_d) \n",
    "\n",
    "D_mat_close.index = Fs_sub.index\n",
    "D_mat_close.columns = Fs_sub.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracts up to 100 clusters\n",
    "## in practice all SNVs should fall into one of a fairly small number of clusters\n",
    "## really should re-write this with a while loop but this works for now\n",
    "## the idea is that we exhaust all clusters—there should only be a small number of them ultimately\n",
    "\n",
    "###Idea with while loop:\n",
    "##### While there are still variants out there, have it try to be clusterings\n",
    "\n",
    "all_clus_pol = []\n",
    "all_clus_idx = []\n",
    "all_clus_A = []\n",
    "all_clus_D = []\n",
    "\n",
    "all_clus_F = []\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        clus,clus_idxs = return_clus(D_mat_close,Fs_sub)\n",
    "#         clus,clus_idxs = return_clus(D_mat_close,Fs_sub, co_cluster_pct=0.5) #Finding points that cluster with 25% other points. That's a cluster.\n",
    "                                                            #We would modify this function to get smaller clusters...\n",
    "        clus_pol = polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2)\n",
    "        clus_pol.index = clus_idxs\n",
    "        D_mat_close = drop_clus_idxs(D_mat_close,clus_idxs)\n",
    "\n",
    "        if clus_pol.shape[0] > min_cluster_size and clus_pol.shape[0] > Fs.shape[0]*min_cluster_fraction:\n",
    "\n",
    "            all_clus_D.append(Dss.loc[clus.index].mean().values)\n",
    "            all_clus_pol.append(clus_pol)\n",
    "            all_clus_A.append(clus_pol.mean()*all_clus_D[-1])\n",
    "            all_clus_F.append(clus_pol.mean())\n",
    "\n",
    "            print(clus_pol.shape[0])\n",
    "\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## now, choosing a representative SNV from each cluster, and finding all other sites (not just limited to the 20k)\n",
    "## which are consistent w/ being linked to it\n",
    "\n",
    "final_clusters = []\n",
    "\n",
    "all_aligned_sites = []\n",
    "\n",
    "for i in range(len(all_clus_D)):\n",
    "    \n",
    "    sys.stderr.write(f'\\n\\nCluster {i+1}\\n')\n",
    "    ancD = all_clus_D[i]\n",
    "    ancF = all_clus_F[i]\n",
    "\n",
    "    dss = Dss.values\n",
    "    fss = Fs.values\n",
    "    \n",
    "    disAnc_forward = []\n",
    "    disAnc_backward = []\n",
    "\n",
    "    for j in range(Dss.shape[0]):\n",
    "        disAnc_forward.append(calc_dis(ancD,dss[j],ancF,fss[j]))\n",
    "        disAnc_backward.append(calc_dis(ancD,dss[j],ancF,1-fss[j]))\n",
    "        if j % 1000 == 0:\n",
    "            sys.stderr.write(f\"\\n\\t{np.around(100*j/Dss.shape[0],3)}% finished\")\n",
    "    \n",
    "    disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "    disAnc = np.array(disAnc)\n",
    "    aligned_sites = Fs.loc[disAnc < max_d].index\n",
    "    f_dist =  pd.DataFrame(np.array([disAnc_forward,disAnc_backward]).T,index=Fs.index)\n",
    "    pols = f_dist.T.idxmin() > 0\n",
    "    \n",
    "    aligned_sites = [a for a in aligned_sites if a not in all_aligned_sites]\n",
    "    \n",
    "    pols = pols.loc[aligned_sites]\n",
    "    re_polarize = pols.loc[pols].index\n",
    "    \n",
    "    all_aligned_sites.extend(aligned_sites)\n",
    "    \n",
    "    Fs_cluster = Fs.loc[aligned_sites]\n",
    "    \n",
    "    Fs_cluster.loc[re_polarize] = 1 - Fs_cluster.loc[re_polarize]\n",
    "        \n",
    "    final_clusters.append(Fs_cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SAVING RAW FILE\n",
    "# species_raw_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_RawCluster.pckl\")\n",
    "\n",
    "# pickle_object = open(species_raw_cluster_path, \"wb\")\n",
    "# pickle.dump(final_clusters, pickle_object)\n",
    "# pickle_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LOADING THE RAW FILE\n",
    "species_raw_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_RawCluster.pckl\")\n",
    "\n",
    "final_clusters = pd.read_pickle(species_raw_cluster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating polarized clusters\n",
    "\n",
    "Once clusters have been identified and internally polarized, they need to be polarized relative to one another. In the best case, the sum of strain frequencies will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No clusters detected.\n",
      "Not using read support.\n"
     ]
    }
   ],
   "source": [
    "no_cluster = False\n",
    "polarize = True\n",
    "\n",
    "if len(final_clusters) > 1:\n",
    "    sys.stderr.write(\"More than 1 strains detected.\\n\")\n",
    "## If only a single cluster is detected, add a second \"cluster\" which is simply 1 minus the allele frequencies\n",
    "## in the first cluster\n",
    "## aids in visualization for people not familiar with this kind of clustering\n",
    "    \n",
    "if len(final_clusters) == 0:\n",
    "    sys.stderr.write(\"No clusters detected.\\n\")\n",
    "    no_cluster = True\n",
    "\n",
    "    Fs,Ass,Dss = return_FAD(species, min_coverage=0, \n",
    "                            min_sample_coverage=min_sample_coverage, \n",
    "                            poly_cov_frac = 0, \n",
    "                            calculate_poly_cov_frac=False, \n",
    "                            read_support = False, \n",
    "                            subject_id=subject_id) \n",
    "    \n",
    "    if Fs.mean().mean() < 0.5:\n",
    "        df_final_f = 1 - Fs.mean().T\n",
    "        df_final_f.loc[:,:] = 1\n",
    "        final_f = []\n",
    "        final_f.append(df_final_f)\n",
    "        final_clusters = []\n",
    "        final_clusters.append(1 - Fs)\n",
    "    else:\n",
    "        df_final_f = Fs.mean().T\n",
    "        df_final_f.loc[:,:] = 1\n",
    "        final_f = []\n",
    "        final_f.append(df_final_f)\n",
    "        final_clusters = []\n",
    "        final_clusters.append(Fs)\n",
    "        \n",
    "else:\n",
    "    sys.stderr.write(\"At least one strain is present. Polarizing\\n\")\n",
    "    if len(final_clusters) == 1:\n",
    "        final_clusters.append(1-final_clusters[0])\n",
    "\n",
    "    ## add cluster centroids\n",
    "    final_f = []\n",
    "    for cluster in final_clusters:\n",
    "        final_f.append(cluster.mean())\n",
    "    df_final_f = pd.DataFrame(final_f)\n",
    "\n",
    "    ## now, polarize clusters so that the sum of squareds of the centroids to 1 is minimized\n",
    "    ## the idea here is that accurate strain frequencies should sum to 1\n",
    "\n",
    "    pol_d2 = {}\n",
    "\n",
    "    for i in range(df_final_f.shape[0]):\n",
    "        df_final_f_temp = df_final_f.copy() #Makes a copy of the centroids\n",
    "        df_final_f_temp.iloc[i] = 1 - df_final_f_temp.iloc[i] #gets the polarized version of ONE of the centroids.\n",
    "        pol_d2[i] =  ((1 - df_final_f_temp.sum())**2).sum()   #Get the across centroids for all samples (should be close to 1), \n",
    "                                                                    #subtract this from 1, and square. Sum all those values\n",
    "                                                                    #Ideally, this value is really close to 0. \n",
    "                                                                    #Add this value to the dictionary.\n",
    "\n",
    "        pol_d2 = pd.Series(pol_d2)                                #Make the dictionary a series \n",
    "\n",
    "        if pol_d2.min() < ((1 - df_final_f.sum())**2).sum(): #If any of the above repolarizations actually made the overall sum of centroids closer to 1, repolarize.\n",
    "            clus_to_re_pol = pol_d2.idxmin()\n",
    "            final_f[clus_to_re_pol] = 1 - final_f[clus_to_re_pol]\n",
    "            final_clusters[clus_to_re_pol] = 1 - final_clusters[clus_to_re_pol]\n",
    "            df_final_f = pd.DataFrame(final_f)  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out samples in which each cluster does not have adequate snvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indices = []\n",
    "\n",
    "for i,cluster in enumerate(final_clusters):\n",
    "    if i == 0:\n",
    "        good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "    else:\n",
    "        new_good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "        good_samples = good_samples & new_good_samples\n",
    "\n",
    "for i,cluster in enumerate(final_clusters): \n",
    "    final_clusters[i] = final_clusters[i].T.loc[good_samples].T\n",
    "    final_f[i] = final_f[i].T.loc[good_samples]\n",
    "\n",
    "Fs = Fs.T.loc[good_samples].T\n",
    "Ass = Ass.T.loc[good_samples].T\n",
    "Dss = Dss.T.loc[good_samples].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter all all na columns if there are any - THis is redundant\n",
    "if len(final_clusters) > 0:\n",
    "    for i,cluster in enumerate(final_clusters):\n",
    "        if i == 0:\n",
    "            mask = ~np.isnan(cluster).all(axis = 0)\n",
    "        final_clusters[i] = cluster.loc[:,mask]\n",
    "        final_f[i] = final_f[i][mask]\n",
    "    Fs = Fs.loc[:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #SAVING RAW FILE\n",
    "# species_centroid_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_ClusterCentroid.pckl\")\n",
    "# species_polarized_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_PolarizedCluster.pckl\")\n",
    "# final_Fs_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_final_Fs.pckl\")\n",
    "\n",
    "# pickle_object = open(species_centroid_cluster_path, \"wb\")\n",
    "# pickle.dump(final_f, pickle_object)\n",
    "# pickle_object.close()\n",
    "\n",
    "# pickle_object = open(species_polarized_cluster_path, \"wb\")\n",
    "# pickle.dump(final_clusters, pickle_object)\n",
    "# pickle_object.close()\n",
    "\n",
    "# pickle_object = open(final_Fs_path, \"wb\")\n",
    "# pickle.dump(Fs, pickle_object)\n",
    "# pickle_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot the chosen polarization of strains\n",
    "## sum of strain frequencies should be ~1 at all timepoints\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(pd.DataFrame(final_f).sum().values,zorder=10,lw=3)\n",
    "ax.set_ylim([.5,1.5])\n",
    "ax.axhline(1,color=\"k\",ls=\"--\")\n",
    "ax.set_ylabel(\"Sum of strain frequencies\",size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting strain frequencies\n",
    "\n",
    "Strain frequencies can be plotted using a main key (e.g. Day) and a secondary key (e.g. Capsule), yielding a two-level identification of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0780d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = True\n",
    "subset_value = 1000\n",
    "\n",
    "if subset:\n",
    "    high_coverage_snv_idxs = Dss.median(axis = 1).sort_values(ascending = False).index\n",
    "\n",
    "# Create the line plot\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    if subset:\n",
    "        high_coverage_snv_idxs_strain = high_coverage_snv_idxs.intersection(final_clusters[i].index)[:subset_value]\n",
    "        sns.lineplot(data = final_clusters[i].loc[high_coverage_snv_idxs_strain].T.values, ax = ax, palette=[[\"red\", \"blue\", \"green\"][i]]*subset_value, alpha = 0.075, dashes = False, legend = False)\n",
    "    else:\n",
    "        sns.lineplot(data = final_clusters[i].T.values, ax = ax, palette=[[\"red\", \"blue\", \"green\"][i]]*final_clusters[i].shape[0], alpha = 0.075, dashes = False, legend = False)\n",
    "    # sns.lineplot(data = f.values, ax = ax, color = [\"red\", \"blue\", \"green\"][i], linewidth = 4)\n",
    "    sns.lineplot(data = pd.DataFrame(f), x = \"sample\", y = 0,  ax = ax, color = \"black\", linewidth = 9)\n",
    "    sns.lineplot(data = pd.DataFrame(f), x = \"sample\", y = 0,  ax = ax, color = [\"red\", \"blue\", \"green\"][i], linewidth = 7)\n",
    "\n",
    "    \n",
    "\n",
    "# Creating x axis minor ticks and extracting locations for vspan; creating major tick labels\n",
    "major_ticks = []\n",
    "major_tick_labels = []\n",
    "minor_ticks = []\n",
    "minor_tick_labels = []\n",
    "time_point = \"\"\n",
    "x_ticks_loc = ax.get_xticks()\n",
    "vspan_counter = 0\n",
    "vspan_vec = []\n",
    "for i, column in enumerate(final_clusters[0].columns):\n",
    "    # Major ticks: timepoint\n",
    "    new_time_point = \"\\n\\n\" +column[1] + \",\\n\" + column[2]\n",
    "    if (time_point != new_time_point) & (i != len(final_clusters[0].columns) - 1):\n",
    "        time_point = new_time_point\n",
    "        # major_ticks.append(x_ticks_loc[i])\n",
    "        major_tick_labels.append(time_point)\n",
    "\n",
    "        # add vspan\n",
    "        vspan_counter += 1\n",
    "\n",
    "        if vspan_counter == 1:\n",
    "            xmin = 0\n",
    "        else: \n",
    "            xmax = x_ticks_loc[i] - 0.5\n",
    "            vspan_vec.append([xmin, xmax])\n",
    "            xmin = xmax\n",
    "    elif (time_point != new_time_point) & (i == len(final_clusters[0].columns) - 1):  \n",
    "        time_point = new_time_point    \n",
    "        major_tick_labels.append(time_point)\n",
    "        \n",
    "        xmax = x_ticks_loc[i] - 0.5\n",
    "        vspan_vec.append([xmin, xmax])\n",
    "        xmin = xmax\n",
    "        xmax = x_ticks_loc[i]\n",
    "        vspan_vec.append([xmin, xmax])\n",
    "    elif (i == len(final_clusters[0].columns) - 1):\n",
    "        xmax = x_ticks_loc[i]\n",
    "        vspan_vec.append([xmin, xmax])\n",
    "\n",
    "    # Minor ticks: sample type\n",
    "    sample_type = column[0]\n",
    "    minor_ticks.append(x_ticks_loc[i])\n",
    "    minor_tick_labels.append(sample_type)\n",
    "\n",
    "# adding vspan and creating minor ticks\n",
    "for i,v in enumerate(vspan_vec):\n",
    "    # adding vspan\n",
    "    if i % 2 == 1:\n",
    "        ax.axvspan(v[0],v[1],alpha=.2,color='grey') \n",
    "    \n",
    "    if (i == 0) & (v[1] == 0.5):\n",
    "        major_ticks.append(0)\n",
    "    elif (i == len(vspan_vec) - 1):\n",
    "        major_ticks.append(np.mean([v[0] + 0.5, v[1]]))\n",
    "    else:\n",
    "        major_ticks.append(np.mean(v))\n",
    "\n",
    "\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticklabels(major_tick_labels)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_xticklabels(minor_tick_labels, minor=True)\n",
    "\n",
    "ax.xaxis.remove_overlapping_locs = False\n",
    "\n",
    "plt.tick_params(axis='x',which='major',bottom=False,left=False,top=False) \n",
    "\n",
    "# Titles\n",
    "\n",
    "ax.set_title(\"%s%s%s%s\" % (\"Strains of \", species, \" in subject \", subject_id), size = 20)\n",
    "ax.set_xlabel(\"Sample\", size = 20)\n",
    "ax.set_ylabel(\"Frequency\", size = 20)\n",
    "\n",
    "        \n",
    "# Legend\n",
    "\n",
    "legend_elements = []\n",
    "\n",
    "for i in np.arange(len(final_f)):\n",
    "    legend_elements.append(Line2D([0], [0], color=[\"red\", \"blue\", \"green\"][i], lw=8, label='Strain %s' % (i+1)))\n",
    "\n",
    "ax.legend(handles=legend_elements, fontsize = 16)\n",
    "\n",
    "\n",
    "# Saving\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# out_file = \"%s%s%s%s%s\" % (strain_phasing_figures_dir, species, \"_subject_\", subject_id, \"_StrainFreq.png\")\n",
    "# fig.savefig(out_file, dpi = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a466fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"%s%s%s%s%s\" % (strain_phasing_figures_dir, species, \"_subject_\", subject_id, \"_StrainFreq_ReadSupport251104.png\")\n",
    "fig.savefig(out_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484aea50",
   "metadata": {},
   "source": [
    "## Reclustering around a new "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b9f7e",
   "metadata": {},
   "source": [
    "## Bootstrapping support and saving file for R plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99695608",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strains = True\n",
    "bootstrap_ci = True\n",
    "boostrap_k = 100\n",
    "bootstrap_N = 1000\n",
    "if len(final_clusters) > 0:\n",
    "    for i in np.arange(len(final_f)): \n",
    "        if i == 0:\n",
    "            final_f_all_strains = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "            final_f_all_strains['species'] = [species]*len(final_f_all_strains)\n",
    "            final_f_all_strains['strain'] = [i + 1]*len(final_f_all_strains)\n",
    "            final_f_all_strains['subject'] = final_f_all_strains['sample'].apply(lambda x: sample_metadata_map[x][0])\n",
    "            final_f_all_strains['sample_type'] = final_f_all_strains['sample'].apply(lambda x: sample_metadata_map[x][9])\n",
    "            final_f_all_strains['tissue'] = final_f_all_strains['sample'].apply(lambda x: sample_metadata_map[x][10])\n",
    "            final_f_all_strains = final_f_all_strains[['sample_type', \n",
    "                                                       'date', \n",
    "                                                       'time', \n",
    "                                                       'sample', \n",
    "                                                       'freq', \n",
    "                                                       'species', \n",
    "                                                       'strain',\n",
    "                                                       'subject', \n",
    "                                                       'tissue']]\n",
    "            iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().loc[:,['sample', 0.75]]\n",
    "            iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().loc[:,['sample', 0.25]]\n",
    "            final_f_all_strains = final_f_all_strains.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "            if bootstrap_ci:\n",
    "                upper_ci_vec = []\n",
    "                lower_ci_vec = []\n",
    "                \n",
    "                for sample_i in range(final_clusters[i].shape[1]):\n",
    "                    \n",
    "                    sample_name = final_clusters[i].iloc[:,sample_i].name[3]\n",
    "                    snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                    mean_freq_vec = []\n",
    "                    for n in range(bootstrap_N):\n",
    "                        sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                        mean_freq = np.mean(sampled_snv_freqs)\n",
    "                        mean_freq_vec.append(mean_freq)\n",
    "                    upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                    lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                    upper_ci_vec.append(upper_ci)\n",
    "                    lower_ci_vec.append(lower_ci)\n",
    "                    \n",
    "                \n",
    "\n",
    "        else:\n",
    "            final_f_all_strains_temp = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "            final_f_all_strains_temp['species'] = [species]*len(final_f_all_strains_temp)\n",
    "            final_f_all_strains_temp['strain'] = [i + 1]*len(final_f_all_strains_temp)\n",
    "            final_f_all_strains_temp['subject'] = final_f_all_strains_temp['sample'].apply(lambda x: sample_metadata_map[x][0])\n",
    "            final_f_all_strains_temp['sample_type'] = final_f_all_strains_temp['sample'].apply(lambda x: sample_metadata_map[x][9])\n",
    "            final_f_all_strains_temp['tissue'] = final_f_all_strains_temp['sample'].apply(lambda x: sample_metadata_map[x][10])\n",
    "            final_f_all_strains_temp = final_f_all_strains_temp[['sample_type', \n",
    "                                                                 'date', \n",
    "                                                                 'time', \n",
    "                                                                 'sample', \n",
    "                                                                 'freq', \n",
    "                                                                 'species', \n",
    "                                                                 'strain',\n",
    "                                                                 'subject', \n",
    "                                                                 'tissue']]\n",
    "            iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().reset_index().loc[:,['sample', 0.75]]\n",
    "            iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().reset_index().loc[:,['sample', 0.25]]\n",
    "            final_f_all_strains_temp = final_f_all_strains_temp.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "            final_f_all_strains = pd.concat([final_f_all_strains, final_f_all_strains_temp], ignore_index=True)\n",
    "            \n",
    "            if bootstrap_ci:\n",
    "                \n",
    "                for sample_i in range(final_clusters[i].shape[1]): #if we did not infer multiple strains, but there may have been multiple strains in the inoculum, just calculate CI for inoculum\n",
    "             \n",
    "                    snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                    mean_freq_vec = []\n",
    "\n",
    "                    for n in range(bootstrap_N):\n",
    "                        sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                        mean_freq = np.mean(sampled_snv_freqs)\n",
    "                        mean_freq_vec.append(mean_freq)\n",
    "                    upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                    lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                    upper_ci_vec.append(upper_ci)\n",
    "                    lower_ci_vec.append(lower_ci)\n",
    "            \n",
    "            \n",
    "        final_f_all_strains['upper_ci'] = upper_ci_vec\n",
    "        final_f_all_strains['lower_ci'] = lower_ci_vec\n",
    "    #Renaming\n",
    "    final_f_all_strains = final_f_all_strains.rename(columns = {0.25: \"quantile_25\", 0.75: \"quantile_75\"})\n",
    "    \n",
    "    #Saving \n",
    "    \n",
    "    output_file = \"%sstrain_phasing/strain_clusters/%s/%s_subject_%s_strain_frequency.csv\" % (config.project_directory, species, species, subject_id)\n",
    "    final_f_all_strains.to_csv(output_file, sep = \"\\t\", index = False)\n",
    "else:\n",
    "    sys.stderr.write(\"Only one strain detected\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c493fd",
   "metadata": {},
   "source": [
    "# For loop through species list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.strain_phasing_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "min_cluster_size = 1000\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "min_cluster_fraction = 1/10\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more   \n",
    "max_d = 3.5\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 10 \n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "poly_cov_frac = 1/5 #\n",
    "\n",
    "## Number of clusters to calculate\n",
    "n_clusters = 100\n",
    "\n",
    "#Minimum number of snvs per sample\n",
    "min_num_snvs_per_sample = 100\n",
    "\n",
    "\n",
    "# Load species list\n",
    "species_path = \"%s%s\" % (config.analysis_directory, \"metadata/species_snps.txt\")\n",
    "with open(species_path, 'r') as file:\n",
    "    species_list = [line.strip() for line in file]\n",
    "\n",
    "# Load subject list\n",
    "sample_metadata_map = parse_sample_metadata_map()\n",
    "subject_sample_map = parse_subject_sample_map()\n",
    "subjects = subject_sample_map.keys()\n",
    "\n",
    "debug_counter = 0\n",
    "\n",
    "subjects = [\"9\"]\n",
    "species_list = [\"Blautia_wexlerae_56130\"]\n",
    "\n",
    "for subject_i,subject_id in enumerate(list(subjects)):\n",
    "    sys.stderr.write(\"Processing subject %s (%s / %s)\\n\" % (subject_id, subject_i + 1, len(subjects)))\n",
    "    samples_of_interest = list(subject_sample_map[subject_id].keys())\n",
    "    for species_i,species in enumerate(species_list):\n",
    "        sys.stderr.write(\"Processing %s (%s / %s)\\n\" % (species, species_i + 1, len(species_list)))\n",
    "        # Defining directories\n",
    "\n",
    "        strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "\n",
    "        #Raw cluster\n",
    "        # raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "        raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "        species_raw_cluster_dir = \"%s%s/\" % (raw_cluster_path, species)\n",
    "\n",
    "        # Load Fs\n",
    "        Fs,Ass,Dss = return_FAD(species, min_coverage=min_coverage, \n",
    "                                min_sample_coverage=min_sample_coverage, \n",
    "                                poly_cov_frac = poly_cov_frac, \n",
    "                                calculate_poly_cov_frac=False, \n",
    "                                read_support = False, \n",
    "                                subject_id=subject_id) \n",
    "        \n",
    "        # Filter out samplers without adequate read coverage\n",
    "        sample_with_adequate_snv_count = ~((~np.isnan(Fs)).sum() < min_num_snvs_per_sample)\n",
    "\n",
    "        Fs = Fs.loc[:,sample_with_adequate_snv_count]\n",
    "        Ass = Ass.loc[:,sample_with_adequate_snv_count]\n",
    "        Dss = Dss.loc[:,sample_with_adequate_snv_count]\n",
    "\n",
    "        # Read in raw cluster file\n",
    "        species_raw_cluster_path = \"%s%s%s%s%s\" % (species_raw_cluster_dir, species, \"_subject_\", subject_id, \"_RawCluster.pckl\")\n",
    "\n",
    "        if not os.path.exists(species_raw_cluster_path):\n",
    "            sys.stderr.write(f\"\\tFile not found: {species_raw_cluster_path}, skipping species.\\n\")\n",
    "            continue  # Skip to the next iteration\n",
    "        else:\n",
    "            final_clusters = pd.read_pickle(species_raw_cluster_path)\n",
    "\n",
    "        # Polarizing clusters\n",
    "        if len(final_clusters) == 0:\n",
    "            sys.stderr.write(\"\\tNo strains detected, skipping species.\\n\")\n",
    "            continue\n",
    "        \n",
    "        ## If only a single cluster is detected, add a second \"cluster\" which is simply 1 minus the allele frequencies\n",
    "        ## in the first cluster\n",
    "        ## aids in visualization for people not familiar with this kind of clustering\n",
    "        if len(final_clusters) == 1:\n",
    "            final_clusters.append(1-final_clusters[0])\n",
    "\n",
    "        ## add cluster centroids\n",
    "        final_f = []\n",
    "        for cluster in final_clusters:\n",
    "            final_f.append(cluster.mean())\n",
    "        df_final_f = pd.DataFrame(final_f)\n",
    "\n",
    "        ## now, polarize clusters so that the sum of squareds of the centroids to 1 is minimized\n",
    "        ## the idea here is that accurate strain frequencies should sum to 1\n",
    "        polarize = True\n",
    "\n",
    "        pol_d2 = {}\n",
    "\n",
    "        for i in range(df_final_f.shape[0]):\n",
    "            df_final_f_temp = df_final_f.copy() #Makes a copy of the centroids\n",
    "            df_final_f_temp.iloc[i] = 1 - df_final_f_temp.iloc[i] #gets the polarized version of ONE of the centroids.\n",
    "            pol_d2[i] =  ((1 - df_final_f_temp.sum())**2).sum()   #Get the across centroids for all samples (should be close to 1), \n",
    "                                                                        #subtract this from 1, and square. Sum all those values\n",
    "                                                                        #Ideally, this value is really close to 0. \n",
    "                                                                        #Add this value to the dictionary.\n",
    "\n",
    "            pol_d2 = pd.Series(pol_d2)                                #Make the dictionary a series \n",
    "\n",
    "            if pol_d2.min() < ((1 - df_final_f.sum())**2).sum(): #If any of the above repolarizations actually made the overall sum of centroids closer to 1, repolarize.\n",
    "                clus_to_re_pol = pol_d2.idxmin()\n",
    "                final_f[clus_to_re_pol] = 1 - final_f[clus_to_re_pol]\n",
    "                final_clusters[clus_to_re_pol] = 1 - final_clusters[clus_to_re_pol]\n",
    "                df_final_f = pd.DataFrame(final_f)  \n",
    "\n",
    "        # Filtering out samples without adequate snvs after polarization step\n",
    "        good_indices = []\n",
    "\n",
    "        for i,cluster in enumerate(final_clusters):\n",
    "            if i == 0:\n",
    "                good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "            else:\n",
    "                new_good_samples = (len(final_clusters[i]) - np.isnan(final_clusters[i]).sum(axis = 0) > min_num_snvs_per_sample).values\n",
    "                good_samples = good_samples & new_good_samples\n",
    "\n",
    "        if sum(good_samples == False) == len(good_samples): # if no sample is \"good\" across all clusters, then skip to the next species\n",
    "            sys.stderr.write(\"\\tNo good samples after filter, skipping species.\\n\")\n",
    "            continue\n",
    "\n",
    "        for i,cluster in enumerate(final_clusters): \n",
    "            final_clusters[i] = final_clusters[i].T.loc[good_samples].T\n",
    "            final_f[i] = final_f[i].T.loc[good_samples]\n",
    "\n",
    "        Fs = Fs.T.loc[good_samples].T\n",
    "        Ass = Ass.T.loc[good_samples].T\n",
    "        Dss = Dss.T.loc[good_samples].T\n",
    "\n",
    "        # Filtering out columsn that have only na (redundant)\n",
    "        #Filter all all na columns if there are any - THis is redundant\n",
    "        if len(final_clusters) > 0:\n",
    "            for i,cluster in enumerate(final_clusters):\n",
    "                if i == 0:\n",
    "                    mask = ~np.isnan(cluster).all(axis = 0)\n",
    "                final_clusters[i] = cluster.loc[:,mask]\n",
    "                final_f[i] = final_f[i][mask]\n",
    "            # Fs = Fs.loc[:,mask]\n",
    "            Fs = Fs.loc[:, Fs.columns[mask]]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Bootstrapping CI and outputting dataframe\n",
    "        all_strains = True\n",
    "        bootstrap_ci = True\n",
    "        boostrap_k = 100\n",
    "        bootstrap_N = 1000\n",
    "        if len(final_clusters) > 0:\n",
    "            for i in np.arange(len(final_f)): \n",
    "                if i == 0:\n",
    "                    final_f_all_strains = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "                    final_f_all_strains['species'] = [species]*len(final_f_all_strains)\n",
    "                    final_f_all_strains['strain'] = [i + 1]*len(final_f_all_strains)\n",
    "                    final_f_all_strains['subject'] = final_f_all_strains['sample'].apply(lambda x: sample_metadata_map[x][0])\n",
    "                    final_f_all_strains['sample_type'] = final_f_all_strains['sample'].apply(lambda x: sample_metadata_map[x][9])\n",
    "                    final_f_all_strains['tissue'] = final_f_all_strains['sample'].apply(lambda x: sample_metadata_map[x][10])\n",
    "                    final_f_all_strains = final_f_all_strains[['sample_type', \n",
    "                                                            'date', \n",
    "                                                            'time', \n",
    "                                                            'sample', \n",
    "                                                            'freq', \n",
    "                                                            'species', \n",
    "                                                            'strain',\n",
    "                                                            'subject', \n",
    "                                                            'tissue']]\n",
    "                    iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().loc[:,['sample', 0.75]]\n",
    "                    iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().loc[:,['sample', 0.25]]\n",
    "                    final_f_all_strains = final_f_all_strains.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "                    if bootstrap_ci:\n",
    "                        upper_ci_vec = []\n",
    "                        lower_ci_vec = []\n",
    "                        \n",
    "                        for sample_i in range(final_clusters[i].shape[1]):\n",
    "                            \n",
    "                            sample_name = final_clusters[i].iloc[:,sample_i].name[3]\n",
    "                            snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                            mean_freq_vec = []\n",
    "                            for n in range(bootstrap_N):\n",
    "                                sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                                mean_freq = np.mean(sampled_snv_freqs)\n",
    "                                mean_freq_vec.append(mean_freq)\n",
    "                            upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                            lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                            upper_ci_vec.append(upper_ci)\n",
    "                            lower_ci_vec.append(lower_ci)\n",
    "                            \n",
    "                        \n",
    "\n",
    "                else:\n",
    "                    final_f_all_strains_temp = pd.DataFrame(final_f[i]).reset_index().rename(columns = {0: \"freq\"})\n",
    "                    final_f_all_strains_temp['species'] = [species]*len(final_f_all_strains_temp)\n",
    "                    final_f_all_strains_temp['strain'] = [i + 1]*len(final_f_all_strains_temp)\n",
    "                    final_f_all_strains_temp['subject'] = final_f_all_strains_temp['sample'].apply(lambda x: sample_metadata_map[x][0])\n",
    "                    final_f_all_strains_temp['sample_type'] = final_f_all_strains_temp['sample'].apply(lambda x: sample_metadata_map[x][9])\n",
    "                    final_f_all_strains_temp['tissue'] = final_f_all_strains_temp['sample'].apply(lambda x: sample_metadata_map[x][10])\n",
    "                    final_f_all_strains_temp = final_f_all_strains_temp[['sample_type', \n",
    "                                                                        'date', \n",
    "                                                                        'time', \n",
    "                                                                        'sample', \n",
    "                                                                        'freq', \n",
    "                                                                        'species', \n",
    "                                                                        'strain',\n",
    "                                                                        'subject', \n",
    "                                                                        'tissue']]\n",
    "                    iqr_75 = pd.DataFrame(final_clusters[i].quantile(0.75)).reset_index().reset_index().loc[:,['sample', 0.75]]\n",
    "                    iqr_25 = pd.DataFrame(final_clusters[i].quantile(0.25)).reset_index().reset_index().loc[:,['sample', 0.25]]\n",
    "                    final_f_all_strains_temp = final_f_all_strains_temp.merge(iqr_25, on = [\"sample\"]).merge(iqr_75, on = [\"sample\"])\n",
    "                    final_f_all_strains = pd.concat([final_f_all_strains, final_f_all_strains_temp], ignore_index=True)\n",
    "                    \n",
    "                    if bootstrap_ci:\n",
    "                        \n",
    "                        for sample_i in range(final_clusters[i].shape[1]): #if we did not infer multiple strains, but there may have been multiple strains in the inoculum, just calculate CI for inoculum\n",
    "                    \n",
    "                            snv_freqs = final_clusters[i].iloc[:,sample_i].dropna().to_list()\n",
    "                            mean_freq_vec = []\n",
    "\n",
    "                            for n in range(bootstrap_N):\n",
    "                                sampled_snv_freqs = choices(snv_freqs, k = boostrap_k)\n",
    "                                mean_freq = np.mean(sampled_snv_freqs)\n",
    "                                mean_freq_vec.append(mean_freq)\n",
    "                            upper_ci = np.quantile(mean_freq_vec, 0.975)\n",
    "                            lower_ci = np.quantile(mean_freq_vec, 0.025)\n",
    "                            upper_ci_vec.append(upper_ci)\n",
    "                            lower_ci_vec.append(lower_ci)\n",
    "                    \n",
    "                    \n",
    "                final_f_all_strains['upper_ci'] = upper_ci_vec\n",
    "                final_f_all_strains['lower_ci'] = lower_ci_vec\n",
    "            #Renaming\n",
    "            final_f_all_strains = final_f_all_strains.rename(columns = {0.25: \"quantile_25\", 0.75: \"quantile_75\"})\n",
    "            \n",
    "            #Saving \n",
    "            \n",
    "            output_file = \"%sstrain_phasing/strain_clusters/%s/%s_subject_%s_strain_frequency.csv\" % (config.project_directory, species, species, subject_id)\n",
    "            final_f_all_strains.to_csv(output_file, sep = \"\\t\", index = False)\n",
    "            sys.stderr.write(\"\\t%s done!\\n\\n\" % (species))\n",
    "        else:\n",
    "            sys.stderr.write(\"\\tOnly one strain detected\\n\\n\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f_all_strains.groupby(['sample'])['freq'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
