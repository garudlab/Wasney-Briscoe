{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('text', usetex=True)\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}') \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid.inset_locator import (inset_axes, InsetPosition,\n",
    "                                                  mark_inset)\n",
    "import scipy.stats\n",
    "import figure_utils as fu\n",
    "from return_gene_descriptions import return_gene_descriptions\n",
    "\n",
    "from numba import njit \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import config\n",
    "import numpy\n",
    "import random as rand\n",
    "\n",
    "from random import randint,sample\n",
    "from math import log\n",
    "\n",
    "import sys\n",
    "import os \n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid.inset_locator import (inset_axes, InsetPosition,\n",
    "                                                  mark_inset)\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "hap_cmap = ListedColormap(['grey', 'red', 'black', 'black','blue'], 'indexed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## useful utility for quickly returning the upper triangle of a 2-d array as a 1-d array\n",
    "def take_triu(df):\n",
    "    \n",
    "    N = df.shape[0]\n",
    "    p=np.triu_indices(N,k=1)\n",
    "    \n",
    "    return(df[p])\n",
    "\n",
    "## returns annotations \n",
    "def read_sites(species):\n",
    "    \n",
    "    snps_directory = \"/u/project/ngarud/Garud_lab/HumanizedMouse/merged_midas_output/snps\"\n",
    "    \n",
    "    df_sites = pd.read_csv(f\"{snps_directory}/{species}/snps_info.txt.bz2\",sep=\"\\t\",index_col=0,na_values=\"NaN\")\n",
    "   \n",
    "    df_sites[\"contig\"] = [d.split(\"|\")[0] for d in df_sites.index]\n",
    "    df_sites.index = [d.split(\"|\")[1] for d in df_sites.index]\n",
    "    \n",
    "    df_sites[\"gene_id\"] = df_sites[\"gene_id\"].fillna(\"non coding\")\n",
    "    gene_ids = df_sites[\"gene_id\"].values\n",
    "\n",
    "    gene_breaks = [0]\n",
    "\n",
    "    gc = gene_ids[0]\n",
    "    unq_genes = [gc]\n",
    "    unq_cont = [df_sites[\"contig\"][0]]\n",
    "\n",
    "    for i,g in enumerate(gene_ids):\n",
    "        if g is not gc:\n",
    "            gene_breaks.append(i)\n",
    "            gc = g\n",
    "            unq_genes.append(gc)\n",
    "            unq_cont.append(df_sites[\"contig\"][i])\n",
    "\n",
    "    gene_breaks = np.array(gene_breaks)       \n",
    "    gene_lengths = gene_breaks[1:] - gene_breaks[:-1] \n",
    "\n",
    "    df_sites.index.set_names(\"site_pos\",inplace=True)\n",
    "    \n",
    "    df_sites.set_index('gene_id', append=True, inplace=True)\n",
    "    df_sites.set_index('contig', append=True, inplace=True)\n",
    "\n",
    "    df_sites = df_sites.reorder_levels([\"contig\",'gene_id', 'site_pos'])\n",
    "    \n",
    "    level_to_change = 2\n",
    "    df_sites.index = df_sites.index.set_levels(df_sites.index.levels[level_to_change].astype(int), level=level_to_change)\n",
    "\n",
    "    return(df_sites)\n",
    "\n",
    "def read_haplotypes(species,good_samples=None,clade_control=False):\n",
    "    \n",
    "    output_dir = \"/u/scratch/r/rwolff/LD/HMP\"\n",
    "    haplotype_directory = \"%s/haplotypes\" % output_dir\n",
    "    \n",
    "    hap_files = glob.glob(f\"{haplotype_directory}/{species}/*_haplotypes.csv\")\n",
    "    \n",
    "    idx_cols = [\"contig\",\"gene_id\",\"site_pos\",\"site_type\"]\n",
    "    all_haps = []\n",
    "\n",
    "    for hap in hap_files:\n",
    "        \n",
    "        if good_samples is not None and clade_control==False:\n",
    "            df = pd.read_csv(hap,index_col=[0,1,2,3],usecols = idx_cols + list(good_samples))\n",
    "            \n",
    "        elif good_samples is None and clade_control != False:\n",
    "            df = pd.read_csv(hap,index_col=[0,1,2,3],usecols = idx_cols + list(clade_utils.load_largest_clade(species)))\n",
    "                  \n",
    "        else:\n",
    "            df = pd.read_csv(hap,index_col=[0,1,2,3])\n",
    "\n",
    "        all_haps.append(df)\n",
    "\n",
    "    df = pd.concat(all_haps)\n",
    "    \n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmap(n, name='Set3_r'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use of numba/njit below *substantially* increases computational efficiency\n",
    "\n",
    "## calculate distances for forward polarization \n",
    "@njit\n",
    "def D_mat_fun1(num,F,D,D_mat):   \n",
    "\n",
    "    for k in range(num - 1):\n",
    "        \n",
    "        O = np.zeros(num)\n",
    "        \n",
    "        di = D[k]\n",
    "        fi = F[k]\n",
    "        \n",
    "        for i in range(num - k - 1):\n",
    "\n",
    "            j = i + k + 1\n",
    "\n",
    "            fj = F[j]\n",
    "            dj = D[j]\n",
    "\n",
    "            O[j] = 2*np.nanmean((di + dj)*((fi - fj)**2)/((fi + fj)*(1 - fi + 1 - fj)))        \n",
    "        \n",
    "        D_mat[k] = O\n",
    "    \n",
    "    return D_mat\n",
    "\n",
    "## calculate distances for reverse polarization\n",
    "@njit\n",
    "def D_mat_fun2(num,F,D,D_mat_in):   \n",
    "\n",
    "    for k in range(num - 1):\n",
    "        \n",
    "        O = np.zeros(num)       \n",
    "        di = D[k]\n",
    "        \n",
    "        fi = 1-F[k]\n",
    "        \n",
    "        for i in range(num - k - 1):\n",
    "\n",
    "            j = i + k + 1\n",
    "            \n",
    "            fj = F[j]\n",
    "            dj = D[j]\n",
    "\n",
    "            O[j] = 2*np.nanmean((di + dj)*((fi - fj)**2)/((fi + fj)*(1 - fi + 1 - fj)))        \n",
    "        \n",
    "        D_mat_in[k] = O\n",
    "    \n",
    "    return D_mat_in\n",
    "\n",
    "def return_clus(D_mat_close,Fs_sub):\n",
    "    D_mat_close_sorted_sum = D_mat_close.sum().sort_values()\n",
    "    desired_idx = D_mat_close_sorted_sum.index[-1]\n",
    "    clus_idxs = D_mat_close.loc[D_mat_close[desired_idx]].index\n",
    "    \n",
    "    ### only return indices which are co-clustered w/ at least .25 of other points\n",
    "    idxtrue = (D_mat_close.loc[clus_idxs,clus_idxs].T.mean() > .25)\n",
    "    idxtrue = idxtrue[idxtrue].index\n",
    "    clus_idxs = idxtrue\n",
    "    clus = Fs_sub.loc[clus_idxs]\n",
    "     \n",
    "    return clus,clus_idxs\n",
    "\n",
    "def drop_clus_idxs(D_mat_close,clus_idxs):\n",
    "    D_mat_close_out = D_mat_close.drop(clus_idxs).drop(clus_idxs,axis=1)\n",
    "    return D_mat_close_out\n",
    "\n",
    "def polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2):\n",
    "    \n",
    "    ## polarize whole cluster based on polarization of first cluster element\n",
    "    clus_to_pol = clus_idxs[np.where(D_mat_1.loc[clus_idxs[:1],clus_idxs] >= D_mat_2.loc[clus_idxs[:1],clus_idxs])[1]]\n",
    "    pol_clus = 1 - clus.loc[clus_to_pol]\n",
    "    clus_non_pol = clus_idxs[np.where(D_mat_1.loc[clus_idxs[:1],clus_idxs] < D_mat_2.loc[clus_idxs[:1],clus_idxs])[1]]\n",
    "    non_pol_plus = clus.loc[clus_non_pol]\n",
    "    clus_pol = pd.concat([pol_clus,non_pol_plus],ignore_index=True)\n",
    "    \n",
    "    return(clus_pol)\n",
    "\n",
    "@njit\n",
    "def symmetrize(D_mat):\n",
    "    for i in range(D_mat.shape[0]-1):\n",
    "        for j in range(i,D_mat.shape[0]):\n",
    "            D_mat[j][i] = D_mat[i][j]\n",
    "    return(D_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## non-looped version of distance calculation\n",
    "def calc_dis(di,dj,fi,fj):\n",
    "    \n",
    "    return(2*np.nanmean((di + dj)*((fi - fj)**2)/((fi + fj)*(1 - fi + 1 - fj))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_FAD(species,min_coverage=10,min_sample_coverage=5,poly_cov_frac=1/5):\n",
    "\n",
    "    snp_alignment = pd.read_pickle(\"%s/%s/%s.strainfinder.p\" %  (strainfinder_dir ,species, species))\n",
    "    samples = pd.read_pickle(\"%s/%s/%s.strainfinder.samples.p\" % (strainfinder_dir ,species, species))\n",
    "    samples = [s.decode(\"utf-8\") for s in samples]\n",
    "    snp_locations = pd.read_pickle(\"%s/%s/%s.strainfinder.locations.p\" % (strainfinder_dir,species,species))\n",
    "\n",
    "    cluster_As = []\n",
    "    cluster_Ds = []\n",
    "    for snp_idx in range(0,snp_alignment.shape[1]):\n",
    "\n",
    "        Ds = snp_alignment[:,snp_idx,:].sum(axis=1)\n",
    "        As = snp_alignment[:,snp_idx,0]\n",
    "        As = np.reshape(As, (1,len(As)))\n",
    "        Ds = np.reshape(Ds, (1,len(Ds)))\n",
    "\n",
    "        cluster_As.append(As[0])\n",
    "        cluster_Ds.append(Ds[0])\n",
    "\n",
    "    cluster_As = np.array(cluster_As)\n",
    "    cluster_Ds = np.array(cluster_Ds)\n",
    "\n",
    "    As = pd.DataFrame(cluster_As,columns=samples,index=snp_locations)\n",
    "    Ds = pd.DataFrame(cluster_Ds,columns=samples,index=snp_locations)\n",
    "\n",
    "    F = As/Ds\n",
    "\n",
    "    Ass = As\n",
    "    Dss = Ds.loc[Ass.index]\n",
    "    Ass = Ass.mask(Dss < min_coverage)\n",
    "    \n",
    "    Fs = Ass/Dss\n",
    "\n",
    "    samps = Dss.mean() > min_sample_coverage\n",
    "    samps = samps[samps].index\n",
    "    Dss = Dss[samps]\n",
    "    Ass = Ass[samps]\n",
    "    Fs = Fs[samps]\n",
    "\n",
    "    Fs = Fs.loc[Fs.mask(Ass == 0).mask(Dss < min_coverage).notna().T.sum() > int(Fs.shape[1]*poly_cov_frac)]\n",
    "\n",
    "    Ass = Ass.loc[Fs.index]\n",
    "    Dss = Dss.loc[Fs.index]\n",
    "\n",
    "    mnum = [f[:2] for f in Fs.columns]\n",
    "    msite = [f[2:].split(\"_\")[0][:-1] for f in Fs.columns]\n",
    "    mdiet = [f[2:].split(\"_\")[0][-1] for f in Fs.columns]\n",
    "    Fs = Fs.T\n",
    "    Fs[\"mouse_number\"] = mnum\n",
    "    Fs.set_index('mouse_number', append=True, inplace=True)\n",
    "    Fs[\"region\"] = msite\n",
    "    Fs.set_index('region', append=True, inplace=True)\n",
    "    Fs[\"diet\"] = mdiet\n",
    "    Fs.set_index('diet', append=True, inplace=True)\n",
    "\n",
    "    Fs.index.names = [\"sample\",\"mouse_number\",\"region\",\"diet\"]\n",
    "    Fs = Fs.reorder_levels([\"mouse_number\",\"region\",\"diet\",\"sample\"])\n",
    "    Fs = Fs.T\n",
    "\n",
    "    Ass.columns = Fs.columns\n",
    "    Dss.columns = Fs.columns\n",
    "\n",
    "    Fs = Fs.sort_index(level=\"mouse_number\",axis=1)\n",
    "    Dss = Dss.sort_index(level=\"mouse_number\",axis=1)\n",
    "    Ass = Ass.sort_index(level=\"mouse_number\",axis=1)\n",
    "\n",
    "    snv_idx = pd.MultiIndex.from_tuples(Fs.index,names=[\"contig\",\"site_pos\",\"ref/alt\"])\n",
    "    Fs.index = snv_idx\n",
    "    Fs = Fs.droplevel(\"ref/alt\")\n",
    "\n",
    "    Ass.index = Fs.index\n",
    "    Dss.index = Fs.index\n",
    "\n",
    "    Fs=Fs.sort_index(level=[\"contig\",'site_pos'])\n",
    "    Ass=Ass.sort_index(level=[\"contig\",'site_pos'])\n",
    "    Dss=Dss.sort_index(level=[\"contig\",'site_pos'])\n",
    "\n",
    "    C_list = np.unique(Fs.index.get_level_values(\"contig\"))\n",
    "\n",
    "    all_site_pos = []\n",
    "    offset = 0\n",
    "    for C in C_list:\n",
    "\n",
    "        all_site_pos.extend(Fs.loc[C].index + offset)\n",
    "\n",
    "        offset += Fs.loc[C].index[-1]\n",
    "\n",
    "    Fs['all_site_pos'] = all_site_pos\n",
    "    Fs.set_index('all_site_pos', append=True, inplace=True)\n",
    "    Ass['all_site_pos'] = all_site_pos\n",
    "    Ass.set_index('all_site_pos', append=True, inplace=True)\n",
    "    Dss['all_site_pos'] = all_site_pos\n",
    "    Dss.set_index('all_site_pos', append=True, inplace=True)\n",
    "\n",
    "    return(Fs,Ass,Dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defines an order for each level type\n",
    "## M1 --> M6\n",
    "## Upper gut --> lower gut\n",
    "## Control --> Guar gum\n",
    "## Co-housing treatment 1 --> co-housing treatment 3\n",
    "order_dict = {\"M1\":0,\"M2\":1,\"M3\":2,\"M4\":3,\"M5\":4,\"M6\":5,\n",
    "              'D': 0, 'J': 1, 'I': 2,\"Ce\":3,\"Co\":4,\n",
    "              \"C\":0,\"G\":1,\n",
    "              \"C1\":0,\"C2\":1,\"C3\":2} \n",
    "\n",
    "## function sorts our multiindex of frequencies according to whatever key we specify, e.g. mouse_number\n",
    "## while maintaining the order of subsequent levels according to order_dict\n",
    "def reorder_sort(df,first_idx,order_dict=order_dict):\n",
    "    \n",
    "    reorder=list(Fs.T.index.names)\n",
    "    reorder.remove(first_idx)\n",
    "    reorder.insert(0, first_idx)\n",
    "    return df.reorder_levels(reorder).sort_index(key=lambda x: x.map(order_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls /u/project/ngarud/michaelw/PaulAllen/humanized_mouse/strainfinder/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Clostridiales_bacterium_61057\"\n",
    "\n",
    "strainfinder_dir = \"/u/project/ngarud/michaelw/PaulAllen/humanized_mouse/strainfinder/input/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "## if we didn't cap max_num_snvs, then min_cluster_size would be O(10^4), based on the typical evolutionary\n",
    "## distance between strains\n",
    "min_cluster_size = 1000\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "## basically, the idea is that as the initial number of sites we pass in gets bigger, we want to incrwease the min_cluster_size\n",
    "## here, we say that 10% of all variable sites must be in a cluster in order for it to be considered a \"strain\"\n",
    "## this will largely be redundant w/ min_cluster_size, but adds some more functionality to play with\n",
    "min_cluster_fraction = 1/10\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "## should still give us the same strain trajectory \n",
    "## clustering 20k SNVs takes ~90 seconds. \n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more \n",
    "## similar, higher means less similar, to be in a cluster\n",
    "max_d = 3.5\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 10\n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "## NOTE: we may want to disaggregate coverage and polymorphic-ness so as to not lose evolutionary snvs\n",
    "## but for strain clustering purposes, I think we should focus on SNVs that are actually polymorphic\n",
    "## in a good number of samples\n",
    "poly_cov_frac = 1/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs,Ass,Dss = return_FAD(species, min_coverage=min_coverage, min_sample_coverage=min_sample_coverage, poly_cov_frac = poly_cov_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fss = Ass.values/(Dss.values + (Dss.values == 0))\n",
    "\n",
    "cluster_As = Ass.values\n",
    "cluster_Ds = Dss.values\n",
    "cluster_fs = cluster_As/(cluster_Ds + (cluster_Ds == 0))\n",
    "\n",
    "## for compatibility in case of threshold number of SNVs\n",
    "num = min(max_num_snvs,Fs.shape[0])\n",
    "\n",
    "i_list = Dss.T.mean().sort_values(ascending=False).index[:num]\n",
    "\n",
    "sys.stderr.write(\"Processing %s SNVs\" % num)\n",
    "\n",
    "## simply shuffles indices if no threshold is specified\n",
    "#i_list = sample(range(Fs.shape[0]),num)\n",
    "i_list_idx = Fs.loc[i_list].index\n",
    "\n",
    "Ass_sub = Ass.loc[i_list_idx]\n",
    "Dss_sub = Dss.loc[i_list_idx]\n",
    "Fs_sub = Fs.loc[i_list_idx]\n",
    "\n",
    "fss_sub = Ass_sub.values/(Dss_sub.values + (Dss_sub.values == 0))\n",
    "\n",
    "cluster_As_sub = Ass_sub.values\n",
    "cluster_Ds_sub = Dss_sub.values\n",
    "cluster_fs_sub = cluster_As_sub/(cluster_Ds_sub + (cluster_Ds_sub == 0))\n",
    "\n",
    "D_mat = np.zeros([num,num])\n",
    "D_mat_1 = D_mat_fun1(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "D_mat = np.zeros([num,num]) \n",
    "D_mat_2 = D_mat_fun2(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "\n",
    "D_mat = np.fmin(D_mat_1,D_mat_2)\n",
    "D_mat = symmetrize(D_mat)\n",
    "\n",
    "D_mat_1 = pd.DataFrame(D_mat_1,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "D_mat_2 = pd.DataFrame(D_mat_2,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "\n",
    "D_mat_close = pd.DataFrame(D_mat < max_d) \n",
    "\n",
    "D_mat_close.index = Fs_sub.index\n",
    "D_mat_close.columns = Fs_sub.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracts up to 100 clusters\n",
    "## in practice all SNVs should fall into one of a fairly small number of clusters\n",
    "## really should re-write this with a while loop but this works for now\n",
    "## the idea is that we exhaust all clusters—there should only be a small number of them ultimately\n",
    "\n",
    "all_clus_pol = []\n",
    "all_clus_idx = []\n",
    "all_clus_A = []\n",
    "all_clus_D = []\n",
    "\n",
    "all_clus_F = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        clus,clus_idxs = return_clus(D_mat_close,Fs_sub)\n",
    "        clus_pol = polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2)\n",
    "        clus_pol.index = clus_idxs\n",
    "        D_mat_close = drop_clus_idxs(D_mat_close,clus_idxs)\n",
    "        \n",
    "        if clus_pol.shape[0] > min_cluster_size and clus_pol.shape[0] > Fs.shape[0]*min_cluster_fraction:\n",
    "            \n",
    "            all_clus_D.append(Dss.loc[clus.index].mean().values)\n",
    "            all_clus_pol.append(clus_pol)\n",
    "            all_clus_A.append(clus_pol.mean()*all_clus_D[-1])\n",
    "            all_clus_F.append(clus_pol.mean())\n",
    "\n",
    "            print(clus_pol.shape[0])\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now, choosing a representative SNV from each cluster, and finding all other sites (not just limited to the 20k)\n",
    "## which are consistent w/ being linked to it\n",
    "\n",
    "final_clusters = []\n",
    "\n",
    "all_aligned_sites = []\n",
    "\n",
    "for i in range(len(all_clus_D)):\n",
    "    \n",
    "    sys.stderr.write(f'\\n\\nCluster {i+1}\\n')\n",
    "    ancD = all_clus_D[i]\n",
    "    ancF = all_clus_F[i]\n",
    "\n",
    "    dss = Dss.values\n",
    "    fss = Fs.values\n",
    "    \n",
    "    disAnc_forward = []\n",
    "    disAnc_backward = []\n",
    "\n",
    "    for i in range(Dss.shape[0]):\n",
    "        disAnc_forward.append(calc_dis(ancD,dss[i],ancF,fss[i]))\n",
    "        disAnc_backward.append(calc_dis(ancD,dss[i],ancF,1-fss[i]))\n",
    "        if i % 1000 == 0:\n",
    "            sys.stderr.write(f\"\\n\\t{np.around(100*i/Dss.shape[0],3)}% finished\")\n",
    "    \n",
    "    disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "    disAnc = np.array(disAnc)\n",
    "    aligned_sites = Fs.loc[disAnc < max_d].index\n",
    "    f_dist =  pd.DataFrame(np.array([disAnc_forward,disAnc_backward]).T,index=Fs.index)\n",
    "    pols = f_dist.T.idxmin() > 0\n",
    "    \n",
    "    aligned_sites = [a for a in aligned_sites if a not in all_aligned_sites]\n",
    "    \n",
    "    pols = pols.loc[aligned_sites]\n",
    "    re_polarize = pols.loc[pols].index\n",
    "    \n",
    "    all_aligned_sites.extend(aligned_sites)\n",
    "    \n",
    "    Fs_cluster = Fs.loc[aligned_sites]\n",
    "    \n",
    "    Fs_cluster.loc[re_polarize] = 1 - Fs_cluster.loc[re_polarize]\n",
    "        \n",
    "    final_clusters.append(Fs_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating polarized clusters\n",
    "\n",
    "Once clusters have been identified and internally polarized, they need to be polarized relative to one another. In the best case, the sum of strain frequencies will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If only a single cluster is detected, add a second \"cluster\" which is simply 1 minus the allele frequencies\n",
    "## in the first cluster\n",
    "## aids in visualization for people not familiar with this kind of clustering\n",
    "if len(final_clusters) == 1:\n",
    "    final_clusters.append(1-final_clusters[0])\n",
    "    \n",
    "\n",
    "## add cluster centroids\n",
    "final_f = []\n",
    "for cluster in final_clusters:\n",
    "    final_f.append(cluster.mean())\n",
    "df_final_f = pd.DataFrame(final_f)\n",
    "\n",
    "## now, polarize clusters so that the sum of squareds of the centroids to 1 is minimized\n",
    "## the idea here is that accurate strain frequencies should sum to 1\n",
    "polarize = True\n",
    "\n",
    "if polarize:\n",
    "\n",
    "    pol_d2 = {}\n",
    "\n",
    "    for i in range(df_final_f.shape[0]):\n",
    "        df_final_f_temp = df_final_f.copy()\n",
    "        df_final_f_temp.iloc[i] = 1 - df_final_f_temp.iloc[i]\n",
    "        pol_d2[i] =  ((1 - df_final_f_temp.sum())**2).sum()\n",
    "\n",
    "    pol_d2 = pd.Series(pol_d2)\n",
    "\n",
    "    if pol_d2.min() < ((1 - df_final_f.sum())**2).sum():\n",
    "        clus_to_re_pol = pol_d2.idxmin()\n",
    "        final_f[clus_to_re_pol] = 1 - final_f[clus_to_re_pol]\n",
    "        final_clusters[clus_to_re_pol] = 1 - final_clusters[clus_to_re_pol]\n",
    "        df_final_f = pd.DataFrame(final_f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot the chosen polarization of strains\n",
    "## sum of strain frequencies should be ~1 at all timepoints\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(pd.DataFrame(final_f).sum().values,zorder=10,lw=3)\n",
    "ax.set_ylim([.5,1.5])\n",
    "ax.axhline(1,color=\"k\",ls=\"--\")\n",
    "ax.set_ylabel(\"Sum of strain frequencies\",size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting strain frequencies\n",
    "\n",
    "Strain frequencies can be plotted using a main key (e.g. mouse_number) and a secondary key (e.g. region), yielding a two-level identification of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More ordering utilities\n",
    "\n",
    "mnum = list(set(Fs.T.index.get_level_values(\"mouse_number\")))\n",
    "msite = list(set(Fs.T.index.get_level_values(\"region\")))\n",
    "mdiet = list(set(Fs.T.index.get_level_values(\"diet\")))\n",
    "\n",
    "mnum_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"mouse_number\").index.get_level_values(\"mouse_number\") == m).ravel() for m in list(set(mnum))}\n",
    "\n",
    "msite_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"region\").index.get_level_values(\"region\") == m).ravel() for m in list(set(msite))}\n",
    "\n",
    "mdiet_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"diet\").index.get_level_values(\"diet\") == m).ravel() for m in list(set(mdiet))}\n",
    "\n",
    "all_sample_dics = {\"diet\":mdiet_sample_dic,\"region\":msite_sample_dic,\"mouse_number\":mnum_sample_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_sort = \"mouse_number\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "cmap = get_cmap(len(list(set(mnum))))\n",
    "cmap_clus = get_cmap(len(list(set(mnum))),name=\"Set3\")\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    \n",
    "    ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "    ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "    \n",
    "    ax.plot(ff.values,zorder=100,lw=6,color=cmap_clus(i),label=f\"Strain {i+1}\");\n",
    "    ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "    ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "        \n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff.index.get_level_values(secondary_key))\n",
    "\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "        \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    ax.axvspan(xmin - .1,xmax+.1,alpha=.2,color=cmap(i))\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)\n",
    "        ax.text(e, -0.1, second_xlabels[e], ha='center', clip_on=False,size=15)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "\n",
    "    ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels(labels);\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"SNV frequency\",size=20)\n",
    "ax.set_ylim([-0.05,1.05]);\n",
    "\n",
    "fig.legend(prop={\"size\":20});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"/u/project/ngarud/michaelw/PaulAllen/humanized_mouse/figures/strain_phasing/{species}_strains_minclustersize{min_cluster_size}.png\", facecolor='white', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot locations of SNVs and SFS's\n",
    "\n",
    "Plotting the physical location of SNVs can help to identify instances of HGT. Lots of SNVs which lie very close to one another are a good indication of possible HGT, though synteny issues of course persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True,figsize=(32,12))\n",
    "\n",
    "gs = fig.add_gridspec(2, 16)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, :-2])\n",
    "ax1_H = fig.add_subplot(gs[0, -2:],sharey=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, :-2])\n",
    "ax2_H = fig.add_subplot(gs[1, -2:],sharey=ax2)\n",
    "\n",
    "alpha=1\n",
    "B = np.linspace(0,1,30)\n",
    "\n",
    "\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "ax1_H.tick_params(axis='both', which='major', labelsize=0)\n",
    "ax2_H.tick_params(axis='both', which='major', labelsize=0)\n",
    "\n",
    "sample_1 = 1\n",
    "\n",
    "for k,s in enumerate(range(len(final_clusters))):\n",
    "    \n",
    "    ax1.scatter(final_clusters[s].index.get_level_values(\"all_site_pos\"),final_clusters[s].iloc[:,sample_1],alpha=alpha,color=cmap_clus(s),zorder=k)\n",
    "    ax1_H.hist(final_clusters[s].iloc[:,sample_1],density=True,alpha=.8,color=cmap_clus(s), orientation='horizontal',label=f\"Strain {s+1}\",bins=B)\n",
    "    \n",
    "ax1.set_ylim([-0.05,1.05])\n",
    "ax1.set_xticklabels([])\n",
    "\n",
    "## can zoom in on a specific region using these ll (lower limit), ul (upper limit), and off (offset) variables\n",
    "# ll = 53799\n",
    "# ul = 53799\n",
    "# off = 10000\n",
    "\n",
    "# ax1.set_xlim([ll-off,ul+off])\n",
    "# ax2.set_xlim([ll-off,ul+off])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_2 = 7\n",
    "\n",
    "for k,s in enumerate(range(len(final_clusters))):\n",
    "    ax2.scatter(final_clusters[s].index.get_level_values(\"all_site_pos\"),final_clusters[s].iloc[:,sample_2],alpha=alpha,color=cmap_clus(s),zorder=k)\n",
    "    ax2_H.hist(final_clusters[s].iloc[:,sample_2],density=True,alpha=.8,color=cmap_clus(s), orientation='horizontal',bins=B)\n",
    "\n",
    "# ax2.scatter(final_clusters[s1].index.get_level_values(\"all_site_pos\"),final_clusters[s1].iloc[:,i],alpha=.4,color=cmap(s1))\n",
    "# ax2.scatter(final_clusters[s2].index.get_level_values(\"all_site_pos\"),final_clusters[s2].iloc[:,i],alpha=.4,color=cmap(s2),zorder=10)\n",
    "# ax2.scatter(final_clusters[s3].index.get_level_values(\"all_site_pos\"),final_clusters[s3].iloc[:,i],alpha=.4,color=cmap(s3),zorder=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax2.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax2.set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "fig.text(0.075,0.42,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "ax2_H.set_xlabel(\"Site frequency\\nspectra\",size=30)\n",
    "\n",
    "ax1_H.set_xticks([],[])\n",
    "\n",
    "ax1_H.tick_params(labelleft=False)\n",
    "ax2_H.tick_params(labelleft=False)\n",
    "\n",
    "ax1_H.xaxis.set_major_locator(plt.NullLocator())\n",
    "ax1_H.xaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "ax2_H.xaxis.set_major_locator(plt.NullLocator())\n",
    "ax2_H.xaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "fig.legend(prop={\"size\":20},bbox_to_anchor=(.98,.89))\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0.01);\n",
    "\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_of_samples = len(final_clusters[0].columns.get_level_values(3))\n",
    "#no_of_samples = 3\n",
    "ax = []\n",
    "ax_H = []\n",
    "\n",
    "fig = plt.figure(constrained_layout=True,figsize=(32,6*no_of_samples))\n",
    "\n",
    "gs = fig.add_gridspec(no_of_samples, 16)\n",
    "\n",
    "for sample in range(no_of_samples):\n",
    "    \n",
    "    ax_temp = fig.add_subplot(gs[sample, :-2])\n",
    "    ax.append(ax_temp)\n",
    "    ax_H_temp = fig.add_subplot(gs[sample, -2:],sharey=ax[sample])\n",
    "    ax_H.append(ax_H_temp)\n",
    "\n",
    "    alpha=1\n",
    "    B = np.linspace(0,1,30)\n",
    "\n",
    "\n",
    "    ax[sample].grid(True)\n",
    "\n",
    "    ax[sample].tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    ax_H[sample].tick_params(axis='both', which='major', labelsize=0)\n",
    "\n",
    "    for k,s in enumerate(range(len(final_clusters))):\n",
    "\n",
    "        ax[sample].scatter(final_clusters[s].index.get_level_values(\"all_site_pos\"),final_clusters[s].iloc[:,sample],alpha=alpha,color=cmap_clus(s),zorder=k)\n",
    "        ax_H[sample].hist(final_clusters[s].iloc[:,sample],density=True,alpha=.8,color=cmap_clus(s), orientation='horizontal',label=f\"Strain {s+1}\",bins=B)\n",
    "        ax[sample].set_title(final_clusters[s].columns.get_level_values(\"mouse_number\")[sample] + \" \" +final_clusters[s].columns.get_level_values(\"region\")[sample], fontsize=20)  # Add title with the sample label\n",
    "\n",
    "        \n",
    "        \n",
    "    ax[sample].set_ylim([-0.05,1.05])\n",
    "    if sample != (no_of_samples-1):\n",
    "        ax[sample].set_xticklabels([])\n",
    "    ax_H[0].set_xticks([],[])\n",
    "    ax_H[sample].tick_params(labelleft=False)\n",
    "    ax_H[sample].xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax_H[sample].xaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## can zoom in on a specific region using these ll (lower limit), ul (upper limit), and off (offset) variables\n",
    "    # ll = 53799\n",
    "    # ul = 53799\n",
    "    # off = 10000\n",
    "\n",
    "    # ax1.set_xlim([ll-off,ul+off])\n",
    "    # ax2.set_xlim([ll-off,ul+off])\n",
    "\n",
    "ax[sample].set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "fig.text(0.075,0.42,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "ax_H[sample].set_xlabel(\"Site frequency\\nspectra\",size=30)\n",
    "\n",
    "fig.legend(prop={\"size\":20},bbox_to_anchor=(.98,.89))\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"/u/project/ngarud/michaelw/PaulAllen/humanized_mouse/figures/strain_phasing/{species}_genomiclocus_minclustersize{min_cluster_size}.png\",\n",
    "            facecolor='white', transparent=False, dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting 2-D sfs w/ marginal 1-D sfs's\n",
    "\n",
    "sample_1 = 0\n",
    "sample_2 = 5\n",
    "g = sns.JointGrid(Fs.iloc[:,sample_1].values,Fs.iloc[:,sample_2].values,\n",
    "                  height=8, ratio=5, space=.05,ylim=(-.05, 1.05),xlim=(-.05, 1.05))\n",
    "\n",
    "g.plot_joint(sns.scatterplot, s=25, alpha=.1)\n",
    "g.plot_marginals(sns.histplot, kde=False,bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying potential anchors\n",
    "There are lots of ways to find potentially evolutionarily interesting SNVs. A few that I came up with are:\n",
    "* Finding SNVs whose mean frequency differs most by some level (e.g. diet)\n",
    "* Finding SNVs with particularly high variance relative to some level\n",
    "\n",
    "Identifying potential anchors using some strategies is just step 1. Step 2 involves sorting through what comes out and actually finding SNVs that have some behavior we might be interested in (e.g. looks like there's a sweep between samples etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael's ideas:\n",
    "- subset by mouse, and see if all SNVs always fall into the same clusters\n",
    "- Identify SNVs that are outside of 2 SDs from the cluster mean in ANY sample\n",
    "    - maybe further filter out sites if they don't have some number of sites that are traveling under tight linkage with the anchor SNV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 1: Look for frequency differences across levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, pull out the SNV frequency matrices, but this time w/ more/less permissive filtering if desired\n",
    "\n",
    "Fs,Ass,Dss = return_FAD(species,poly_cov_frac = 1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency differs most by diet\n",
    "top_n = 25\n",
    "var_sites =(Fs.T.xs(\"C\",level=\"diet\").mean() - Fs.T.xs(\"G\",level=\"diet\").mean()).abs().dropna().sort_values(ascending=False).index[:top_n]\n",
    "\n",
    "## frequency has high variance across mice\n",
    "#var_sites = Fs.T.groupby(\"mouse_number\").mean().var().sort_values(ascending=False)[:top_n].index\n",
    "\n",
    "## Fs_sites is the dataframe of evolutionary SNVs\n",
    "## can do a primitive polarization of them all to relative to some sample (2nd line)\n",
    "## or come up w/ your own polarization? \n",
    "Fs_sites = Fs.loc[var_sites]\n",
    "# Fs_sites.loc[Fs_sites.iloc[:,0] > 0.5] = 1 - Fs_sites.loc[Fs_sites.iloc[:,0] > 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trial and error produces a potentially interesting site\n",
    "Fs_sites = Fs.loc[var_sites[0]].reset_index()\n",
    "\n",
    "Fs_sites['label'] = Fs_sites[\"mouse_number\"] + \", \" + Fs_sites[\"region\"] \n",
    "\n",
    "Fs_sites = Fs_sites.set_index(\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_sites.plot(color=\"grey\",legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2: identify SNVs that fall outside of 2 SDs from the cluster mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs,Ass,Dss = return_FAD(species,poly_cov_frac = 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying evolutionary SNVs using anchors\n",
    "If we have identified a SNV that looks like a promising evolutionary target, we can find all SNVs that are clustered with it using the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B vulgatus empirically found anchors\n",
    "# ancD = Dss.loc[('NC_009614', 663753)].values\n",
    "# ancF = Fs.loc[('NC_009614', 663753)].values\n",
    "# ancD = Dss.loc[('NC_009614', 1854143)].values\n",
    "# ancF = Fs.loc[('NC_009614', 1854143)].values\n",
    "\n",
    "\n",
    "## B. wex anchors\n",
    "# ancD = Dss.loc[(\"AXVN01000080\",  11540)].values\n",
    "# ancF = Fs.loc[(\"AXVN01000080\",  11540)].values\n",
    "\n",
    "## B. uni anchors\n",
    "# ancD = Dss.loc[('NZ_DS362247', 176768)]\n",
    "# ancF = Fs.loc[('NZ_DS362247', 176768)]\n",
    "\n",
    "## C. bac anchors\n",
    "# ancD = Dss.loc[('NZ_DS362247', 176768)]\n",
    "# ancF = Fs.loc[('NZ_DS362247', 176768)]\n",
    "ancD = Dss.loc[('FP929062', 2094823)]\n",
    "ancF = Fs.loc[('FP929062', 2094823)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can play around w/ max_d again here to find particularly tightly linked sites\n",
    "max_d = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = Dss.values\n",
    "fss = Fs.values\n",
    "\n",
    "disAnc_forward = []\n",
    "disAnc_backward = []\n",
    "for i in range(Dss.shape[0]):\n",
    "    disAnc_forward.append(calc_dis(ancD,dss[i],ancF,fss[i]))\n",
    "    disAnc_backward.append(calc_dis(ancD,dss[i],ancF,1-fss[i]))\n",
    "    \n",
    "disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "disAnc = np.array(disAnc)\n",
    "var_sites = Fs.loc[disAnc < max_d].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## very simple polarization w/r/t a given sample\n",
    "Fs_sites = Fs.loc[var_sites]\n",
    "\n",
    "samp_to_pol = 5\n",
    "Fs_sites.loc[Fs_sites.iloc[:,samp_to_pol] > 0.5] = 1 - Fs_sites.loc[Fs_sites.iloc[:,samp_to_pol] > 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "\n",
    "key_to_sort = \"mouse_number\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "cmap = get_cmap(len(list(set(mnum))))\n",
    "cmap_clus = get_cmap(len(list(set(mnum))),name=\"Set3\")\n",
    "\n",
    "\n",
    "Fs_sites_plot = reorder_sort(Fs_sites.T,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "\n",
    "ax.plot(Fs_sites_plot.values,color=\"red\",alpha=.5,zorder=500);\n",
    "\n",
    "for i in range(Fs_sites.shape[0]):\n",
    "    ax.scatter(range(Fs_sites.shape[1]),Fs_sites_plot.values[:,i],color=\"red\",alpha=.5,zorder=500,s=90);\n",
    "\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    \n",
    "    ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "    ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "    \n",
    "    ax.plot(ff.values,zorder=100,lw=6,color=cmap_clus(i),label=f\"Strain {i+1}\");\n",
    "    ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "    ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "        \n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff.index.get_level_values(secondary_key))\n",
    "\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "        \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    ax.axvspan(xmin - .1,xmax+.1,alpha=.2,color=cmap(i))\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)\n",
    "        ax.text(e, -0.1, second_xlabels[e], ha='center', clip_on=False,size=15)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "\n",
    "    ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels(labels);\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"SNV frequency\",size=20)\n",
    "ax.set_ylim([-0.05,1.05]);\n",
    "\n",
    "fig.legend(prop={\"size\":20});\n",
    "\n",
    "\n",
    "#fig.savefig(f\"figures/strains/{species}_strains\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting locations of evolutionary SNVs\n",
    "Same procedure as above, but now instead of plotting just locations of SNVs segregating between relative to one another, show particularly the evolutionary SNVs relative to a strain background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,1,figsize=(18,12))\n",
    "\n",
    "axs = axs.ravel()\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "i = 3\n",
    "s1 = 0\n",
    "ax1.scatter(all_clus_pol[s1].index.get_level_values(\"all_site_pos\"),all_clus_pol[s1].iloc[:,i],alpha=.3,color=\"grey\")\n",
    "ax1.scatter(Fs_sites.index.get_level_values(\"all_site_pos\"),Fs_sites.iloc[:,i],color=\"tomato\",s=220,edgecolor=\"yellow\")\n",
    "\n",
    "ax1.set_ylim([-0.05,1.05])\n",
    "\n",
    "# ll = 11678\n",
    "# ul = 11678\n",
    "# off = 2500\n",
    "# ax1.set_xlim([ll-off,ul+off])\n",
    "# ax2.set_xlim([ll-off,ul+off])\n",
    "\n",
    "i = 0\n",
    "ax2.scatter(all_clus_pol[s1].index.get_level_values(\"all_site_pos\"),all_clus_pol[s1].iloc[:,i],alpha=.3,color=\"grey\")\n",
    "ax2.scatter(Fs_sites.index.get_level_values(\"all_site_pos\"),Fs_sites.iloc[:,i],color=\"tomato\",s=220,edgecolor=\"yellow\")\n",
    "\n",
    "ax2.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax2.set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "fig.text(-0.05,0.44,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "# fig.text(.775,.0925,\"Mouse 1\",size=45,rotation=0,color=\"darkslategrey\")\n",
    "# fig.text(.775,.575,\"Mouse 2\",size=45,rotation=0,color=\"darkslategrey\")\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary SNVs: functions\n",
    "Now, pull out the functions of potentially interesting SNVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gene description and snps info files are produced using the write_snps_info.py script\n",
    "\n",
    "gene_descriptions = pd.read_pickle(f\"/u/project/ngarud/rwolff/mouse_sites/gene_descriptions/{species}_gene_descriptions.pkl\")\n",
    "\n",
    "snps_info = pd.read_pickle(f\"/u/project/ngarud/rwolff/mouse_sites/snps_info/{species}_snps_info.pkl\")\n",
    "snps_info = snps_info.reset_index(\"gene_id\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all site pos shows overall position in genome, but is not included in the snps_info file\n",
    "var_sites_xref = var_sites.droplevel(\"all_site_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## which genes are evolutionarily interesting snvs in? \n",
    "\n",
    "gene_descriptions.loc[snps_info.loc[var_sites_xref].groupby(\"gene_id\").size().sort_values(ascending=False).index][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many evolutionary snvs are in each gene? \n",
    "\n",
    "snps_info.loc[var_sites_xref].groupby(\"gene_id\").size().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## is each SNV syn or non-syn? \n",
    "\n",
    "for x in var_sites:\n",
    "    st = snps_info.loc[[x]].site_type.values[0]\n",
    "    if st == \"1D\":\n",
    "        g = snps_info.loc[[x]].gene_id.values[0]\n",
    "        print(g + \": (non-syn) \" + gene_descriptions.loc[g] + \"\\n\")\n",
    "    else:\n",
    "        g = snps_info.loc[[x]].gene_id.values[0]\n",
    "        print(g + \" (syn) : \" + gene_descriptions.loc[g] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count up # of snvs by site type\n",
    "np.unique(snps_info.loc[var_sites].site_type,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental: comparing w/ HMP haplotypes\n",
    "It could potentially be interesting to look at whether the strains we see in the mice have near relative haplotypes in HMP (my preliminary testing says maybe? for some species? needs work!). If so, where are the differences evolutionarily? Can we spot mouse-specific adaptations by comparing w/ HMP? \n",
    "\n",
    "Can look at whether strains pop up in HMP, or at linkage/allele frequencies of evolutionary SNVs, or other stuff. Basically, any kind of question where it might be interesting to have a broader cohort of controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = read_haplotypes(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_rec = []\n",
    "\n",
    "for gene_id in list(set(snps_info.loc[var_sites].gene_id))[:15]:\n",
    "\n",
    "    focal_sites  = 1-(Fs.droplevel(\"all_site_pos\").loc[snps_info.gene_id == gene_id]).T\n",
    "    F_rec.append(focal_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_sites = pd.concat(F_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA_focal = dfA.droplevel([\"gene_id\",\"site_type\"])\n",
    "\n",
    "dfA_focal = dfA_focal.loc[[f for f in focal_sites.columns if f in dfA_focal.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfA_focal =  dfA.xs(gene_id,level=\"gene_id\").droplevel(\"site_type\")\n",
    "\n",
    "\n",
    "# df_gene = dfA_focal.loc[[g for g in focal_sites.columns if g in dfA_focal.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene = dfA_focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = squareform(pdist(df_gene.T,metric=\"hamming\"))\n",
    "D = pd.DataFrame(D,index=df_gene.columns,columns=df_gene.columns)\n",
    "#thresh = 10\n",
    "thresh = np.percentile(take_triu(D.values)[take_triu(D.values) > 0],20)\n",
    "\n",
    "order = []\n",
    "group_centroids = []\n",
    "\n",
    "D_c = D < thresh\n",
    "while D_c.shape[0]>0:\n",
    "\n",
    "    D_c_o = D_c.sum().sort_values(ascending=False)\n",
    "    group_centroid = D_c_o.index[0]\n",
    "    D_c_i = D_c.loc[group_centroid]\n",
    "    group_centroids.append(group_centroid)\n",
    "    group = D_c_i.loc[D_c_i].index\n",
    "    \n",
    "    group = D.loc[group_centroid,group].sort_values().index\n",
    "    \n",
    "    order.append(list(group))\n",
    "    D_c = D_c.drop(group,axis=0).drop(group,axis=1)\n",
    "    \n",
    "order = np.array(order)\n",
    "order = sum(list(order[np.argsort(D.loc[group_centroids[0],group_centroids]).values]),[])\n",
    "\n",
    "dfplot = df_gene.copy()\n",
    "\n",
    "dfplot.loc[(snps_info.loc[df_gene.index] != \"1D\").site_type] = 4*dfplot.loc[(snps_info.loc[df_gene.index] != \"1D\").site_type]\n",
    "dfplot = dfplot[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,14))\n",
    "\n",
    "sns.heatmap(dfplot.T,cmap=hap_cmap,ax=ax,linecolor='k',cbar=False)\n",
    "\n",
    "ax.set_xticklabels([]);\n",
    "\n",
    "#ax.set_title(gene_id,size=60,color=\"tomato\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps_info.loc[df_gene.T.mean().sort_values(ascending=False).index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
