{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/m/michaelw/.conda/envs/python_env/lib/python3.6/site-packages/ipykernel_launcher.py:16: MatplotlibDeprecationWarning: \n",
      "The mpl_toolkits.axes_grid module was deprecated in Matplotlib 2.1 and will be removed two minor releases later. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist, which provide the same functionality instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import bz2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('text', usetex=True)\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}') \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid.inset_locator import (inset_axes, InsetPosition,\n",
    "                                                  mark_inset)\n",
    "import scipy.stats\n",
    "import figure_utils as fu\n",
    "from return_gene_descriptions import return_gene_descriptions\n",
    "\n",
    "from numba import njit \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import config\n",
    "import numpy\n",
    "import random as rand\n",
    "\n",
    "from random import randint,sample\n",
    "from math import log\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "plasma_cmap = cm.get_cmap('plasma')\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sys\n",
    "import os \n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid.inset_locator import (inset_axes, InsetPosition,\n",
    "                                                  mark_inset)\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "hap_cmap = ListedColormap(['grey', 'red', 'black', 'black','blue'], 'indexed')\n",
    "\n",
    "from strain_phasing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adlercreutzia_equolifaciens_60310\t Dorea_longicatena_61473\r\n",
      "Akkermansia_muciniphila_55290\t\t Enterococcus_avium_44557\r\n",
      "Alistipes_finegoldii_56071\t\t Enterococcus_faecalis_56297\r\n",
      "Alistipes_onderdonkii_55464\t\t Enterococcus_faecium_56947\r\n",
      "Alistipes_putredinis_61533\t\t Escherichia_coli_58110\r\n",
      "Alistipes_shahii_62199\t\t\t Eubacterium_cylindroides_56237\r\n",
      "Anaerostipes_hadrus_55206\t\t Eubacterium_eligens_61678\r\n",
      "Bacteroides_cellulosilyticus_58046\t Eubacterium_hallii_61477\r\n",
      "Bacteroides_finegoldii_57739\t\t Eubacterium_rectale_56927\r\n",
      "Bacteroides_fragilis_54507\t\t Faecalibacterium_cf_62236\r\n",
      "Bacteroides_massiliensis_44749\t\t Faecalibacterium_prausnitzii_57453\r\n",
      "Bacteroides_ovatus_58035\t\t Faecalibacterium_prausnitzii_61481\r\n",
      "Bacteroides_pectinophilus_61619\t\t Faecalibacterium_prausnitzii_62201\r\n",
      "Bacteroides_thetaiotaomicron_56941\t Guyana_massiliensis_60772\r\n",
      "Bacteroides_uniformis_57318\t\t Intestinimonas_butyriciproducens_60001\r\n",
      "Bacteroides_vulgatus_57955\t\t Lachnospiraceae_bacterium_51870\r\n",
      "Barnesiella_intestinihominis_62208\t Lachnospiraceae_bacterium_56833\r\n",
      "Bifidobacterium_adolescentis_56815\t Lactobacillus_acidophilus_51143\r\n",
      "Bifidobacterium_animalis_58116\t\t Lactococcus_lactis_57073\r\n",
      "Bifidobacterium_bifidum_55065\t\t Odoribacter_splanchnicus_62174\r\n",
      "Bifidobacterium_longum_57796\t\t Oscillibacter_sp_60799\r\n",
      "Bifidobacterium_pseudocatenulatum_57754  Oscillospiraceae_bacterium_54867\r\n",
      "Bilophila_wadsworthia_57364\t\t Parabacteroides_distasonis_56985\r\n",
      "Blautia_producta_56315\t\t\t Prevotella_copri_61740\r\n",
      "Blautia_wexlerae_56130\t\t\t Roseburia_hominis_61877\r\n",
      "Burkholderiales_bacterium_56577\t\t Roseburia_intestinalis_56239\r\n",
      "Clostridiales_bacterium_52743\t\t Roseburia_inulinivorans_61943\r\n",
      "Clostridiales_bacterium_52957\t\t Ruminococcus_bicirculans_59300\r\n",
      "Clostridiales_bacterium_55367\t\t Ruminococcus_bromii_62047\r\n",
      "Clostridiales_bacterium_59663\t\t Ruminococcus_gauvreauii_59033\r\n",
      "Clostridiales_bacterium_61057\t\t Ruminococcus_obeum_61472\r\n",
      "Clostridium_bolteae_57158\t\t Ruminococcus_obeum_62046\r\n",
      "Clostridium_clostridioforme_51842\t Ruminococcus_sp_55468\r\n",
      "Clostridium_hathewayi_61827\t\t Ruminococcus_sp_58571\r\n",
      "Clostridium_scindens_58238\t\t Ruminococcus_torques_62045\r\n",
      "Clostridium_sp_60465\t\t\t Streptococcus_parasanguinis_58487\r\n",
      "Clostridium_symbiosum_54029\t\t Streptococcus_salivarius_58022\r\n",
      "Coprococcus_catus_62200\t\t\t Streptococcus_salivarius_58037\r\n",
      "Coprococcus_comes_61587\t\t\t Streptococcus_thermophilus_54772\r\n",
      "Coprococcus_sp_62244\t\t\t Subdoligranulum_sp_62068\r\n",
      "Coriobacteriaceae_bacterium_58375\t Sutterella_wadsworthensis_56828\r\n",
      "Dorea_formicigenerans_56346\t\t Weissella_confusa_59158\r\n",
      "Dorea_longicatena_59913\r\n"
     ]
    }
   ],
   "source": [
    "!ls /u/project/ngarud/Garud_lab/HumanizedMouse/HumanizedMouse_Batch2/strain_phasing/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Blautia_wexlerae_56130\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster directory already exists.\n",
      "Figure directory already exists.\n"
     ]
    }
   ],
   "source": [
    "strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "\n",
    "#Raw cluster\n",
    "raw_cluster_path = \"%s%s\" % (config.strain_phasing_directory, \"strain_clusters/\")\n",
    "species_raw_cluster_dir = \"%s%s/\" % (raw_cluster_path, species)\n",
    "if not os.path.exists(species_raw_cluster_dir):\n",
    "    os.makedirs(species_raw_cluster_dir)\n",
    "    print(\"Cluster directory created successfully!\")\n",
    "else:\n",
    "    print(\"Cluster directory already exists.\")\n",
    "species_raw_cluster_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_RawCluster.pckl\")\n",
    "\n",
    "#Centroid and polarized cluster paths\n",
    "species_centroid_cluster_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_ClusterCentroid.pckl\")\n",
    "species_polarized_cluster_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_PolarizedCluster.pckl\")\n",
    "\n",
    "#Evolutionary SNPs\n",
    "evo_snvs_directory = \"%sstrain_phasing/snp_changes/%s/\" % (config.project_directory, species)\n",
    "inoculum_to_mouse_changes_path = \"%s%s%s\" % (evo_snvs_directory, species,\"_inoculum_mouse_changes.pckl\")\n",
    "all_snp_changes_path = \"%s%s%s\" % (evo_snvs_directory, species,\"_all_snp_changes.pckl\")\n",
    "\n",
    "#MIDAS data\n",
    "annotated_snps_path = \"%ssnps/%s/annotated_snps.txt.bz2\" % (config.data_directory, species)\n",
    "\n",
    "#Figure directory\n",
    "figure_directory = \"%s%s%s\" % (config.figure_directory, \"strain_phasing/\", species)\n",
    "if not os.path.exists(figure_directory):\n",
    "    os.makedirs(figure_directory)\n",
    "    print(\"Figure directory created successfully!\")\n",
    "else:\n",
    "    print(\"Figure directory already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "## if we didn't cap max_num_snvs, then min_cluster_size would be O(10^4), based on the typical evolutionary\n",
    "## distance between strains\n",
    "min_cluster_size = 1000\n",
    "#min_cluster_size = 100\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "## basically, the idea is that as the initial number of sites we pass in gets bigger, we want to incrwease the min_cluster_size\n",
    "## here, we say that 10% of all variable sites must be in a cluster in order for it to be considered a \"strain\"\n",
    "## this will largely be redundant w/ min_cluster_size, but adds some more functionality to play with\n",
    "min_cluster_fraction = 1/10\n",
    "#min_cluster_fraction = 0\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "## should still give us the same strain trajectory \n",
    "## clustering 20k SNVs takes ~90 seconds. \n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more \n",
    "## similar, higher means less similar, to be in a cluster\n",
    "#max_d = 5\n",
    "max_d = 3.5 \n",
    "max_d = 6 #USE THIS FOR B. WEXLERAE\n",
    "#max_d = 4 #USE THIS FOR B. UNIFORMIS\n",
    "#max_d = 4.25 #USE THIS FOR E. hallii\n",
    "#max_d = 5 \n",
    "\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 20\n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "## NOTE: we may want to disaggregate coverage and polymorphic-ness so as to not lose evolutionary snvs\n",
    "## but for strain clustering purposes, I think we should focus on SNVs that are actually polymorphic\n",
    "## in a good number of samples\n",
    "poly_cov_frac = 1/5 #\n",
    "\n",
    "## Number of clusters to calculate\n",
    "n_clusters = 100\n",
    "#n_clusters = 500\n",
    "\n",
    "#Minimum number of snvs per sample\n",
    "min_num_snvs_per_sample = 100\n",
    "\n",
    "#Another idea: cluster the centroids. If centroids are below a certain distance from each other, just merge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs,Ass,Dss = return_FAD(species, min_coverage=min_coverage, min_sample_coverage=min_sample_coverage, poly_cov_frac = poly_cov_frac, calculate_poly_cov_frac=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out samples without an adequate number of SNVs when all is said and done\n",
    "sample_with_adequate_snv_count = ~((~np.isnan(Fs)).sum() < min_num_snvs_per_sample)\n",
    "\n",
    "Fs = Fs.loc[:,sample_with_adequate_snv_count]\n",
    "Ass = Ass.loc[:,sample_with_adequate_snv_count]\n",
    "Dss = Dss.loc[:,sample_with_adequate_snv_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Inoculum\n",
    "\n",
    "Fs_inoculum = Fs.loc[:,(Fs.columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "Fs = Fs.loc[:,~(Fs.columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "\n",
    "Ass_inoculum = Ass.loc[:,(Ass.columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "Ass = Ass.loc[:,~(Ass.columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "\n",
    "Dss_inoculum = Dss.loc[:,(Dss.columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "Dss = Dss.loc[:,~(Dss.columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 20000 SNVs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 6.32 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fss = Ass.values/(Dss.values + (Dss.values == 0)) #This is so it doesn't produce a na (division by 0)\n",
    "\n",
    "cluster_As = Ass.values\n",
    "cluster_Ds = Dss.values\n",
    "cluster_fs = cluster_As/(cluster_Ds + (cluster_Ds == 0))\n",
    "\n",
    "## for compatibility in case of threshold number of SNVs\n",
    "num = min(max_num_snvs,Fs.shape[0])\n",
    "\n",
    "i_list = Dss.T.mean().sort_values(ascending=False).index[:num]\n",
    "\n",
    "sys.stderr.write(\"Processing %s SNVs\" % num)\n",
    "\n",
    "## simply shuffles indices if no threshold is specified\n",
    "#i_list = sample(range(Fs.shape[0]),num)\n",
    "i_list_idx = Fs.loc[i_list].index\n",
    "\n",
    "Ass_sub = Ass.loc[i_list_idx]\n",
    "Dss_sub = Dss.loc[i_list_idx]\n",
    "Fs_sub = Fs.loc[i_list_idx]\n",
    "\n",
    "fss_sub = Ass_sub.values/(Dss_sub.values + (Dss_sub.values == 0))\n",
    "\n",
    "cluster_As_sub = Ass_sub.values\n",
    "cluster_Ds_sub = Dss_sub.values\n",
    "cluster_fs_sub = cluster_As_sub/(cluster_Ds_sub + (cluster_Ds_sub == 0))\n",
    "\n",
    "D_mat = np.zeros([num,num])\n",
    "D_mat_1 = D_mat_fun1(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "D_mat = np.zeros([num,num]) \n",
    "D_mat_2 = D_mat_fun2(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "\n",
    "D_mat = np.fmin(D_mat_1,D_mat_2) #I believe this is filling in the minimum of the two polarizations\n",
    "D_mat = symmetrize(D_mat)\n",
    "\n",
    "D_mat_1 = pd.DataFrame(D_mat_1,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "D_mat_2 = pd.DataFrame(D_mat_2,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "\n",
    "D_mat_close = pd.DataFrame(D_mat < max_d) \n",
    "\n",
    "D_mat_close.index = Fs_sub.index\n",
    "D_mat_close.columns = Fs_sub.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9368\n",
      "6077\n"
     ]
    }
   ],
   "source": [
    "## extracts up to 100 clusters\n",
    "## in practice all SNVs should fall into one of a fairly small number of clusters\n",
    "## really should re-write this with a while loop but this works for now\n",
    "## the idea is that we exhaust all clusters—there should only be a small number of them ultimately\n",
    "\n",
    "###Idea with while loop:\n",
    "##### While there are still variants out there, have it try to be clusterings\n",
    "\n",
    "all_clus_pol = []\n",
    "all_clus_idx = []\n",
    "all_clus_A = []\n",
    "all_clus_D = []\n",
    "\n",
    "all_clus_F = []\n",
    "\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        clus,clus_idxs = return_clus(D_mat_close,Fs_sub) #Finding points that cluster with 25% other points. That's a cluster.\n",
    "                                                         #We would modify this function to get smaller clusters...\n",
    "        clus_pol = polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2)\n",
    "        clus_pol.index = clus_idxs\n",
    "        D_mat_close = drop_clus_idxs(D_mat_close,clus_idxs)\n",
    "        \n",
    "        if clus_pol.shape[0] > min_cluster_size and clus_pol.shape[0] > Fs.shape[0]*min_cluster_fraction:\n",
    "            \n",
    "            all_clus_D.append(Dss.loc[clus.index].mean().values)\n",
    "            all_clus_pol.append(clus_pol)\n",
    "            all_clus_A.append(clus_pol.mean()*all_clus_D[-1])\n",
    "            all_clus_F.append(clus_pol.mean())\n",
    "\n",
    "            print(clus_pol.shape[0])\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cluster 1\n",
      "\n",
      "\t0.0% finished\n",
      "\t2.663% finished\n",
      "\t5.327% finished\n",
      "\t7.99% finished\n",
      "\t10.654% finished\n",
      "\t13.317% finished\n",
      "\t15.98% finished\n",
      "\t18.644% finished\n",
      "\t21.307% finished\n",
      "\t23.971% finished\n",
      "\t26.634% finished\n",
      "\t29.297% finished"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d6a92b9382e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdisAnc_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mancF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdisAnc_backward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mancF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\t{np.around(100*j/Dss.shape[0],3)}% finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/project/ngarud/michaelw/Diversity-Along-Gut/HumanizedMouse/scripts/strain_phasing/strain_phasing_functions.py\u001b[0m in \u001b[0;36mcalc_dis\u001b[0;34m(di, dj, fi, fj)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreturn_FAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_coverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_sample_coverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoly_cov_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_poly_cov_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.6/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0mtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_divide_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[0misbad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python_env/lib/python3.6/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_divide_by_count\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# This is questionable, but currently a numpy scalar can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## now, choosing a representative SNV from each cluster, and finding all other sites (not just limited to the 20k)\n",
    "## which are consistent w/ being linked to it\n",
    "\n",
    "final_clusters = []\n",
    "\n",
    "all_aligned_sites = []\n",
    "\n",
    "for i in range(len(all_clus_D)):\n",
    "    \n",
    "    sys.stderr.write(f'\\n\\nCluster {i+1}\\n')\n",
    "    ancD = all_clus_D[i]\n",
    "    ancF = all_clus_F[i]\n",
    "\n",
    "    dss = Dss.values\n",
    "    fss = Fs.values\n",
    "    \n",
    "    disAnc_forward = []\n",
    "    disAnc_backward = []\n",
    "\n",
    "    for j in range(Dss.shape[0]):\n",
    "        disAnc_forward.append(calc_dis(ancD,dss[j],ancF,fss[j]))\n",
    "        disAnc_backward.append(calc_dis(ancD,dss[j],ancF,1-fss[j]))\n",
    "        if j % 1000 == 0:\n",
    "            sys.stderr.write(f\"\\n\\t{np.around(100*j/Dss.shape[0],3)}% finished\")\n",
    "    \n",
    "    disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "    disAnc = np.array(disAnc)\n",
    "    aligned_sites = Fs.loc[disAnc < max_d].index\n",
    "    f_dist =  pd.DataFrame(np.array([disAnc_forward,disAnc_backward]).T,index=Fs.index)\n",
    "    pols = f_dist.T.idxmin() > 0\n",
    "    \n",
    "    aligned_sites = [a for a in aligned_sites if a not in all_aligned_sites]\n",
    "    \n",
    "    pols = pols.loc[aligned_sites]\n",
    "    re_polarize = pols.loc[pols].index\n",
    "    \n",
    "    all_aligned_sites.extend(aligned_sites)\n",
    "    \n",
    "    Fs_cluster = Fs.loc[aligned_sites]\n",
    "    \n",
    "    Fs_cluster.loc[re_polarize] = 1 - Fs_cluster.loc[re_polarize]\n",
    "        \n",
    "    final_clusters.append(Fs_cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_clusters = False\n",
    "clusters_to_merge=[0,1]\n",
    "\n",
    "if merge_clusters:\n",
    "    i = 0\n",
    "    while i < len(clusters_to_merge)-1:\n",
    "        final_clusters[clusters_to_merge[0]] = final_clusters[i].append(final_clusters[i+1])\n",
    "        i += 1\n",
    "\n",
    "    clusters_to_remove = clusters_to_merge[1:]\n",
    "\n",
    "    final_clusters = [final_clusters[cluster] for cluster in np.arange(len(final_clusters)) if cluster not in clusters_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING RAW FILE\n",
    "\n",
    "pickle_object = open(species_raw_cluster_path, \"wb\")\n",
    "pickle.dump(final_clusters, pickle_object)\n",
    "pickle_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE RAW FILE\n",
    "\n",
    "final_clusters = pd.read_pickle(species_raw_cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More ordering utilities\n",
    "mnum = list(set(Fs.T.index.get_level_values(\"mouse_number\")))\n",
    "msite = list(set(Fs.T.index.get_level_values(\"region\")))\n",
    "mdiet = list(set(Fs.T.index.get_level_values(\"diet\")))\n",
    "mcage = list(set(Fs.T.index.get_level_values(\"cage\")))\n",
    "\n",
    "mnum_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"mouse_number\").index.get_level_values(\"mouse_number\") == m).ravel() for m in list(set(mnum))}\n",
    "\n",
    "msite_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"region\").index.get_level_values(\"region\") == m).ravel() for m in list(set(msite))}\n",
    "\n",
    "mdiet_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"diet\").index.get_level_values(\"diet\") == m).ravel() for m in list(set(mdiet))}\n",
    "\n",
    "mcage_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"cage\").index.get_level_values(\"cage\") == m).ravel() for m in list(set(mcage))}\n",
    "\n",
    "all_sample_dics = {\"diet\":mdiet_sample_dic,\"region\":msite_sample_dic,\"mouse_number\":mnum_sample_dic,\"cage\":mcage_sample_dic}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_to_sort = \"mouse_number\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "cmap = get_cmap(len(list(set(mnum))))\n",
    "cmap_clus = get_cmap(len(list(set(mnum))),name=\"Set3\")\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(20,8))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "if \"final_clusters\" in locals() or \"final_clusters\" in globals():\n",
    "    if len(final_clusters) != 0:\n",
    "        for i in np.arange(len(final_clusters)):\n",
    "            final_clusters_plot = final_clusters[i].loc[:, ~(final_clusters[i].columns.get_level_values('sample') == 'TL1gDNAshort')]\n",
    "\n",
    "            #ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "            ff_c = reorder_sort(final_clusters_plot.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "            #ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "\n",
    "            #ax.plot(ff.values,zorder=100,lw=6,color=cmap_clus(i),label=f\"Strain {i+1}\");\n",
    "            #ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "            ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "    else:\n",
    "        for i in np.arange(len([Fs])):\n",
    "            ff_c = reorder_sort(Fs.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "            ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "\n",
    "if \"potential_anchors\" in locals() or \"potential_anchors\" in globals():\n",
    "    pa = reorder_sort(potential_anchors.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "    ax.plot(pa.T.values,color=cmap_clus(i),alpha=1)\n",
    "        \n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff_c.T.index.get_level_values(secondary_key))\n",
    "\n",
    "#Making the vertical lines and labels\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)\n",
    "        ax.text(e, -0.05, second_xlabels[e], ha='center',va='top', clip_on=False,size=15, rotation=45)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "\n",
    "    ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "#Making the vertical colors\n",
    "key_to_sort = \"cage\"\n",
    "i = 0    \n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    ax.axvspan(xmin - .1,xmax+.1,alpha=.2,color=cmap(i))\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels(labels);\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"SNV frequency\",size=20)\n",
    "ax.set_ylim([-0.05,1.05]);\n",
    "\n",
    "\n",
    "fig.legend(prop={\"size\":20});\n",
    "\n",
    "# Create a new axis on the right side\n",
    "\n",
    "hist_data = Fs_inoculum.values\n",
    "ax_hist = ax.inset_axes([1, 0, 0.1, 1])\n",
    "ax_hist.hist(hist_data, bins=20, orientation=\"horizontal\", color=\"grey\")\n",
    "ax_hist.invert_yaxis()\n",
    "ax_hist.set_yticks([])\n",
    "ax_hist.set_xticks([])\n",
    "ax_hist.set_frame_on(False)\n",
    "ax_hist.invert_yaxis()\n",
    "\n",
    "\n",
    "# Set the title for the histogram\n",
    "ax_hist.set_title(\"Inoculum\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating polarized clusters\n",
    "\n",
    "Once clusters have been identified and internally polarized, they need to be polarized relative to one another. In the best case, the sum of strain frequencies will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(final_clusters) == 0:\n",
    "    df_final_f = pd.DataFrame(Fs.mean()).T\n",
    "    df_final_f.loc[:,:] = 1\n",
    "    final_f = []\n",
    "    final_f.append(df_final_f.mean())\n",
    "\n",
    "elif (len(final_clusters) == 1) & ((final_clusters[0].mean() < 0.05).sum()/len(final_clusters[0].columns) > 0.9):\n",
    "    df_final_f = pd.DataFrame(Fs.mean()).T\n",
    "    df_final_f.loc[:,:] = 1\n",
    "    final_f = []\n",
    "    final_f.append(df_final_f.mean())\n",
    "\n",
    "else:\n",
    "    ## If only a single cluster is detected, add a second \"cluster\" which is simply 1 minus the allele frequencies\n",
    "    ## in the first cluster\n",
    "    ## aids in visualization for people not familiar with this kind of clustering\n",
    "    if len(final_clusters) == 1:\n",
    "        final_clusters.append(1-final_clusters[0])\n",
    "\n",
    "\n",
    "    ## add cluster centroids\n",
    "    final_f = []\n",
    "    for cluster in final_clusters:\n",
    "        final_f.append(cluster.mean())\n",
    "    df_final_f = pd.DataFrame(final_f)\n",
    "\n",
    "    ## now, polarize clusters so that the sum of squareds of the centroids to 1 is minimized\n",
    "    ## the idea here is that accurate strain frequencies should sum to 1\n",
    "    polarize = True\n",
    "\n",
    "    if polarize:\n",
    "\n",
    "        pol_d2 = {}\n",
    "\n",
    "        for i in range(df_final_f.shape[0]):\n",
    "            df_final_f_temp = df_final_f.copy() #Makes a copy of the centroids\n",
    "            df_final_f_temp.iloc[i] = 1 - df_final_f_temp.iloc[i] #gets the polarized version of ONE of the centroids.\n",
    "            pol_d2[i] =  ((1 - df_final_f_temp.sum())**2).sum()   #Get the across centroids for all samples (should be close to 1), \n",
    "                                                                  #subtract this from 1, and square. Sum all those values\n",
    "                                                                  #Ideally, this value is really close to 0. \n",
    "                                                                  #Add this value to the dictionary.\n",
    "\n",
    "        pol_d2 = pd.Series(pol_d2)                                #Make the dictionary a series \n",
    "\n",
    "        if pol_d2.min() < ((1 - df_final_f.sum())**2).sum(): #If any of the above repolarizations actually made the overall sum of centroids closer to 1, repolarize.\n",
    "            clus_to_re_pol = pol_d2.idxmin()\n",
    "            final_f[clus_to_re_pol] = 1 - final_f[clus_to_re_pol]\n",
    "            final_clusters[clus_to_re_pol] = 1 - final_clusters[clus_to_re_pol]\n",
    "            df_final_f = pd.DataFrame(final_f)  \n",
    "            Fs_inoculum.loc[final_clusters[clus_to_re_pol].index] = 1 -  Fs_inoculum.loc[final_clusters[clus_to_re_pol].index]#polarize inoculum accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#SAVING RAW FILE\n",
    "\n",
    "pickle_object = open(species_centroid_cluster_path, \"wb\")\n",
    "pickle.dump(final_f, pickle_object)\n",
    "pickle_object.close()\n",
    "\n",
    "pickle_object = open(species_polarized_cluster_path, \"wb\")\n",
    "pickle.dump(final_clusters, pickle_object)\n",
    "pickle_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plot the chosen polarization of strains\n",
    "## sum of strain frequencies should be ~1 at all timepoints\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(pd.DataFrame(final_f).sum().values,zorder=10,lw=3)\n",
    "ax.set_ylim([.5,1.5])\n",
    "ax.axhline(1,color=\"k\",ls=\"--\")\n",
    "ax.set_ylabel(\"Sum of strain frequencies\",size=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting strain frequencies\n",
    "\n",
    "Strain frequencies can be plotted using a main key (e.g. mouse_number) and a secondary key (e.g. region), yielding a two-level identification of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cluster in enumerate(final_clusters):\n",
    "    if i == 0:\n",
    "        mask = ~np.isnan(cluster).all(axis = 0)\n",
    "    final_clusters[i] = cluster.loc[:,mask]\n",
    "    final_f[i] = final_f[i][mask]\n",
    "    Fs = Fs.loc[:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More ordering utilities\n",
    "mnum = list(set(Fs.T.index.get_level_values(\"mouse_number\")))\n",
    "msite = list(set(Fs.T.index.get_level_values(\"region\")))\n",
    "mdiet = list(set(Fs.T.index.get_level_values(\"diet\")))\n",
    "mcage = list(set(Fs.T.index.get_level_values(\"cage\")))\n",
    "\n",
    "mnum_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"mouse_number\").index.get_level_values(\"mouse_number\") == m).ravel() for m in list(set(mnum))}\n",
    "\n",
    "msite_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"region\").index.get_level_values(\"region\") == m).ravel() for m in list(set(msite))}\n",
    "\n",
    "mdiet_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"diet\").index.get_level_values(\"diet\") == m).ravel() for m in list(set(mdiet))}\n",
    "\n",
    "mcage_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"cage\").index.get_level_values(\"cage\") == m).ravel() for m in list(set(mcage))}\n",
    "\n",
    "all_sample_dics = {\"diet\":mdiet_sample_dic,\"region\":msite_sample_dic,\"mouse_number\":mnum_sample_dic,\"cage\":mcage_sample_dic}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clusters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_sort = \"mouse_number\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "cmap = get_cmap(len(list(set(mnum))))\n",
    "# cmap_clus = get_cmap(len(list(set(mnum))),name=\"Set3\")\n",
    "cmap_clus = get_cmap(5,name=\"Set3\")\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    \n",
    "    ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "    ax.plot(ff.values,zorder=100,lw=6,color=cmap_clus(i),label=f\"Strain {i+1}\");\n",
    "    ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "    if len(final_clusters) != 0:\n",
    "        ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "        ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "    else:\n",
    "        for i in np.arange(len([Fs])):\n",
    "            ff_c = reorder_sort(Fs.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "            ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "        \n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff.index.get_level_values(secondary_key))\n",
    "\n",
    "\n",
    "#Making the vertical lines and labels\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)\n",
    "        ax.text(e, -0.05, second_xlabels[e], ha='center',va='top', clip_on=False,size=15, rotation=45)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "\n",
    "    ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "#Making the vertical colors\n",
    "key_to_sort = \"cage\"\n",
    "i = 0    \n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    ax.axvspan(xmin - .1,xmax+.1,alpha=.2,color=cmap(i))\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels(labels);\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"SNV frequency\",size=20)\n",
    "ax.set_ylim([-0.05,1.05]);\n",
    "\n",
    "fig.legend(prop={\"size\":20});\n",
    "\n",
    "# Create a new axis on the right side\n",
    "\n",
    "hist_data = Fs_inoculum.values\n",
    "ax_hist = ax.inset_axes([1, 0, 0.1, 1])\n",
    "ax_hist.hist(hist_data, bins=20, orientation=\"horizontal\", color=\"grey\")\n",
    "ax_hist.invert_yaxis()\n",
    "ax_hist.set_yticks([])\n",
    "ax_hist.set_xticks([])\n",
    "ax_hist.set_frame_on(False)\n",
    "ax_hist.set_xlabel(\"Inoculum\", size = 15)\n",
    "ax_hist.invert_yaxis()\n",
    "\n",
    "\n",
    "# Set the title for the histogram\n",
    "#ax_hist.set_title(\"Inoculum\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_path = \"%s/%s_phased_strains.png\" % (figure_directory, species)\n",
    "fig.savefig(figure_path, facecolor='white', transparent=False, dpi=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlaying potential evolutionary SNVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all SNPS changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMCs = pd.read_pickle(all_snp_changes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_changes_df = pd.DataFrame(columns = ['contig','site_pos','gene','variant_type','sample1', 'sample2', 'alternate_freq_1', 'depth_1', 'alternate_freq_2', 'depth_2'])\n",
    "contig = []\n",
    "site_pos = []\n",
    "gene = []\n",
    "variant_type = []\n",
    "sample1 = []\n",
    "sample2 = []\n",
    "alternate_freq_1 = []\n",
    "depth_1 = []\n",
    "alternate_freq_2 = []\n",
    "depth_2 = []\n",
    "for key in IMCs.keys():\n",
    "    for snp in IMCs[key][2]:\n",
    "        contig.append(snp[1])\n",
    "        site_pos.append(snp[2])\n",
    "        gene.append(snp[0])\n",
    "        variant_type.append(snp[3])\n",
    "        sample1.append(key[0])\n",
    "        sample2.append(key[1])\n",
    "        alternate_freq_1.append(snp[4])\n",
    "        depth_1.append(snp[5])\n",
    "        alternate_freq_2.append(snp[6])\n",
    "        depth_2.append(snp[7])\n",
    "\n",
    "snp_changes_df['contig'] = contig\n",
    "snp_changes_df['site_pos'] = site_pos\n",
    "snp_changes_df['gene'] = gene\n",
    "snp_changes_df['variant_type'] = variant_type\n",
    "snp_changes_df['sample1'] = sample1\n",
    "snp_changes_df['sample2'] = sample2\n",
    "snp_changes_df['alternate_freq_1'] = alternate_freq_1\n",
    "snp_changes_df['depth_1'] = depth_1\n",
    "snp_changes_df['alternate_freq_2'] = alternate_freq_2\n",
    "snp_changes_df['depth_2'] = depth_2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_changes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_changes_df[['contig', 'site_pos', 'variant_type']].drop_duplicates().sort_values('site_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_locations = [(Fs.index.get_level_values('contig')[i], Fs.index.get_level_values('site_pos')[i]) for i in np.arange(len(Fs))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_locations = [[(snp[1],snp[2]) for snp in IMCs[key][2]] for key in IMCs.keys()]\n",
    "snp_locations = [snp for sublist in snp_locations for snp in sublist]\n",
    "snp_locations = list(set(snp_locations))\n",
    "snps_not_present = [snp for snp in snp_locations if snp not in Fs_locations]\n",
    "snp_locations = [snp for snp in snp_locations if snp in Fs_locations]\n",
    "Fs_idx = [True if snp in snp_locations else False for snp in Fs_locations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_snp = Fs.loc[Fs_idx]\n",
    "#Fs_snp = Fs_snp.loc[np.isnan(Fs_snp).sum(axis=1)/len(Fs_snp.columns) < 0.5]\n",
    "Fs_inoculum_snp = Fs_inoculum.loc[Fs_snp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_inoculum_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_snp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More ordering utilities\n",
    "mnum = list(set(Fs.T.index.get_level_values(\"mouse_number\")))\n",
    "msite = list(set(Fs.T.index.get_level_values(\"region\")))\n",
    "mdiet = list(set(Fs.T.index.get_level_values(\"diet\")))\n",
    "mcage = list(set(Fs.T.index.get_level_values(\"cage\")))\n",
    "\n",
    "mnum_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"mouse_number\").index.get_level_values(\"mouse_number\") == m).ravel() for m in list(set(mnum))}\n",
    "\n",
    "msite_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"region\").index.get_level_values(\"region\") == m).ravel() for m in list(set(msite))}\n",
    "\n",
    "mdiet_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"diet\").index.get_level_values(\"diet\") == m).ravel() for m in list(set(mdiet))}\n",
    "\n",
    "mcage_sample_dic = {m:np.argwhere(reorder_sort(Fs.T,\"cage\").index.get_level_values(\"cage\") == m).ravel() for m in list(set(mcage))}\n",
    "\n",
    "all_sample_dics = {\"diet\":mdiet_sample_dic,\"region\":msite_sample_dic,\"mouse_number\":mnum_sample_dic,\"cage\":mcage_sample_dic}\n",
    "\n",
    "colors = [\n",
    "    '#FF0000',  # Red\n",
    "    '#FFFF00',  # Yellow\n",
    "    '#00FF00',  # Lime Green\n",
    "    '#0000FF',  # Blue\n",
    "    '#FF00FF',  # Magenta\n",
    "    '#FFA500',  # Orange\n",
    "    '#00FFFF',  # Cyan\n",
    "    '#FF4500',  # Orange-Red\n",
    "    '#800080',  # Purple\n",
    "    '#008000'   # Green\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_sort = \"mouse_number\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "cmap = get_cmap(len(list(set(mnum))))\n",
    "# cmap_clus = get_cmap(len(list(set(mnum))),name=\"Set3\")\n",
    "cmap_clus = get_cmap(5,name=\"Set3\")\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    \n",
    "    ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "    ax.plot(ff.values,zorder=100,lw=6,color=cmap_clus(i),label=f\"Strain {i+1}\");\n",
    "    ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "    if len(final_clusters) != 0:\n",
    "        ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "        ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "    else:\n",
    "        for i in np.arange(len([Fs])):\n",
    "            ff_c = reorder_sort(Fs.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "            ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "#SNPS\n",
    "if len(Fs_snp) < 10:\n",
    "    ff_snps = reorder_sort(Fs_snp.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "    snp_df = pd.DataFrame(columns = ['sample_index','snp', 'freq'], index = np.arange(len(Fs_snp.T)))\n",
    "    row_counter = 0\n",
    "    freq_counter = 0\n",
    "    for i,row in Fs_snp.T.iterrows():\n",
    "        for j in np.arange(len(Fs_snp.T.columns)):\n",
    "            snp_df.loc[row_counter,'sample_index'] = freq_counter\n",
    "            snp_df.loc[row_counter,'snp'] = Fs_snp.T.columns[j]\n",
    "            snp_df.loc[row_counter,'freq'] = row[j]\n",
    "            row_counter += 1    \n",
    "        freq_counter += 1\n",
    "    color_category_map = {np.unique(snp_df['snp'])[snp_idx]: colors[snp_idx] for snp_idx in np.arange(len(np.unique(snp_df['snp'])))}\n",
    "    dot_colors = snp_df['snp'].map(color_category_map) \n",
    "    ax.scatter(data = snp_df, x = 'sample_index',y='freq',alpha=1, c = dot_colors, label = '')\n",
    "else:\n",
    "    ff_snps = reorder_sort(Fs_snp.T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "    ax.plot(ff_snps.sample(min(ff_snps.shape[0],10000)).T.values,color=\"red\",alpha=1)\n",
    "\n",
    "        \n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff.index.get_level_values(secondary_key))\n",
    "\n",
    "\n",
    "#Making the vertical lines and labels\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)\n",
    "        ax.text(e, -0.05, second_xlabels[e], ha='center',va='top', clip_on=False,size=15, rotation=45)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "\n",
    "    ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "#Making the vertical colors\n",
    "key_to_sort = \"cage\"\n",
    "i = 0    \n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "    \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    ax.axvspan(xmin - .1,xmax+.1,alpha=.2,color=cmap(i))\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels(labels);\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"SNV frequency\",size=20)\n",
    "ax.set_ylim([-0.05,1.05]);\n",
    "\n",
    "fig.legend(prop={\"size\":20}, loc=\"upper right\");\n",
    "\n",
    "# Create a new axis on the right side\n",
    "\n",
    "hist_data = Fs_inoculum.values\n",
    "ax_hist = ax.inset_axes([1, 0, 0.1, 1])\n",
    "ax_hist.hist(hist_data, bins=20, orientation=\"horizontal\", color=\"grey\", zorder=0)\n",
    "ax_hist.invert_yaxis()\n",
    "ax_hist.set_yticks([])\n",
    "ax_hist.set_xticks([])\n",
    "ax_hist.set_frame_on(False)\n",
    "ax_hist.set_xlabel(\"Inoculum\", size = 15)\n",
    "ax_hist.invert_yaxis()\n",
    "if len(Fs_snp) < 10:\n",
    "    dot_x_positions = np.linspace(0,(Fs_inoculum == 0).sum(axis=0).values[0],len(Fs_inoculum_snp.values))\n",
    "    for i in np.arange(len(Fs_inoculum_snp.values)):\n",
    "        dots_y_position = Fs_inoculum_snp.values[i]  # Assuming this is an array of y positions\n",
    "        dots_x_position = dot_x_positions[i]\n",
    "        dot_color = dot_colors[i]\n",
    "        ax_hist.scatter(dots_x_position, dots_y_position, color = dot_color, zorder=1)\n",
    "else:\n",
    "    ax_hist.scatter([0] * len(Fs_inoculum_snp.values), Fs_inoculum_snp.values, c='red', s=30, zorder=1)\n",
    "    \n",
    "\n",
    "\n",
    "# Set the title for the histogram\n",
    "#ax_hist.set_title(\"Inoculum\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_path = \"%s/%s_snp_changes.png\" % (figure_directory, species)\n",
    "fig.savefig(figure_path, facecolor='white', transparent=False, dpi=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SNPS that differ with the inoculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMCs = pd.read_pickle(inoculum_to_mouse_changes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Fs,TEST_Ass,TEST_Dss = return_FAD(species, min_coverage=5, min_sample_coverage=5, poly_cov_frac = 1/100, calculate_poly_cov_frac=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Fs.xs[['NZ_EQ973495', 619725], level=['contig', 'site_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Fs.xs(['619725'], level=['site_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Fs.xs(['NZ_EQ973495'], level=['contig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Fs.xs(['NZ_EQ973490', 1573676], level = ['contig', 'site_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strainfinder_dir = \"%sinput\" % (config.strain_phasing_directory)\n",
    "snp_alignment = pd.read_pickle(\"%s/%s/%s.strainfinder.p\" %  (strainfinder_dir ,species, species))\n",
    "samples = pd.read_pickle(\"%s/%s/%s.strainfinder.samples.p\" % (strainfinder_dir ,species, species))\n",
    "samples = [s.decode(\"utf-8\") for s in samples]\n",
    "snp_locations = pd.read_pickle(\"%s/%s/%s.strainfinder.locations.p\" % (strainfinder_dir,species,species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('NZ_EQ973495', 619725, 'R') in snp_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B vulgatus empirically found anchors\n",
    "ancD = Dss.loc[('NC_009614', 663753, 663753)].values\n",
    "ancF = Fs.loc[('NC_009614', 663753, 663753)].values\n",
    "ancD = Dss.loc[('NC_009614', 1854143, 1854143)].values\n",
    "ancF = Fs.loc[('NC_009614', 1854143, 1854143)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clusters[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot locations of SNVs and SFS's\n",
    "\n",
    "Plotting the physical location of SNVs can help to identify instances of HGT. Lots of SNVs which lie very close to one another are a good indication of possible HGT, though synteny issues of course persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,5))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "\n",
    "alpha=1\n",
    "B = np.linspace(0,1,30)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "for k,s in enumerate(final_clusters):\n",
    "    for i in np.arange(len(s)):\n",
    "        if i == 0:\n",
    "            ax.axvline(x = s.index.get_level_values(\"all_site_pos\").values[i], color=cmap_clus(k), linestyle='-', linewidth =1, label=f\"Strain {k+1}\")\n",
    "        else:\n",
    "            ax.axvline(x = s.index.get_level_values(\"all_site_pos\").values[i], color=cmap_clus(k), linestyle='-', linewidth =1)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "#ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "ax.set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "#fig.text(0.075,0.42,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "fig.legend(prop={\"size\":20},bbox_to_anchor=(.98,.89))\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0.01);\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True,figsize=(32,12))\n",
    "\n",
    "gs = fig.add_gridspec(2, 16)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, :-2])\n",
    "ax1_H = fig.add_subplot(gs[0, -2:],sharey=ax1)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, :-2])\n",
    "ax2_H = fig.add_subplot(gs[1, -2:],sharey=ax2)\n",
    "\n",
    "alpha=1\n",
    "B = np.linspace(0,1,30)\n",
    "\n",
    "\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "ax1_H.tick_params(axis='both', which='major', labelsize=0)\n",
    "ax2_H.tick_params(axis='both', which='major', labelsize=0)\n",
    "\n",
    "sample_1 = 1\n",
    "\n",
    "for k,s in enumerate(range(len(final_clusters))):\n",
    "    \n",
    "    ax1.scatter(final_clusters[s].index.get_level_values(\"all_site_pos\"),final_clusters[s].iloc[:,sample_1],alpha=alpha,color=cmap_clus(s),zorder=k)\n",
    "    ax1_H.hist(final_clusters[s].iloc[:,sample_1],density=True,alpha=.8,color=cmap_clus(s), orientation='horizontal',label=f\"Strain {s+1}\",bins=B)\n",
    "    \n",
    "ax1.set_ylim([-0.05,1.05])\n",
    "ax1.set_xticklabels([])\n",
    "\n",
    "## can zoom in on a specific region using these ll (lower limit), ul (upper limit), and off (offset) variables\n",
    "# ll = 53799\n",
    "# ul = 53799\n",
    "# off = 10000\n",
    "\n",
    "# ax1.set_xlim([ll-off,ul+off])\n",
    "# ax2.set_xlim([ll-off,ul+off])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_2 = 7\n",
    "\n",
    "for k,s in enumerate(range(len(final_clusters))):\n",
    "    ax2.scatter(final_clusters[s].index.get_level_values(\"all_site_pos\"),final_clusters[s].iloc[:,sample_2],alpha=alpha,color=cmap_clus(s),zorder=k)\n",
    "    ax2_H.hist(final_clusters[s].iloc[:,sample_2],density=True,alpha=.8,color=cmap_clus(s), orientation='horizontal',bins=B)\n",
    "\n",
    "# ax2.scatter(final_clusters[s1].index.get_level_values(\"all_site_pos\"),final_clusters[s1].iloc[:,i],alpha=.4,color=cmap(s1))\n",
    "# ax2.scatter(final_clusters[s2].index.get_level_values(\"all_site_pos\"),final_clusters[s2].iloc[:,i],alpha=.4,color=cmap(s2),zorder=10)\n",
    "# ax2.scatter(final_clusters[s3].index.get_level_values(\"all_site_pos\"),final_clusters[s3].iloc[:,i],alpha=.4,color=cmap(s3),zorder=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax2.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax2.set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "fig.text(0.075,0.42,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "ax2_H.set_xlabel(\"Site frequency\\nspectra\",size=30)\n",
    "\n",
    "ax1_H.set_xticks([],[])\n",
    "\n",
    "ax1_H.tick_params(labelleft=False)\n",
    "ax2_H.tick_params(labelleft=False)\n",
    "\n",
    "ax1_H.xaxis.set_major_locator(plt.NullLocator())\n",
    "ax1_H.xaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "ax2_H.xaxis.set_major_locator(plt.NullLocator())\n",
    "ax2_H.xaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "fig.legend(prop={\"size\":20},bbox_to_anchor=(.98,.89))\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0.01);\n",
    "\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_of_samples = len(final_clusters[0].columns.get_level_values(3))\n",
    "#no_of_samples = 3\n",
    "ax = []\n",
    "ax_H = []\n",
    "\n",
    "fig = plt.figure(constrained_layout=True,figsize=(32,6*no_of_samples))\n",
    "\n",
    "gs = fig.add_gridspec(no_of_samples, 16)\n",
    "\n",
    "for sample in range(no_of_samples):\n",
    "    \n",
    "    ax_temp = fig.add_subplot(gs[sample, :-2])\n",
    "    ax.append(ax_temp)\n",
    "    ax_H_temp = fig.add_subplot(gs[sample, -2:],sharey=ax[sample])\n",
    "    ax_H.append(ax_H_temp)\n",
    "\n",
    "    alpha=1\n",
    "    B = np.linspace(0,1,30)\n",
    "\n",
    "\n",
    "    ax[sample].grid(True)\n",
    "\n",
    "    ax[sample].tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "    ax_H[sample].tick_params(axis='both', which='major', labelsize=0)\n",
    "\n",
    "    for k,s in enumerate(range(len(final_clusters))):\n",
    "\n",
    "        ax[sample].scatter(final_clusters[s].index.get_level_values(\"all_site_pos\"),final_clusters[s].iloc[:,sample],alpha=alpha,color=cmap_clus(s),zorder=k)\n",
    "        ax_H[sample].hist(final_clusters[s].iloc[:,sample],density=True,alpha=.8,color=cmap_clus(s), orientation='horizontal',label=f\"Strain {s+1}\",bins=B)\n",
    "        ax[sample].set_title(final_clusters[s].columns.get_level_values(\"mouse_number\")[sample] + \" \" +final_clusters[s].columns.get_level_values(\"region\")[sample], fontsize=20)  # Add title with the sample label\n",
    "\n",
    "        \n",
    "        \n",
    "    ax[sample].set_ylim([-0.05,1.05])\n",
    "    if sample != (no_of_samples-1):\n",
    "        ax[sample].set_xticklabels([])\n",
    "    ax_H[0].set_xticks([],[])\n",
    "    ax_H[sample].tick_params(labelleft=False)\n",
    "    ax_H[sample].xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax_H[sample].xaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## can zoom in on a specific region using these ll (lower limit), ul (upper limit), and off (offset) variables\n",
    "    # ll = 53799\n",
    "    # ul = 53799\n",
    "    # off = 10000\n",
    "\n",
    "    # ax1.set_xlim([ll-off,ul+off])\n",
    "    # ax2.set_xlim([ll-off,ul+off])\n",
    "\n",
    "ax[sample].set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "fig.text(0.075,0.42,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "ax_H[sample].set_xlabel(\"Site frequency\\nspectra\",size=30)\n",
    "\n",
    "fig.legend(prop={\"size\":20},bbox_to_anchor=(.98,.89))\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"/u/project/ngarud/michaelw/PaulAllen/humanized_mouse/figures/strain_phasing/{species}_genomiclocus_minclustersize{min_cluster_size}.png\",\n",
    "            facecolor='white', transparent=False, dpi=300, bbox_inches='tight', pad_inches=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting 2-D sfs w/ marginal 1-D sfs's\n",
    "\n",
    "sample_1 = 0\n",
    "sample_2 = 5\n",
    "g = sns.JointGrid(Fs.iloc[:,sample_1].values,Fs.iloc[:,sample_2].values,\n",
    "                  height=8, ratio=5, space=.05,ylim=(-.05, 1.05),xlim=(-.05, 1.05))\n",
    "\n",
    "g.plot_joint(sns.scatterplot, s=25, alpha=.1)\n",
    "g.plot_marginals(sns.histplot, kde=False,bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying potential anchors\n",
    "There are lots of ways to find potentially evolutionarily interesting SNVs. A few that I came up with are:\n",
    "* Finding SNVs whose mean frequency differs most by some level (e.g. diet)\n",
    "* Finding SNVs with particularly high variance relative to some level\n",
    "\n",
    "Identifying potential anchors using some strategies is just step 1. Step 2 involves sorting through what comes out and actually finding SNVs that have some behavior we might be interested in (e.g. looks like there's a sweep between samples etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael's ideas:\n",
    "- subset by mouse, and see if all SNVs always fall into the same clusters\n",
    "- Identify SNVs that are outside of 2 SDs from the cluster mean in ANY sample\n",
    "    - maybe further filter out sites if they don't have some number of sites that are traveling under tight linkage with the anchor SNV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method 1: Look for frequency differences across levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, pull out the SNV frequency matrices, but this time w/ more/less permissive filtering if desired\n",
    "\n",
    "Fs,Ass,Dss = return_FAD(species,poly_cov_frac = 1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency differs most by diet\n",
    "top_n = 25\n",
    "var_sites =(Fs.T.xs(\"C\",level=\"diet\").mean() - Fs.T.xs(\"G\",level=\"diet\").mean()).abs().dropna().sort_values(ascending=False).index[:top_n]\n",
    "\n",
    "## frequency has high variance across mice\n",
    "#var_sites = Fs.T.groupby(\"mouse_number\").mean().var().sort_values(ascending=False)[:top_n].index\n",
    "\n",
    "## Fs_sites is the dataframe of evolutionary SNVs\n",
    "## can do a primitive polarization of them all to relative to some sample (2nd line)\n",
    "## or come up w/ your own polarization? \n",
    "Fs_sites = Fs.loc[var_sites]\n",
    "# Fs_sites.loc[Fs_sites.iloc[:,0] > 0.5] = 1 - Fs_sites.loc[Fs_sites.iloc[:,0] > 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "ax.plot(Fs_sites.T.values, zorder=80,lw=2,color=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trial and error produces a potentially interesting site\n",
    "Fs_sites = Fs.loc[var_sites[0]].reset_index()\n",
    "\n",
    "Fs_sites['label'] = Fs_sites[\"mouse_number\"] + \", \" + Fs_sites[\"region\"] \n",
    "\n",
    "Fs_sites = Fs_sites.set_index(\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_sites.plot(color=\"grey\",legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2: identify SNVs that fall outside of 2 SDs from the cluster mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs,Ass,Dss = return_FAD(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Meta-parameters: experiment with these—no hard and fast rules!\n",
    "\n",
    "## minimum number of SNVs which need to be clustered together in order to qualify as a \"strain\"\n",
    "## if we didn't cap max_num_snvs, then min_cluster_size would be O(10^4), based on the typical evolutionary\n",
    "## distance between strains\n",
    "min_cluster_size = 1000\n",
    "\n",
    "## minimum fraction of sites which pass our coverage threshold which must be in a cluster in order for it to qualify \n",
    "## as a strain\n",
    "## basically, the idea is that as the initial number of sites we pass in gets bigger, we want to incrwease the min_cluster_size\n",
    "## here, we say that 10% of all variable sites must be in a cluster in order for it to be considered a \"strain\"\n",
    "## this will largely be redundant w/ min_cluster_size, but adds some more functionality to play with\n",
    "min_cluster_fraction = 1/10\n",
    "\n",
    "## For computational efficiency, we can downsample the SNVs we actually perform strain phasing on\n",
    "## should still give us the same strain trajectory \n",
    "## clustering 20k SNVs takes ~90 seconds. \n",
    "max_num_snvs = 20000\n",
    "\n",
    "## distance threshold to be considered linked—lower means trajectories have to be more \n",
    "## similar, higher means less similar, to be in a cluster\n",
    "max_d = 3.5\n",
    "\n",
    "## minimum coverage to consider allele frequency at a site for purposes of clustering\n",
    "min_coverage = 10\n",
    "\n",
    "## minimum average sample coverage at polymorphic sites (e.g. sites in the A/D matrices)\n",
    "min_sample_coverage = 5\n",
    "\n",
    "## polymorphic & covered fraction: what percentage of samples does a site need \n",
    "## with coverage > min_coverage and polymorphic to be included in downstream analyses? \n",
    "## NOTE: we may want to disaggregate coverage and polymorphic-ness so as to not lose evolutionary snvs\n",
    "## but for strain clustering purposes, I think we should focus on SNVs that are actually polymorphic\n",
    "## in a good number of samples\n",
    "poly_cov_frac = 1/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fss = Ass.values/(Dss.values + (Dss.values == 0)) #This is so it doesn't produce a na (division by 0)\n",
    "\n",
    "cluster_As = Ass.values\n",
    "cluster_Ds = Dss.values\n",
    "cluster_fs = cluster_As/(cluster_Ds + (cluster_Ds == 0))\n",
    "\n",
    "## for compatibility in case of threshold number of SNVs\n",
    "num = min(max_num_snvs,Fs.shape[0])\n",
    "\n",
    "i_list = Dss.T.mean().sort_values(ascending=False).index[:num]\n",
    "\n",
    "sys.stderr.write(\"Processing %s SNVs\" % num)\n",
    "\n",
    "## simply shuffles indices if no threshold is specified\n",
    "#i_list = sample(range(Fs.shape[0]),num)\n",
    "i_list_idx = Fs.loc[i_list].index\n",
    "\n",
    "Ass_sub = Ass.loc[i_list_idx]\n",
    "Dss_sub = Dss.loc[i_list_idx]\n",
    "Fs_sub = Fs.loc[i_list_idx]\n",
    "\n",
    "fss_sub = Ass_sub.values/(Dss_sub.values + (Dss_sub.values == 0))\n",
    "\n",
    "cluster_As_sub = Ass_sub.values\n",
    "cluster_Ds_sub = Dss_sub.values\n",
    "cluster_fs_sub = cluster_As_sub/(cluster_Ds_sub + (cluster_Ds_sub == 0))\n",
    "\n",
    "D_mat = np.zeros([num,num])\n",
    "D_mat_1 = D_mat_fun1(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "D_mat = np.zeros([num,num]) \n",
    "D_mat_2 = D_mat_fun2(num,fss_sub,cluster_Ds_sub,D_mat)\n",
    "\n",
    "D_mat = np.fmin(D_mat_1,D_mat_2) #I believe this is filling in the minimum of the two poliarizations\n",
    "D_mat = symmetrize(D_mat)\n",
    "\n",
    "D_mat_1 = pd.DataFrame(D_mat_1,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "D_mat_2 = pd.DataFrame(D_mat_2,index=Fs_sub.index,columns=Fs_sub.index)\n",
    "\n",
    "D_mat_close = pd.DataFrame(D_mat < max_d) \n",
    "\n",
    "D_mat_close.index = Fs_sub.index\n",
    "D_mat_close.columns = Fs_sub.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracts up to 100 clusters\n",
    "## in practice all SNVs should fall into one of a fairly small number of clusters\n",
    "## really should re-write this with a while loop but this works for now\n",
    "## the idea is that we exhaust all clusters—there should only be a small number of them ultimately\n",
    "\n",
    "all_clus_pol = []\n",
    "all_clus_idx = []\n",
    "all_clus_A = []\n",
    "all_clus_D = []\n",
    "\n",
    "all_clus_F = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        clus,clus_idxs = return_clus(D_mat_close,Fs_sub)\n",
    "        clus_pol = polarize_clus(clus,clus_idxs,D_mat_1,D_mat_2)\n",
    "        clus_pol.index = clus_idxs\n",
    "        D_mat_close = drop_clus_idxs(D_mat_close,clus_idxs)\n",
    "        \n",
    "        if clus_pol.shape[0] > min_cluster_size and clus_pol.shape[0] > Fs.shape[0]*min_cluster_fraction:\n",
    "            \n",
    "            all_clus_D.append(Dss.loc[clus.index].mean().values)\n",
    "            all_clus_pol.append(clus_pol)\n",
    "            all_clus_A.append(clus_pol.mean()*all_clus_D[-1])\n",
    "            all_clus_F.append(clus_pol.mean())\n",
    "\n",
    "            print(clus_pol.shape[0])\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying evolutionary SNVs using anchors\n",
    "If we have identified a SNV that looks like a promising evolutionary target, we can find all SNVs that are clustered with it using the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs,Ass,Dss = return_FAD(species, min_coverage=0, min_sample_coverage=min_sample_coverage, poly_cov_frac = poly_cov_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs #FP929062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B vulgatus empirically found anchors\n",
    "ancD = Dss.loc[('NC_009614', 663753, 663753)].values\n",
    "ancF = Fs.loc[('NC_009614', 663753, 663753)].values\n",
    "# ancD = Dss.loc[('NC_009614', 1854143, 1854143)].values\n",
    "# ancF = Fs.loc[('NC_009614', 1854143, 1854143)].values\n",
    "\n",
    "\n",
    "## B. wex anchors\n",
    "# ancD = Dss.loc[(\"AXVN01000080\",  11540)].values\n",
    "# ancF = Fs.loc[(\"AXVN01000080\",  11540)].values\n",
    "\n",
    "## B. uni anchors\n",
    "# ancD = Dss.loc[('NZ_DS362247', 176768)]\n",
    "# ancF = Fs.loc[('NZ_DS362247', 176768)]\n",
    "\n",
    "## C. bac anchors\n",
    "# ancD = Dss.loc[('NZ_DS362247', 176768)]\n",
    "# ancF = Fs.loc[('NZ_DS362247', 176768)]\n",
    "# ancD = Dss.loc[('FP929062', 2094823, 2094823)]\n",
    "# ancF = Fs.loc[('FP929062', 2094823, 2094823)]\n",
    "# ancD = Dss.loc[('FP929062', 4700, 4700)]\n",
    "# ancF = Fs.loc[('FP929062', 4700, 4700)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## can play around w/ max_d again here to find particularly tightly linked sites\n",
    "max_d = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = Dss.values\n",
    "fss = Fs.values\n",
    "\n",
    "disAnc_forward = []\n",
    "disAnc_backward = []\n",
    "for i in range(Dss.shape[0]):\n",
    "    disAnc_forward.append(calc_dis(ancD,dss[i],ancF,fss[i]))\n",
    "    disAnc_backward.append(calc_dis(ancD,dss[i],ancF,1-fss[i]))\n",
    "    \n",
    "disAnc = [min(els) for els in zip(disAnc_forward, disAnc_backward)]\n",
    "disAnc = np.array(disAnc)\n",
    "var_sites = Fs.loc[disAnc < max_d].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## very simple polarization w/r/t a given sample\n",
    "Fs_sites = Fs.loc[var_sites]\n",
    "\n",
    "samp_to_pol = 0\n",
    "Fs_sites.loc[Fs_sites.iloc[:,samp_to_pol] < 0.5] = 1 - Fs_sites.loc[Fs_sites.iloc[:,samp_to_pol] < 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING CLUSTERED FILE\n",
    "\n",
    "raw_cluster_path = \"/u/project/ngarud/michaelw/PaulAllen/humanized_mouse/strain_clusters/\"\n",
    "species_raw_cluster_dir = \"%s%s/\" % (raw_cluster_path, species)\n",
    "if not os.path.exists(species_raw_cluster_dir):\n",
    "    os.makedirs(species_raw_cluster_dir)\n",
    "    print(\"Directory created successfully!\")\n",
    "else:\n",
    "    print(\"Directory already exists.\")\n",
    "evo_snvs_path = \"%s%s%s\" % (species_raw_cluster_dir, species, \"_EvoSnvs_ANC2.pckl\")\n",
    "\n",
    "\n",
    "#SAVING evolutionary_snvs FILE\n",
    "\n",
    "pickle_object = open(species_centroid_cluster_path, \"wb\")\n",
    "pickle.dump(final_f, pickle_object)\n",
    "pickle_object.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds_sites = Dss.loc[var_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "fig.suptitle(fu.get_pretty_species_name(species),size=30)\n",
    "\n",
    "\n",
    "key_to_sort = \"mouse_number\"\n",
    "secondary_key = \"region\"\n",
    "\n",
    "cmap = get_cmap(len(list(set(mnum))))\n",
    "cmap_clus = get_cmap(len(list(set(mnum))),name=\"Set3\")\n",
    "\n",
    "\n",
    "Fs_sites_plot = reorder_sort(Fs_sites.T,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "\n",
    "ax.plot(Fs_sites_plot.values,color=\"red\",alpha=.5,zorder=500);\n",
    "\n",
    "for i in range(Fs_sites.shape[0]):\n",
    "    ax.scatter(range(Fs_sites.shape[1]),Fs_sites_plot.values[:,i],color=\"red\",alpha=.5,zorder=500,s=90);\n",
    "\n",
    "\n",
    "for i,f in enumerate(final_f):\n",
    "    \n",
    "    ff = reorder_sort(f,key_to_sort).sort_index(key=lambda x: x.map(order_dict))\n",
    "    ff_c = reorder_sort(final_clusters[i].T,key_to_sort).T.sort_index(key=lambda x: x.map(order_dict),axis=1)\n",
    "    \n",
    "    ax.plot(ff.values,zorder=100,lw=6,color=cmap_clus(i),label=f\"Strain {i+1}\");\n",
    "    ax.plot(ff.values,zorder=80,lw=7,color=\"k\");\n",
    "    ax.plot(ff_c.sample(min(ff_c.shape[0],10000)).T.values,color=cmap_clus(i),alpha=.01)\n",
    "        \n",
    "major_x = []\n",
    "minor_x = []\n",
    "labels = []\n",
    "\n",
    "second_xlabels = list(ff.index.get_level_values(secondary_key))\n",
    "\n",
    "i = 0\n",
    "for key, item in all_sample_dics[key_to_sort].items():\n",
    "        \n",
    "    xmin = item.min() \n",
    "    xmax = item.max()\n",
    "    ax.axvspan(xmin - .1,xmax+.1,alpha=.2,color=cmap(i))\n",
    "    \n",
    "    for e in item:\n",
    "        ax.axvline(e,color=\"k\",zorder=0,alpha=.5)\n",
    "        ax.text(e, -0.1, second_xlabels[e], ha='center', clip_on=False,size=15)\n",
    "        \n",
    "    if xmin != xmax:\n",
    "        major_x.extend([xmin,(xmax + xmin)/2,xmax])\n",
    "        minor_x.append((xmax + xmin)/2)\n",
    "        labels.extend([\"\",key,\"\"])\n",
    "    else:\n",
    "        major_x.append(xmin)\n",
    "        minor_x.append(xmax)\n",
    "        labels.extend([key])   \n",
    "        \n",
    "    i+=1\n",
    "\n",
    "    ax.vlines(item[0] - 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.vlines(item[-1] + 0.5, 0, -0.15, color='black', lw=0.8, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "\n",
    "ax.set_xticks(major_x)\n",
    "ax.set_xticks(minor_x, minor = True)\n",
    "ax.set_xticklabels(labels);\n",
    "\n",
    "ax.axhline(0,color=\"grey\")\n",
    "ax.axhline(1,color=\"grey\")\n",
    "\n",
    "ax.tick_params(axis = 'x', which = 'major', length=0,labelsize = 20,pad=45)\n",
    "ax.tick_params(axis = 'x', which = 'minor', length = 10,labelsize = 0)\n",
    "    \n",
    "ax.set_ylabel(\"SNV frequency\",size=20)\n",
    "ax.set_ylim([-0.05,1.05]);\n",
    "\n",
    "fig.legend(prop={\"size\":20});\n",
    "\n",
    "\n",
    "#fig.savefig(f\"figures/strains/{species}_strains\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"%s%s%s%s\" % (config.figure_directory, \"strain_phasing/\", species, \"_EvoSnvs_ANC2.png\")\n",
    "fig.savefig(file_path, facecolor='white', transparent=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting locations of evolutionary SNVs\n",
    "Same procedure as above, but now instead of plotting just locations of SNVs segregating between relative to one another, show particularly the evolutionary SNVs relative to a strain background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,1,figsize=(18,12))\n",
    "\n",
    "axs = axs.ravel()\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "i = 3\n",
    "s1 = 0\n",
    "ax1.scatter(all_clus_pol[s1].index.get_level_values(\"all_site_pos\"),all_clus_pol[s1].iloc[:,i],alpha=.3,color=\"grey\")\n",
    "ax1.scatter(Fs_sites.index.get_level_values(\"all_site_pos\"),Fs_sites.iloc[:,i],color=\"tomato\",s=220,edgecolor=\"yellow\")\n",
    "\n",
    "ax1.set_ylim([-0.05,1.05])\n",
    "\n",
    "# ll = 11678\n",
    "# ul = 11678\n",
    "# off = 2500\n",
    "# ax1.set_xlim([ll-off,ul+off])\n",
    "# ax2.set_xlim([ll-off,ul+off])\n",
    "\n",
    "i = 0\n",
    "ax2.scatter(all_clus_pol[s1].index.get_level_values(\"all_site_pos\"),all_clus_pol[s1].iloc[:,i],alpha=.3,color=\"grey\")\n",
    "ax2.scatter(Fs_sites.index.get_level_values(\"all_site_pos\"),Fs_sites.iloc[:,i],color=\"tomato\",s=220,edgecolor=\"yellow\")\n",
    "\n",
    "ax2.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax2.set_xlabel(\"Genomic position (bp)\",size=30)\n",
    "\n",
    "fig.text(-0.05,0.44,\"SNV frequency\",size=30,rotation=90)\n",
    "\n",
    "# fig.text(.775,.0925,\"Mouse 1\",size=45,rotation=0,color=\"darkslategrey\")\n",
    "# fig.text(.775,.575,\"Mouse 2\",size=45,rotation=0,color=\"darkslategrey\")\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary SNVs: functions\n",
    "Now, pull out the functions of potentially interesting SNVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gene description and snps info files are produced using the write_snps_info.py script\n",
    "\n",
    "gene_descriptions = pd.read_pickle(f\"/u/project/ngarud/rwolff/mouse_sites/gene_descriptions/{species}_gene_descriptions.pkl\")\n",
    "\n",
    "snps_info = pd.read_pickle(f\"/u/project/ngarud/rwolff/mouse_sites/snps_info/{species}_snps_info.pkl\")\n",
    "snps_info = snps_info.reset_index(\"gene_id\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all site pos shows overall position in genome, but is not included in the snps_info file\n",
    "var_sites_xref = var_sites.droplevel(\"all_site_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## which genes are evolutionarily interesting snvs in? \n",
    "\n",
    "gene_descriptions.loc[snps_info.loc[var_sites_xref].groupby(\"gene_id\").size().sort_values(ascending=False).index][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many evolutionary snvs are in each gene? \n",
    "\n",
    "snps_info.loc[var_sites_xref].groupby(\"gene_id\").size().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## is each SNV syn or non-syn? \n",
    "\n",
    "for x in var_sites:\n",
    "    st = snps_info.loc[[x]].site_type.values[0]\n",
    "    if st == \"1D\":\n",
    "        g = snps_info.loc[[x]].gene_id.values[0]\n",
    "        print(g + \": (non-syn) \" + gene_descriptions.loc[g] + \"\\n\")\n",
    "    else:\n",
    "        g = snps_info.loc[[x]].gene_id.values[0]\n",
    "        print(g + \" (syn) : \" + gene_descriptions.loc[g] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count up # of snvs by site type\n",
    "np.unique(snps_info.loc[var_sites].site_type,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental: comparing w/ HMP haplotypes\n",
    "It could potentially be interesting to look at whether the strains we see in the mice have near relative haplotypes in HMP (my preliminary testing says maybe? for some species? needs work!). If so, where are the differences evolutionarily? Can we spot mouse-specific adaptations by comparing w/ HMP? \n",
    "\n",
    "Can look at whether strains pop up in HMP, or at linkage/allele frequencies of evolutionary SNVs, or other stuff. Basically, any kind of question where it might be interesting to have a broader cohort of controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = read_haplotypes(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_rec = []\n",
    "\n",
    "for gene_id in list(set(snps_info.loc[var_sites].gene_id))[:15]:\n",
    "\n",
    "    focal_sites  = 1-(Fs.droplevel(\"all_site_pos\").loc[snps_info.gene_id == gene_id]).T\n",
    "    F_rec.append(focal_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_sites = pd.concat(F_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA_focal = dfA.droplevel([\"gene_id\",\"site_type\"])\n",
    "\n",
    "dfA_focal = dfA_focal.loc[[f for f in focal_sites.columns if f in dfA_focal.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfA_focal =  dfA.xs(gene_id,level=\"gene_id\").droplevel(\"site_type\")\n",
    "\n",
    "\n",
    "# df_gene = dfA_focal.loc[[g for g in focal_sites.columns if g in dfA_focal.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene = dfA_focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = squareform(pdist(df_gene.T,metric=\"hamming\"))\n",
    "D = pd.DataFrame(D,index=df_gene.columns,columns=df_gene.columns)\n",
    "#thresh = 10\n",
    "thresh = np.percentile(take_triu(D.values)[take_triu(D.values) > 0],20)\n",
    "\n",
    "order = []\n",
    "group_centroids = []\n",
    "\n",
    "D_c = D < thresh\n",
    "while D_c.shape[0]>0:\n",
    "\n",
    "    D_c_o = D_c.sum().sort_values(ascending=False)\n",
    "    group_centroid = D_c_o.index[0]\n",
    "    D_c_i = D_c.loc[group_centroid]\n",
    "    group_centroids.append(group_centroid)\n",
    "    group = D_c_i.loc[D_c_i].index\n",
    "    \n",
    "    group = D.loc[group_centroid,group].sort_values().index\n",
    "    \n",
    "    order.append(list(group))\n",
    "    D_c = D_c.drop(group,axis=0).drop(group,axis=1)\n",
    "    \n",
    "order = np.array(order)\n",
    "order = sum(list(order[np.argsort(D.loc[group_centroids[0],group_centroids]).values]),[])\n",
    "\n",
    "dfplot = df_gene.copy()\n",
    "\n",
    "dfplot.loc[(snps_info.loc[df_gene.index] != \"1D\").site_type] = 4*dfplot.loc[(snps_info.loc[df_gene.index] != \"1D\").site_type]\n",
    "dfplot = dfplot[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,14))\n",
    "\n",
    "sns.heatmap(dfplot.T,cmap=hap_cmap,ax=ax,linecolor='k',cbar=False)\n",
    "\n",
    "ax.set_xticklabels([]);\n",
    "\n",
    "#ax.set_title(gene_id,size=60,color=\"tomato\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps_info.loc[df_gene.T.mean().sort_values(ascending=False).index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
